{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c2cfde-8771-4194-8f1a-15492adb334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGLMForCausalLM(\n",
      "  (model): XGLMModel(\n",
      "    (embed_tokens): XGLMScaledWordEmbedding(256008, 1024, padding_idx=1)\n",
      "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x XGLMDecoderLayer(\n",
      "        (self_attn): XGLMAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=256008, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad492dc-3d46-4ebb-ab10-9deebd92af97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGLMModel' object has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[32m      2\u001b[39m m = AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/xglm-564M\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(m.model, \u001b[33m\"\u001b[39m\u001b[33mdecoder\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mlayers\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'XGLMModel' object has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "m = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n",
    "print(hasattr(m.model, \"decoder\"), hasattr(m.model.decoder, \"layers\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266477e2-80dc-4f32-86ae-bfb0df4afebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGLMModel' object has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m tok = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/xglm-564M\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m model = AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mfacebook/xglm-564M\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m block = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m.layers[\u001b[32m0\u001b[39m]\n\u001b[32m      8\u001b[39m cache = {}\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhook_fc1\u001b[39m(module, inp, out):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'XGLMModel' object has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n",
    "block = model.model.decoder.layers[0]\n",
    "\n",
    "cache = {}\n",
    "def hook_fc1(module, inp, out):\n",
    "    print(\"ðŸ”¥ fc1 triggered\", out.shape)\n",
    "def hook_fc2(module, inp, out):\n",
    "    print(\"ðŸ”¥ fc2 triggered\", out.shape)\n",
    "\n",
    "h1 = block.fc1.register_forward_hook(hook_fc1)\n",
    "h2 = block.fc2.register_forward_hook(hook_fc2)\n",
    "\n",
    "inp = tok(\"The opposite of small is\", return_tensors=\"pt\")\n",
    "out = model(**inp)\n",
    "\n",
    "h1.remove(); h2.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7b9de-ffff-46a9-9015-ffa1964b0e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
