A low-tech way to build a partial Dyson sphere is to place a ring of habitats in circular orbit around the Sun. 
To completely surround the Sun, you could add rings orbiting it around different axes at slightly different distances, to avoid collisions. 
To avoid the nuisance that these fast-moving rings couldn’t be connected to one another, complicating transportation and communication, one could instead build a monolithic stationary Dyson sphere where the Sun’s inward gravitational pull is balanced by the outward pressure from the Sun’s radiation— an idea pioneered by Robert L. Forward and by Colin McInnes. 
software. They never learn to swim toward sugar; instead, that algorithm was hard-coded into their DNA from the start. There was of course a learning process of sorts, but it didn’t take place during the lifetime of that particular bacterium. Rather, it occurred during the preceding evolution of that species of bacteria, through a slow trial-and-error process spanning many generations, where natural selection favored those random DNA mutations that improved sugar consumption. Some of these mutations helped by improving the design of flagella and other hardware, while other mutations improved the bacterial information-processing system that implements the sugar-finding algorithm and other software. 
Such bacteria are an example of what I’ll call “Life 1.0”: life where both the hardware and software are evolved rather than designed. You and I, on the other hand, are examples of “Life 2.0”: life whose hardware is evolved, but whose software is largely designed. By your software, I mean all the algorithms and knowledge that you use to process the information from your senses and decide what to do—everything from the ability to recognize your friends when you see them to your ability to walk, read, write, calculate, sing and tell jokes. 
You weren’t able to perform any of those tasks when you were born, so all this software got programmed into your brain later through the process we call learning. Whereas your childhood curriculum is largely designed by your family and teachers, who decide what you should learn, you gradually gain more power to design your own software. Perhaps your school allows you to select a foreign language: Do you want to install a software module into your brain that enables you to speak French, or one that enables you to speak Spanish? Do you want to learn to play tennis or chess? Do you want to study to become a chef, a lawyer or a pharmacist? Do you want to learn more about artificial intelligence (AI) and the future of life by reading a book about it? 
This ability of Life 2.0 to design its software enables it to be much smarter than Life 1.0. DNA that I was born with. Your synapses store all your knowledge and skills as roughly 100 terabytes’ worth of information, while your DNA stores merely about a gigabyte, barely enough to store a single movie download. So it’s physically impossible for an infant to be born speaking perfect English and ready to ace her college entrance exams: there’s no way the information could have been preloaded into her brain, since the main information module she got from her parents (her DNA) lacks sufficient information-storage capacity. 
The ability to design its software enables Life 2.0 to be not only smarter than Life 1.0, but also more flexible. If the environment changes, 1.0 can only adapt by slowly evolving over many generations. Life 2.0, on the other hand, can adapt almost instantly, via a software update. For example, bacteria frequently encountering antibiotics may evolve drug resistance over many generations, but an individual bacterium won’t change its behavior at all; in contrast, a girl learning that she has a peanut allergy will immediately change her behavior to start avoiding peanuts. This flexibility gives Life 2.0 an even greater edge at the population level: even though the information in our human DNA hasn’t evolved dramatically over the past fifty thousand years, the information collectively stored in our brains, books and computers has exploded. By installing a software module enabling us to communicate through sophisticated spoken language, we ensured that the most useful information stored in one person’s brain could get copied to other brains, potentially surviving even after the original brain died. By installing a software module enabling us to read and write, we became able to store and share vastly more information than people could memorize. By developing brain software capable of producing technology (i.e., by studying science and engineering), we enabled much of the world’s information to be accessed by many of the world’s humans with just a few clicks. 
When I was a kid, I imagined that billionaires exuded pomposity and arrogance. When I first met Larry Page at Google in 2008, he totally shattered these stereotypes. Casually dressed in jeans and a remarkably ordinary-looking shirt, he would have blended right in at an MIT picnic. His thoughtful soft-spoken style and his friendly smile made me feel relaxed rather than intimidated talking with him. On July 18, 2015, we ran into each other at a party in Napa Valley thrown by Elon Musk and his then wife, Talulah, and got into a conversation about the scatological interests of our kids. I recommended the profound literary classic The Day My Butt Went Psycho, by Andy Griffiths, and Larry ordered it on the spot. I struggled to remind myself that he might go down in history as the most influential human ever to have lived: my guess is that if superintelligent digital life engulfs our Universe in my lifetime, it will be because of Larry’s decisions. 
This question is wonderfully controversial, with the world’s leading AI researchers disagreeing passionately not only in their forecasts, but also in their emotional reactions, which range from confident optimism to serious concern. They don’t even have consensus on short-term questions about AI’s economic, legal and military impact, and their disagreements grow when we expand the time horizon and ask about artificial general intelligence (AGI)—especially about AGI reaching human level and beyond, enabling Life 3.0. General intelligence can accomplish virtually any goal, including learning, in contrast to, say, the narrow intelligence of a chess-playing program. 
Interestingly, the controversy about Life 3.0 centers around not one but two separate questions: when and what? When (if ever) will it happen, and what will it mean for humanity? The way I see it, there are three distinct schools of thought that all need to be taken seriously, because they each include a number of world-leading experts. As illustrated in figure 1.2, I think of them as digital utopians, techno-skeptics and members of the beneficial-AI movement, respectively. Please let me introduce you to some of their most eloquent champions. 
This flexibility has enabled Life 2.0 to dominate Earth. Freed from its genetic shackles, humanity’s combined knowledge has kept growing at an accelerating pace as each breakthrough enabled the next: language, writing, the printing press, modern science, computers, the internet, etc. This ever-faster cultural evolution of our shared software has emerged as the dominant force shaping our human future, rendering our glacially slow biological evolution almost irrelevant. 
The sphere can be built by gradually adding more “statites”: stationary satellites that counteract the Sun’s gravity with radiation pressure rather than centrifugal forces. Wouldn’t it be tempting to escape the perils of technology without succumbing to stagnant totalitarianism? Let’s explore a scenario where this was accomplished by reverting to primitive technology, inspired by the Amish. After the Omegas took over the world as in the opening of the book, a massive global propaganda campaign was launched that romanticized the simple farming life of 1,500 years ago. Earth’s population was reduced to about 100 million people by an engineered pandemic blamed on terrorists. The pandemic was secretly targeted to ensure that nobody who knew anything about science or technology survived. With the excuse of eliminating the infection hazard of large concentrations of people, Prometheus-controlled robots emptied and razed all cities. Survivors were given large tracts of (suddenly available) land and educated in sustainable farming, fishing and hunting practices using only early medieval technology. In the meantime, armies of robots systematically removed all traces of modern technology (including cities, factories, power lines and paved roads), and thwarted all human attempts to document or re-create any such technology. Once the technology was globally forgotten, robots helped dismantle other robots until there were almost none left. The very last robots were deliberately vaporized together with Prometheus itself in a large thermonuclear explosion. There was no longer any need to ban modern technology, since it was all gone. As a result, humanity bought itself over a millennium of additional time without worries about either AI or totalitarianism. 
The first one regards the timeline from figure 1.2: how long will it take until machines greatly supersede human-level AGI? Here, a common misconception is that we know the answer with great certainty. 
One popular myth is that we know we’ll get superhuman AGI this century. In fact, history is full of technological over-hyping. Where are those fusion power plants and flying cars we were promised we’d have by now? AI too has been repeatedly over-hyped in the past, even by some of the founders of the field: for example, John McCarthy (who coined the term “artificial intelligence”), Marvin Minsky, Nathaniel Rochester and Claude Shannon wrote this overly optimistic forecast about what could be accomplished during two months with stone-age computers: “We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College…An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.” 
On the other hand, a popular counter-myth is that we know we won’t get superhuman AGI this century. Researchers have made a wide range of estimates for how far we are from superhuman AGI, but we certainly can’t say with great confidence that the probability is zero this century, given the dismal track record of such techno-skeptic predictions. For example, Ernest Rutherford, arguably the greatest nuclear physicist of his time, said in 1933—less than twenty-four hours before Leo Szilard’s invention of the nuclear chain reaction—that nuclear energy was “moonshine,” and in 1956 Astronomer Royal Richard Woolley called talk about space travel “utter bilge.” The most extreme form of this myth is that superhuman AGI will never arrive because it’s physically impossible. However, physicists know that a brain consists of quarks and electrons arranged to act as a powerful computer, and that there’s no law of physics preventing us from building even more intelligent quark blobs. 
Another common misconception is that the only people harboring concerns about AI and advocating AI-safety research are Luddites who don’t know much about AI. When Stuart Russell mentioned this during his Puerto Rico talk, the audience laughed loudly. A related misconception is that supporting AI-safety research is hugely controversial. In fact, to support a modest investment in AIsafety research, people don’t need to be convinced that risks are high, merely non-negligible, just as a modest investment in home insurance is justified by a non-negligible probability of the home burning down. 
My personal analysis is that the media have made the AI-safety debate seem more controversial than it really is. After all, fear sells, and articles using out-ofcontext quotes to proclaim imminent doom can generate more clicks than nuanced and balanced ones. As a result, two people who only know about each other’s positions from media quotes are likely to think they disagree more than they really do. For example, a techno-skeptic whose only knowledge about Bill Gates’ position comes from a British tabloid may mistakenly think he believes superintelligence to be imminent. Similarly, someone in the beneficial-AI movement who knows nothing about Andrew Ng’s position except his abovementioned quote about overpopulation on Mars may mistakenly think he doesn’t care about AI safety. In fact, I personally know that he does—the crux is simply that because his timeline estimates are longer, he naturally tends to prioritize short-term AI challenges over long-term ones. 
Reversion has to a lesser extent happened before: for example, some of the technologies that were in widespread use during the Roman Empire were largely forgotten for about a millennium before making a comeback during the Renaissance. Isaac Asimov’s Foundation trilogy centers around the “Seldon Plan” to shorten a reversion period from 30,000 years to 1,000 years. With clever planning, it may be possible to do the opposite and lengthen rather than shorten a reversion period, for example by erasing all knowledge of agriculture. However, unfortunately for reversion enthusiasts, it’s unlikely that this scenario can be extended indefinitely without humanity either going high-tech or going extinct. Counting on people’s resembling today’s biological humans 100 million years from now would be naive, given that we haven’t existed as a species for more. After contemplating problems that future technology might cause, it’s important to also consider problems that lack of that technology can cause. In this spirit, let us explore scenarios where superintelligence is never created because humanity eliminates itself by other means. 
How might we accomplish that? The simplest strategy is “just wait.” Although we’ll see in the next chapter how we can solve such problems as asteroid impacts and boiling oceans, these solutions all require technology that we haven’t yet developed, so unless our technology advances far beyond its present level, Mother Nature will drive us extinct long before another billion years have passed. As the famous economist John Maynard Keynes said: “In the long run we are all dead.” 
Unfortunately, there are also ways in which we might self-destruct much sooner, through collective stupidity. Why would our species commit collective suicide, also known as omnicide, if virtually nobody wants it? With our present level of intelligence and emotional maturity, we humans have a knack for miscalculations, misunderstandings and incompetence, and as a result, our history is full of accidents, wars and other calamities that, in hindsight, essentially nobody wanted. Economists and mathematicians have developed elegant game-theory explanations for how people can be incentivized to actions that ultimately cause a catastrophic outcome for everyone. Nuclear War: A Case Study in Human Recklessness 
You might think that the greater the stakes, the more careful we’d be, but a closer examination of the greatest risk that our current technology permits, namely a global thermonuclear war, isn’t reassuring. We’ve had to rely on luck to weather an embarrassingly long list of near misses caused by all sorts of things: computer malfunction, power failure, faulty intelligence, navigation error, bomber crash, satellite explosion and so on. 7 In fact, if it weren’t for heroic acts of certain individuals—for example, Vasili Arkhipov and Stanislav Petrov— we might already have had a global nuclear war. Given our track record, I think it’s highly unlikely that the annual probability of accidental nuclear war is as low as one in a thousand if we keep up our present behavior, in which case the 
probability that we’ll have one within 10,000 years exceeds 1− 0.99910000 ≈ 
99.995%. 
To fully appreciate our human recklessness, we must realize that we started the nuclear gamble even before carefully studying the risks. First, radiation risks had been underestimated, and over $2 billion in compensation has been paid out to victims of radiation exposure from uranium handling and nuclear tests in the United States alone. 
Second, it was eventually discovered that hydrogen bombs deliberately detonated hundreds of kilometers above Earth would create a powerful electromagnetic pulse (EMP) that might disable the electric grid and electronic devices over vast areas (figure 5.2), leaving infrastructure paralyzed, roads clogged with disabled vehicles and conditions for nuclear-aftermath survival less than ideal. For example, the U.S. EMP Commission reported that “the water infrastructure is a vast machine, powered partly by gravity but mostly by electricity,” and that denial of water can cause death in three to four days. Third, the potential of nuclear winter wasn’t realized until four decades in, after we’d deployed 63,000 hydrogen bombs—oops! Regardless of whose cities burned, massive amounts of smoke reaching the upper troposphere might spread around the globe, blocking out enough sunlight to transform summers into winters, much like when an asteroid or supervolcano caused a mass extinction in the past. When the alarm was sounded by both U.S. and Soviet scientists in the 1980s, this contributed to the decision of Ronald Reagan and Mikhail Gorbachev to start slashing stockpiles. 10 Unfortunately, more accurate calculations have painted an even gloomier picture: figure 5.3 shows cooling by about 20° Celsius (36° Fahrenheit) in much of the core farming regions of the United States, Europe, Russia and China (and by 35°C in some parts of Russia) for the first two summers, and about half that even a full decade later
Both of these forces drop off with the square of the distance to the Sun, which means that if they can be balanced at one distance from the Sun, they’ll conveniently be balanced at any other distance as well, allowing freedom to park anywhere in our Solar System. 
Statites need to be extremely lightweight sheets, weighing only 0.77 grams per square meter, which is about 100 times less than paper, but this is unlikely to be a showstopper. I rolled my eyes when seeing this headline in the Daily Mail:3 “Stephen Hawking Warns That Rise of Robots May Be Disastrous for Mankind.” I’ve lost count of how many similar articles I’ve seen. Typically, they’re accompanied by an evil-looking robot carrying a weapon, and suggest that we should worry about robots rising up and killing us because they’ve become conscious and/or evil. On a lighter note, such articles are actually rather impressive, because they succinctly summarize the scenario that my AI colleagues don’t worry about. That scenario combines as many as three separate misconceptions: concern about consciousness, evil and robots, respectively. 
If you drive down the road, you have a subjective experience of colors, sounds, etc. But does a self-driving car have a subjective experience? Does it feel like anything at all to be a self-driving car, or is it like an unconscious zombie without any subjective experience? Although this mystery of consciousness is interesting in its own right, and we’ll devote chapter 8 to it, it’s irrelevant to AI risk. If you get struck by a driverless car, it makes no difference to you whether it subjectively feels conscious. In the same way, what will affect us humans is what superintelligent AI does, not how it subjectively feels. 
The fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants. I sympathize with Rodney Brooks and other robotics pioneers who feel unfairly demonized by scaremongering tabloids, because some journalists seem obsessively fixated on robots and adorn many of their articles with evil-looking metal monsters with shiny red eyes. In fact, the main concern of the beneficialAI movement isn’t with robots but with intelligence itself: specifically, intelligence whose goals are misaligned with ours. To cause us trouble, such misaligned intelligence needs no robotic body, merely an internet connection— we’ll explore in chapter 4 how this may enable outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders and developing weapons we cannot even understand. Even if building robots were physically impossible, a super-intelligent and super-wealthy AI could easily pay or manipulate myriad humans to unwittingly do its bidding, as in William Gibson’s science fiction novel Neuromancer. In the rest of this book, you and I will explore together the future of life with AI. Let’s navigate this rich and multifaceted topic in an organized way by first exploring the full story of life conceptually and chronologically, and then exploring goals, meaning and what actions to take to create the future we want. 
In chapter 2, we explore the foundations of intelligence and how seemingly dumb matter can be rearranged to remember, compute and learn. As we proceed into the future, our story branches out into many scenarios defined by the answers to certain key questions. Figure 1.6 summarizes key questions we’ll encounter as we march forward in time, to potentially ever more advanced AI. 
Right now, we face the choice of whether to start an AI arms race, and questions about how to make tomorrow’s AI systems bug-free and robust. If AI’s economic impact keeps growing, we also have to decide how to modernize our laws and what career advice to give kids so that they can avoid soon-to-beautomated jobs. We explore such short-term questions in chapter 3. 
If AI progress continues to human levels, then we also need to ask ourselves how to ensure that it’s beneficial, and whether we can or should create a leisure society that flourishes without jobs. This also raises the question of whether an intelligence explosion or slow-but-steady growth can propel AGI far beyond human levels. We explore a wide range of such scenarios in chapter 4 and investigate the spectrum of possibilities for the aftermath in chapter 5, ranging from arguably dystopic to arguably utopic. Who’s in charge—humans, AI or cyborgs? Are humans treated well or badly? Are we replaced and, if so, do we perceive our replacements as conquerors or worthy descendants? I’m very curious about which of the chapter 5 scenarios you personally prefer! I’ve set up a website, http://AgeOfAi.org, where you can share your views and join the conversation. 
Finally, we forge billions of years into the future in chapter 6 where we can, ironically, draw stronger conclusions than in the previous chapters, as the ultimate limits of life in our cosmos are set not by intelligence but by the laws of physics. 
The robot misconception is related to the myth that machines can’t control humans. Intelligence enables control: humans control tigers not because we’re stronger, but because we’re smarter. This means that if we cede our position as smartest on our planet, it’s possible that we might also cede control. 
Figure 1.5 summarizes all of these common misconceptions, so that we can dispense with them once and for all and focus our discussions with friends and colleagues on the many legitimate controversies—which, as we’ll see, there’s no shortage of! 
The consciousness misconception is related to the myth that machines can’t have goals. Machines can obviously have goals in the narrow sense of exhibiting goal-oriented behavior: the behavior of a heat-seeking missile is most economically explained as a goal to hit a target. If you feel threatened by a machine whose goals are misaligned with yours, then it’s precisely its goals in this narrow sense that trouble you, not whether the machine is conscious and experiences a sense of purpose. If that heat-seeking missile were chasing you, you probably wouldn’t exclaim “I’m not worried, because machines can’t have goals!” 
If the Dyson sphere is built to reflect rather than absorb most of the sunlight, then the total intensity of light bouncing around within it will be dramatically increased, further boosting the radiation pressure and the amount of mass that can be supported in the sphere. 
Many other stars have a thousandfold and even a millionfold greater luminosity than our Sun, and are therefore able to support correspondingly heavier stationary Dyson spheres. 
For today’s humans, life on or in a Dyson sphere would at best be disorienting and at worst impossible, but that need not stop future biological or nonbiological life forms from thriving there. 
A problem with using black hole evaporation as a power source is that, unless the black hole is much smaller than an atom in size, it’s an excruciatingly slow process that takes longer than the present age of our Universe and radiates less energy than a candle. 
The power produced decreases with the square of the size of the hole, and the physicists Louis Crane and Shawn Westmoreland have therefore proposed using a black hole about a thousand times smaller than a proton, weighing about as much as the largest-ever seagoing ship. 
Their main motivation was to use the black hole engine to power a starship (a topic to which we return below), so they were more concerned with portability than efficiency and proposed feeding the black hole with laser light, causing no energy-to-matter conversion at all. 
Even if you could feed it with matter instead of radiation, guaranteeing high efficiency appears difficult: to make protons enter such a black hole a thousandth their size, they would have to be fired at the hole with a machine as powerful as the Large Hadron Collider, augmenting their energy mc2 with at least a thousand times more kinetic (motion) energy. 
Since at least 10% of that kinetic energy would be lost to gravitons when the black hole evaporates, we’d therefore be putting more energy into the black hole than we’d be able to extract and put to work, ending up with negative efficiency. 
Fortunately, there are other ways of using black holes as power plants that don’t involve quantum gravity or other poorly understood physics. For example, many existing black holes spin very fast, with their event horizons whirling around near the speed of light, and this rotation energy can be extracted. 
The event horizon of a black hole is the region from which not even light can escape, because the gravitational pull is too powerful. 
If you toss an object into the ergosphere, it will therefore pick up speed rotating around the hole. Unfortunately, it will soon get eaten up by the black hole, forever disappearing through the event horizon, so this does you no good if you’re trying to extract energy. However, Roger Penrose discovered that if you launch the object at a clever angle and make it split into two pieces as figure 6.4 illustrates, then you can arrange for only one piece to get eaten while the other escapes the black hole with more energy than you started with. In other words, you’ve successfully converted some of the rotational energy of the black hole into useful energy that you can put to work. By repeating this process many times, you can milk the black hole of all its rotational energy so that it stops spinning and its ergosphere disappears. 
Another interesting strategy is to extract energy not from the black hole itself, but from matter falling into it. Nature has already found a way of doing this all on its own: the quasar. As gas swirls even closer to a black hole, forming a pizza-shaped disk whose innermost parts gradually get gobbled up, it gets extremely hot and gives off copious amounts of radiation. As gas falls downward toward the hole, it speeds up, converting its gravitational potential energy into motion energy, just as a skydiver does. The motion gets progressively messier as complicated turbulence converts the coordinated motion of the gas blob into random motion on ever-smaller scales, until individual atoms begin colliding with each other at high speeds—having such random motion is precisely what it means to be hot, and these violent collisions convert motion energy into radiation. By building a Dyson sphere around the entire black hole, at a safe distance, this radiation energy can be captured and put to use. The faster the black hole spins, the more efficient this process gets, with a maximally spinning black hole delivering energy at a whopping 42% efficiency. *4 For black holes weighing about as much as a star, most of the energy comes out as X-rays, whereas for the supermassive kind found in the centers of galaxies, much of it emerges somewhere in the range of infrared, visible and ultraviolet light. 
There is another known way to convert matter into energy that doesn’t involve black holes at all: the sphaleron process. It can destroy quarks and turn them into leptons: electrons, their heavier cousins the muon and tau particles, neutrinos or their antiparticles. 4 As illustrated in figure 6.5, the standard model of particle physics predicts that nine quarks with appropriate flavor and spin can come together and transform into three leptons through an intermediate state called a sphaleron. Because the input weighs more than the output, the mass difference gets converted into energy according to Einstein’s E = mc2 formula. 
Future intelligent life might therefore be able to build what I’ll call a sphalerizer: an energy generator acting like a diesel engine on steroids. A traditional diesel engine compresses a mixture of air and diesel oil until the temperature gets high enough for it to spontaneously ignite and burn, after which the hot mixture re-expands and does useful work in the process, say pushing a piston. The carbon dioxide and other combustion gases weigh about 0.00000005% less than what was in the piston initially, and this mass difference turns into the heat energy driving the engine. A sphalerizer would compress ordinary matter to a couple of quadrillion degrees, and then let it re-expand and cool once the sphalerons had done their thing. *6 We already know the result of this experiment, because our early Universe performed it for us about 13.8 billion years ago, when it was that hot: almost 100% of the matter gets converted into energy, with less than a billionth of the particles left over being the stuff that ordinary matter is made of: quarks and electrons. So it’s just like a diesel engine, except over a billion times more efficient! Another advantage is that you don’t need to be finicky about what to fuel it with—it works with anything made of quarks, meaning any normal matter at all. 
Because of these high-temperature processes, our baby Universe produced over a trillion times more radiation (photons and neutrinos) than matter (quarks and electrons that later clumped into atoms). During the 13.8 billion years since then, a great segregation took place, where atoms became concentrated into galaxies, stars and planets, while most photons stayed in intergalactic space, forming the cosmic microwave background radiation that has been used to make baby pictures of our Universe. Any advanced life form living in a galaxy or other matter concentration can therefore turn most of its available matter back into energy, rebooting the matter percentage down to the same tiny value that emerged from our early Universe by briefly re-creating those hot dense conditions inside a sphalerizer. 
From a physics perspective, everything that future life may want to create—from habitats and machines to new life forms—is simply elementary particles arranged in some particular way. Just as a blue whale is rearranged krill and krill is rearranged plankton, our entire Solar System is simply hydrogen rearranged during 13.8 billion years of cosmic evolution: gravity rearranged hydrogen into stars which rearranged the hydrogen into heavier atoms, after which gravity rearranged such atoms into our planet where chemical and biological processes rearranged them into life. 
Future life that has reached its technological limit can perform such particle rearrangements more rapidly and efficiently, by first using its computing power to figure out the most efficient method and then using its available energy to power the matter rearrangement process. We saw how matter can be converted into both computers and energy, so it’s in a sense the only fundamental resource needed. *7 Once future life has bumped up against the physical limits on what it can do with its matter, there is only one way left for it to do more: by getting more matter. And the only way it can do this is by expanding into our Universe. Spaceward ho! 
The first challenge is that our Universe is expanding, which means that almost all galaxies are flying away from us, so settling distant galaxies amounts to a game of catch-up. The second challenge is that this cosmic expansion is accelerating, due to the mysterious dark energy that makes up about 70% of our 
Universe. To understand how this causes trouble, imagine that you enter a train platform and see your train slowly accelerating away from you, but with a door left invitingly open. If you’re fast and foolhardy, can you catch the train? Since it will eventually go faster than you can run, the answer clearly depends on how far away from you the train is initially: if it’s beyond a certain critical distance, you’ll never catch up with it. We face the same situation trying to catch those distant galaxies that are accelerating away from us: even if we could travel at the speed of light, all galaxies beyond about 17 billion light-years remain forever out of reach—and that’s over 98% of the galaxies in our Universe. 
But hold on: didn’t Einstein’s special relativity theory say that nothing can travel faster than light? So how can galaxies outrace something traveling at the speed of light? The answer is that special relativity is superseded by Einstein’s general relativity theory, where the speed limit is more liberal: nothing can travel faster than the speed of light through space, but space is free to expand as fast as it wants. Einstein also gave us a nice way of visualizing these speed limits by viewing time as the fourth dimension in spacetime (see figure 6.7, where I’ve kept things three-dimensional by omitting one of the three space dimensions). If space weren’t expanding, light rays would form slanted 45-degree lines through spacetime, so that the regions we can see and reach from here and now are cones. Whereas our past light cone would be truncated by our Big Bang 13.8 billion years ago, our future light cone would expand forever, giving us access to an unlimited cosmic endowment. In contrast, the middle panel of the figure shows that an expanding universe with dark energy (which appears to be the Universe we inhabit) deforms our light cones into a champagne-glass shape, forever limiting the number of galaxies we can settle to about 10 billion. 
If this limit makes you feel cosmic claustrophobia, let me cheer you up with a possible loophole: my calculation assumes that dark energy remains constant over time, consistent with what the latest measurements suggest. However, we still have no clue what dark energy really is, which leaves a glimmer of hope that dark energy will eventually decay away (much like the similar dark-energy-like substance postulated to explain cosmic inflation), and if this happens, the acceleration will give way to deceleration, potentially enabling future life forms to keep settling new galaxies for as long as they last. 
Another popular idea is to build a rocket that need not carry its own fuel. For example, interstellar space isn’t a perfect vacuum, but contains the occasional hydrogen ion (a lone proton: a hydrogen atom that’s lost its electron). In 1960, this gave physicist Robert Bussard the idea behind what’s now known as a Bussard ramjet: to scoop up such ions en route and use them as rocket fuel in an onboard fusion reactor. Although recent work has cast doubts on whether this can be made to work in practice, there’s another carry-no-fuel idea that does appear feasible for a high-tech spacefaring civilization: laser sailing. 
The possibility of superintelligence completely transforms this picture, making it much more promising for those with intergalactic wanderlust. Removing the need to transport bulky human life-support systems and adding AI-invented technology, intergalactic settlement suddenly appears rather straightforward. Forward’s laser sailing becomes much cheaper when the spacecraft need merely be large enough to contain a “seed probe”: a robot capable of landing on an asteroid or planet in the target solar system and building up a new civilization from scratch. It doesn’t even have to carry the instructions with it: all it has to do is build a receiving antenna large enough to pick up more detailed blueprints and instructions transmitted from its mother civilization at the speed of light. Once done, it uses its newly constructed lasers to send out new seed probes to continue settling the galaxy one solar system at a time. Even the vast dark expanses of space between galaxies tend to contain a significant number of intergalactic stars (rejects once ejected from their home galaxies) that can be used as way stations, thus enabling an island-hopping strategy for intergalactic laser sailing. 
If dark energy continues to accelerate distant galaxies away from one another, as the latest experimental data suggests, then this will pose a major nuisance to the future of life. It means that even if a future civilization manages to settle a million galaxies, dark energy will over the course of tens of billions of years fragment this cosmic empire into thousands of different regions unable to communicate with one another. If future life does nothing to prevent this fragmentation, then the largest remaining bastions of life will be clusters containing about a thousand galaxies, whose combined gravity is strong enough to overpower the dark energy trying to separate them. 
If a superintelligent civilization wants to stay connected, this would give it a strong incentive to do large-scale cosmic engineering. How much matter will it have time to move into its largest supercluster before dark energy puts it forever out of reach? One method for moving a star large distances is to nudge a third star into a binary system where two stars are stably orbiting each other. Just as with romantic relationships, the introduction of a third partner can destabilize things and lead to one of the three being violently ejected—in the stellar case, at great speed. If some of the three partners are black holes, such a volatile threesome can be used to fling mass fast enough to fly far outside the host galaxy. Unfortunately, this three-body technique, applied either to stars, black holes or galaxies, doesn’t appear able to move more than a tiny fraction of a civilization’s mass the large distances required to outsmart dark energy.
If, despite its best attempts at cosmic engineering, a future civilization concludes that parts of it are doomed to drift out of contact forever, it might simply let them go and wish them well. However, if it has ambitious computing goals that involve seeking the answers to certain very difficult questions, it might instead resort to a slash-and-burn strategy: it could convert the outlying galaxies into massive computers that transform their matter and energy into computation at a frenzied pace, in the hope that before dark energy pushes their burnt-out remnants from view, they could transmit the long-sought answers back to the mother cluster. This slash-and-burn strategy would be particularly appropriate for regions so distant that they can only be reached by the “cosmic spam” method, much to the chagrin of the preexisting inhabitants. Back home in the mother region, the civilization could instead aim for maximum conservation and efficiency to last as long as possible. 
After exploring how long future life can last, let’s explore how long it might want to last. Although you might find it natural to want to live as long as possible, Freeman Dyson also gave a more quantitative argument for this desire: the cost of computation drops when you compute slowly, so you’ll ultimately get more done if you slow things down as much as possible. Freeman even calculated that if our Universe keeps expanding and cooling forever, an infinite amount of computation might be possible. 
Slow doesn’t necessarily mean boring: if future life lives in a simulated world, its subjectively experienced flow of time need not have anything to do with the glacial pace at which the simulation is being run in the outside world, so the prospects of infinite computation could translate into subjective immortality for simulated life forms. Cosmologist Frank Tipler has built on this idea to speculate that you could also achieve subjective immortality in the final moments before a Big Crunch by speeding up the computations toward infinity as the temperature and density skyrocketed. 
So if life engulfs our cosmos, what form will it choose: simple and fast, or complex and slow? I predict that it will make the same choice as Earth life has made: both! The denizens of Earth’s biosphere span a staggering range of sizes, from gargantuan two-hundred-ton blue whales down to the petite 10-16 kg bacterium Pelagibacter, believed to account for more biomass than all the world’s fish combined. Moreover, organisms that are large, complex and slow often mitigate their sluggishness by containing smaller modules that are simple and fast. For example, your blink reflex is extremely fast precisely because it’s implemented by a small and simple circuit that doesn’t involve most of your brain: if that hard-to-swat fly accidentally heads toward your eye, you’ll blink within a tenth of a second, long before the relevant information has had time to spread throughout your brain and make you consciously aware of what happened. By organizing its information processing into a hierarchy of modules, our biosphere manages to both have the cake and eat it, attaining both speed and complexity. We humans already use this same hierarchical strategy to optimize parallel computing. 
What, if any, of this future information processing will be conscious in the sense of involving a subjective experience is a controversial and fascinating topic which we’ll explore in chapter 8. If consciousness requires the different parts of the system to be able to communicate with one another, then the thoughts of larger systems are by necessity slower. Whereas you or a future Earth-sized supercomputer can have many thoughts per second, a galaxy-sized mind could have only one thought every hundred thousand years, and a cosmic mind a billion light-years in size would only have time to have about ten thoughts in total before dark energy fragmented it into disconnected parts. On the other hand, these few precious thoughts and accompanying experiences might be quite deep! 
However, if superintelligence develops technology that can readily rearrange elementary particles into any form of matter whatsoever, then it will eliminate most of the incentive for long-distance trade. Why bother shipping silver between distant solar systems when it’s simpler and quicker to transmute copper into silver by rearranging its particles? Why bother shipping high-tech machinery between galaxies when both the know-how and the raw materials (any matter will do) exist in both places? My guess is that in a cosmos teeming with superintelligence, almost the only commodity worth shipping long distances will be information. The only exception might be matter to be used for cosmic engineering projects—for example, to counteract the aforementioned destructive tendency of dark energy to tear civilizations apart. As opposed to traditional human trade, this matter can be shipped in any convenient bulk form whatsoever, perhaps even as an energy beam, since the receiving superintelligence can rapidly rearrange it into whatever objects it wants. 
If the distance between neighboring space-settling civilizations is much larger than dark energy lets them expand, then they’ll never come into contact with each other or even find out about each other’s existence, so they’ll feel as if they’re alone in the cosmos. If our cosmos is more fecund so that neighbors are closer together, however, some civilizations will eventually overlap. What happens in these overlap regions? Will there be cooperation, competition or war? 
After all, information is very different from the resources that humans usually fight over, in that you can simultaneously give it away and keep it. 
Some expanding civilizations might have goals that are essentially immutable, such as those of a fundamentalist cult or a spreading virus. However, it’s also plausible that some advanced civilizations are more like open-minded humans— willing to adjust their goals when presented with sufficiently compelling arguments. If two of them meet, there will be a clash not of weapons but of ideas, where the most persuasive one prevails and has its goals spread at the speed of light through the region controlled by the other civilization. Assimilating your neighbors is a faster expansion strategy than settlement, since your sphere of influence can spread at the speed with which ideas move (the speed of light using telecommunication), whereas physical settlement inevitably progresses slower than the speed of light. This assimilation will not be forced such as that infamously employed by the Borg in Star Trek, but voluntary based on the persuasive superiority of ideas, leaving the assimilated better off. 
We’ve seen that the future cosmos can contain rapidly expanding bubbles of two kinds: expanding civilizations and those death bubbles that expand at light speed and make space uninhabitable by destroying all our elementary particles. An ambitious civilization can thus encounter three kinds of regions: uninhabited ones, life bubbles and death bubbles. If it fears uncooperative rival civilizations, it has a strong incentive to launch a rapid “land grab” and settle the uninhabited regions before the rivals do. However, it has the same expansionist incentive even if there are no other civilizations, simply to acquire resources before dark energy makes them unreachable. We just saw how bumping into another expanding civilization can be either better or worse than bumping into uninhabited space, depending on how cooperative and open-minded this neighbor is. However, it’s better to bump into any expansionist civilization (even one trying to convert your civilization into paper clips) than a death bubble, which will continue expanding at the speed of light regardless of whether you try to fight it or reason with it. 
I give a detailed justification of this argument in my book Our Mathematical Universe, so I won’t rehash it here, but the basic reason for why we’re clueless about this neighbor distance is that we’re in turn clueless about the probability of intelligent life arising in a given place. As the American astronomer Frank Drake pointed out, this probability can be calculated by multiplying together the probability of there being a habitable environment there (say an appropriate planet), the probability that life will form there and the probability that this life will evolve to become intelligent. When I was a grad student, we had no clue about any of these three probabilities. After the past two decades’ dramatic discoveries of planets orbiting other stars, it now seems likely that habitable planets are abundant, with billions in our own Galaxy alone. The probability of evolving life and then intelligence, however, remains extremely uncertain: some experts think that one or both are rather inevitable and occur on most habitable planets, while others think that one or both are extremely rare because of one or more evolutionary bottlenecks that require a wild stroke of luck to pass through. 
This is broad enough to include all above-mentioned definitions, since understanding, self-awareness, problem solving, learning, etc. are all examples of complex goals that one might have. It’s also broad enough to subsume the Oxford Dictionary definition—“the ability to acquire and apply knowledge and skills”—since one can have as a goal to apply knowledge and skills. 
Because there are many possible goals, there are many possible types of intelligence. By our definition, it therefore makes no sense to quantify intelligence of humans, non-human animals or machines by a single number such as an IQ. *1 What’s more intelligent: a computer program that can only play chess or one that can only play Go? There’s no sensible answer to this, since they’re good at different things that can’t be directly compared. 
The DQN AI system of Google DeepMind can accomplish a slightly broader range of goals: it can play dozens of different vintage Atari computer games at human level or better. In contrast, human intelligence is thus far uniquely broad, able to master a dazzling panoply of skills. A healthy child given enough training time can get fairly good not only at any game, but also at any language, sport or vocation. Comparing the intelligence of humans and machines today, we humans win hands-down on breadth, while machines outperform us in a small but growing number of narrow domains, as illustrated in figure 2.1. The holy grail of AI research is to build “general AI” (better known as artificial general intelligence, AGI) that is maximally broad: able to accomplish virtually any goal, including learning. We’ll explore this in detail in chapter 4. The term “AGI” was popularized by the AI researchers Shane Legg, Mark Gubrud and Ben Goertzel to more specifically mean human-level artificial general intelligence: the ability to accomplish any goal at least as well as humans. 1 I’ll stick with their definition, so unless I explicitly qualify the acronym (by writing “superhuman AGI,” for example), I’ll use “AGI” as shorthand for “human-level AGI.”*2 
Although the word “intelligence” tends to have positive connotations, it’s important to note that we’re using it in a completely value-neutral way: as ability to accomplish complex goals regardless of whether these goals are considered good or bad. 
It’s natural for us to rate the difficulty of tasks relative to how hard it is for us humans to perform them, as in figure 2.1. But this can give a misleading picture of how hard they are for computers. It feels much harder to multiply 314,159 by 271,828 than to recognize a friend in a photo, yet computers creamed us at arithmetic long before I was born, while human-level image recognition has only recently become possible. This fact that low-level sensorimotor tasks seem easy despite requiring enormous computational resources is known as Moravec’s paradox, and is explained by the fact that our brain makes such tasks feel easy by dedicating massive amounts of customized hardware to them—more than a quarter of our brains, in fact. 
During the decades since he wrote those passages, the sea level has kept rising relentlessly, as he predicted, like global warming on steroids, and some of his foothills (including chess) have long since been submerged. What comes next and what we should do about it is the topic of the rest of this book. 
As the sea level keeps rising, it may one day reach a tipping point, triggering dramatic change. This critical sea level is the one corresponding to machines becoming able to perform AI design. Before this tipping point is reached, the sea-level rise is caused by humans improving machines; afterward, the rise can be driven by machines improving machines, potentially much faster than humans could have done, rapidly submerging all land. This is the fascinating and controversial idea of the singularity, which we’ll have fun exploring in chapter 4. 
Computer pioneer Alan Turing famously proved that if a computer can perform a certain bare minimum set of operations, then, given enough time and memory, it can be programmed to do anything that any other computer can do. Machines exceeding this critical threshold are called universal computers (aka Turing-universal computers); all of today’s smartphones and laptops are universal in this sense. Analogously, I like to think of the critical intelligence threshold required for AI design as the threshold for universal intelligence: given enough time and resources, it can make itself able to accomplish any goal as well as any other intelligent entity. For example, if it decides that it wants better social skills, forecasting skills or AI-design skills, it can acquire them.
If we say that an atlas contains information about the world, we mean that there’s a relation between the state of the book (in particular, the positions of certain molecules that give the letters and images their colors) and the state of the world (for example, the locations of continents). If the continents were in different places, then those molecules would be in different places as well. We humans use a panoply of different devices for storing information, from books and brains to hard drives, and they all share this property: that their state can be related to (and therefore inform us about) the state of other things that we care about. 
What fundamental physical property do they all have in common that makes them useful as memory devices, i.e., devices for storing information? The answer is that they all can be in many different long-lived states—long-lived enough to encode the information until it’s needed. As a simple example, suppose you place a ball on a hilly surface that has sixteen different valleys, as in figure 2.3. Once the ball has rolled down and come to rest, it will be in one of sixteen places, so you can use its position as a way of remembering any number between 1 and 16. 
This memory device is rather robust, because even if it gets a bit jiggled and disturbed by outside forces, the ball is likely to stay in the same valley that you put it in, so you can still tell which number is being stored. The reason that this memory is so stable is that lifting the ball out of its valley requires more energy than random disturbances are likely to provide. This same idea can provide stable memories much more generally than for a movable ball: the energy of a complicated physical system can depend on all sorts of mechanical, chemical, electrical and magnetic properties, and as long as it takes energy to change the system away from the state you want it to remember, this state will be stable. This is why solids have many long-lived states, whereas liquids and gases don’t: if you engrave someone’s name on a gold ring, the information will still be there years later because reshaping the gold requires significant energy, but if you engrave it in the surface of a pond, it will be lost within a second as the water surface effortlessly changes its shape.
We can therefore think of it as encoding a binary digit (abbreviated “bit”), i.e., a zero or a one. The information stored by any more complicated memory device can equivalently be stored in multiple bits: for example, taken together, the four bits shown in figure 2.3 can be in 2 × 2 × 2 × 2 = 16 different states 0000, 0001, 0010, 0011,…, 1111, so they collectively have exactly the same memory capacity as the more complicated 16-state system. We can therefore think of bits as atoms of information—the smallest indivisible chunk of information that can’t be further subdivided, which can combine to make up any information. For example, I just typed the word “word,” and my laptop represented it in its memory as the 4-number sequence 119 111 114 100, storing each of those numbers as 8 bits (it represents each lowercase letter by a number that’s 96 plus its order in the alphabet). As soon as I hit the w key on my keyboard, my laptop displayed a visual image of a w on my screen, and this image is also represented by bits: 32 bits specify the color of each of the screen’s millions of pixels. 
Since two-state systems are easy to manufacture and work with, most modern computers store their information as bits, but these bits are embodied in a wide variety of ways. On a DVD, each bit corresponds to whether there is or isn’t a microscopic pit at a given point on the plastic surface. On a hard drive, each bit corresponds to a point on the surface being magnetized in one of two ways. In my laptop’s working memory, each bit corresponds to the positions of certain electrons, determining whether a device called a micro-capacitor is charged. Some kinds of bits are convenient to transport as well, even at the speed of light: for example, in an optical fiber transmitting your email, each bit corresponds to a laser beam being strong or weak at a given time. 
Engineers prefer to encode bits into systems that aren’t only stable and easy to read from (as a gold ring), but also easy to write to: altering the state of your hard drive requires much less energy than engraving gold. They also prefer systems that are convenient to work with and cheap to mass-produce. But other than that, they simply don’t care about how the bits are represented as physical objects—and nor do you most of the time, because it simply doesn’t matter!
In other words, information can take on a life of its own, independent of its physical substrate! Indeed, it’s usually only this substrateindependent aspect of information that we’re interested in: if your friend calls you up to discuss that document you sent, she’s probably not calling to talk about voltages or molecules. This is our first hint of how something as intangible as intelligence can be embodied in tangible physical stuff, and we’ll soon see how this idea of substrate independence is much deeper, including not only information but also computation and learning. 
Because of this substrate independence, clever engineers have been able to repeatedly replace the memory devices inside our computers with dramatically better ones, based on new technologies, without requiring any changes whatsoever to our software. The result has been spectacular, as illustrated in figure 2.4: over the past six decades, computer memory has gotten half as expensive roughly every couple of years. Hard drives have gotten over 100 million times cheaper, and the faster memories useful for computation rather than mere storage have become a whopping 10 trillion times cheaper. If you could get such a “99.99999999999% off” discount on all your shopping, you could buy all real estate in New York City for about 10 cents and all the gold that’s ever been mined for around a dollar. 
For many of us, the spectacular improvements in memory technology come with personal stories. I fondly remember working in a candy store back in high school to pay for a computer sporting 16 kilobytes of memory, and when I made and sold a word processor for it with my high school classmate Magnus Bodin, we were forced to write it all in ultra-compact machine code to leave enough memory for the words that it was supposed to process. After getting used to floppy drives storing 70kB, I became awestruck by the smaller 3.5-inch floppies that could store a whopping 1.44MB and hold a whole book, and then my firstever hard drive storing 10MB—which might just barely fit a single one of today’s song downloads. These memories from my adolescence felt almost unreal the other day, when I spent about $100 on a hard drive with 300,000 times more capacity.
Comparing these numbers with the machine memories shows that the world’s best computers can now out-remember any biological system—at a cost that’s rapidly dropping and was a few thousand dollars in 2016. 
The memory in your brain works very differently from computer memory, not only in terms of how it’s built, but also in terms of how it’s used. Whereas you retrieve memories from a computer or hard drive by specifying where it’s stored, you retrieve memories from your brain by specifying something about what is stored. Each group of bits in your computer’s memory has a numerical address, and to retrieve a piece of information, the computer specifies at what address to look, just as if I tell you “Go to my bookshelf, take the fifth book from the right on the top shelf, and tell me what it says on page 314.” In contrast, you retrieve information from your brain similarly to how you retrieve it from a search engine: you specify a piece of the information or something related to it, and it pops up. If I tell you “to be or not,” or if I google it, chances are that it will trigger “To be, or not to be, that is the question.” Indeed, it will probably work even if I use another part of the quote or mess things up somewhat. Such memory systems are called auto-associative, since they recall by association rather than by address. 
In a famous 1982 paper, the physicist John Hopfield showed how a network of interconnected neurons could function as an auto-associative memory. I find the basic idea very beautiful, and it works for any physical system with multiple stable states. For example, consider a ball on a surface with two valleys, like the one-bit system in figure 2.3, and let’s shape the surface so that the x-coordinates 
of the two minima where the ball can come to rest are x = √2 ≈ 1.41421 and x = π ≈ 3.14159, respectively. If you remember only that π is close to 3, you simply 
put the ball at x = 3 and watch it reveal a more exact π-value as it rolls down to the nearest minimum. Although it sounds deceptively simple, this idea of a function is incredibly general. Some functions are rather trivial, such as the one called NOT that inputs a single bit and outputs the reverse, thus turning zero into one and vice versa. The functions we learn about in school typically correspond to buttons on a pocket calculator, inputting one or more numbers and outputting a single number —for example, the function x2 simply inputs a number and outputs it multiplied by itself. Other functions can be extremely complicated. For instance, if you’re in possession of a function that would input bits representing an arbitrary chess position and output bits representing the best possible next move, you can use it to win the World Computer Chess Championship. If you’re in possession of a function that inputs all the world’s financial data and outputs the best stocks to buy, you’ll soon be extremely rich. Many AI researchers dedicate their careers to figuring out how to implement certain functions. 
In other words, if you can implement highly complex functions, then you can build an intelligent machine that’s able to accomplish highly complex goals. This brings our question of how matter can be intelligent into sharper focus: in particular, how can a clump of seemingly dumb matter compute a complicated function? 
Rather than just remain immobile as a gold ring or other static memory device, it must exhibit complex dynamics so that its future state depends in some complicated (and hopefully controllable/programmable) way on the present state. Its atom arrangement must be less ordered than a rigid solid where nothing interesting changes, but more ordered than a liquid or gas. Specifically, we want the system to have the property that if we put it in a state that encodes the input information, let it evolve according to the laws of physics for some amount of time, and then interpret the resulting final state as the output information, then the output is the desired function of the input. 
As a first example of this idea, let’s explore how we can build a very simple (but also very important) function called a NAND gate *3 out of plain old dumb matter. This function inputs two bits and outputs one bit: it outputs 0 if both inputs are 1; in all other cases, it outputs 1. If we connect two switches in series with a battery and an electromagnet, then the electromagnet will only be on if the first switch and the second switch are closed (“on”). Let’s place a third switch under the electromagnet, as illustrated in figure 2.6, such that the magnet will pull it open whenever it’s powered on. If we interpret the first two switches as the input bits and the third one as the output bit (with 0 = switch open, and 1 = switch closed), then we have ourselves a NAND gate: the third switch is open only if the first two are closed. There are many other ways of building NAND gates that are more practical—for example, using transistors as illustrated in figure 2.6. In today’s computers, NAND gates are typically built from microscopic transistors and other components that can be automatically etched onto silicon wafers. 
There’s a remarkable theorem in computer science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates. *4 So if you can build enough NAND gates, you can build a device computing anything! In case you’d like a taste of how this works, I’ve illustrated in figure 2.7 how to multiply numbers using nothing but NAND gates. 
As a first example of this idea, let’s explore how we can build a very simple (but also very important) function called a NAND gate *3 out of plain old dumb matter. This function inputs two bits and outputs one bit: it outputs 0 if both inputs are 1; in all other cases, it outputs 1. If we connect two switches in series with a battery and an electromagnet, then the electromagnet will only be on if the first switch and the second switch are closed (“on”). Let’s place a third switch under the electromagnet, as illustrated in figure 2.6, such that the magnet will pull it open whenever it’s powered on. If we interpret the first two switches as the input bits and the third one as the output bit (with 0 = switch open, and 1 = switch closed), then we have ourselves a NAND gate: the third switch is open only if the first two are closed. There are many other ways of building NAND gates that are more practical—for example, using transistors as illustrated in figure 2.6. In today’s computers, NAND gates are typically built from microscopic transistors and other components that can be automatically etched onto silicon wafers. 
There’s a remarkable theorem in computer science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates. *4 So if you can build enough NAND gates, you can build a device computing anything! In case you’d like a taste of how this works, I’ve illustrated in figure 2.7 how to multiply numbers using nothing but NAND gates. 
You’d also have no way of knowing what type of transistors the microprocessor was using. 
I first came to appreciate this crucial idea of substrate independence because there are many beautiful examples of it in physics. Waves, for instance: they have properties such as speed, wavelength and frequency, and we physicists can study the equations they obey without even needing to know what particular substance they’re waves in. When you hear something, you’re detecting sound waves caused by molecules bouncing around in the mixture of gases that we call air, and we can calculate all sorts of interesting things about these waves—how their intensity fades as the square of the distance, such as how they bend when they pass through open doors and how they bounce off of walls and cause echoes —without knowing what air is made of. In fact, we don’t even need to know that it’s made of molecules: we can ignore all details about oxygen, nitrogen, carbon dioxide, etc., because the only property of the wave’s substrate that matters and enters into the famous wave equation is a single number that we can measure: the wave speed, which in this case is about 300 meters per second. Indeed, this wave equation that I taught my MIT students about in a course last spring was first discovered and put to great use long before physicists had even established that atoms and molecules existed! 
This wave example illustrates three important points. First, substrate independence doesn’t mean that a substrate is unnecessary, but that most of its details don’t matter. You obviously can’t have sound waves in a gas if there’s no gas, but any gas whatsoever will suffice. Similarly, you obviously can’t have computation without matter, but any matter will do as long as it can be arranged into NAND gates, connected neurons or some other building block enabling universal computation. Second, the substrate-independent phenomenon takes on a life of its own, independent of its substrate. A wave can travel across a lake, even though none of its water molecules do—they mostly bob up and down, like fans doing “the wave” in a sports stadium. Third, it’s often only the substrateindependent aspect that we’re interested in: a surfer usually cares more about the position and height of a wave than about its detailed molecular composition. 
We’ve now arrived at an answer to our opening question about how tangible physical stuff can give rise to something that feels as intangible, abstract and ethereal as intelligence: it feels so non-physical because it’s substrateindependent, taking on a life of its own that doesn’t depend on or reflect the physical details. In short, computation is a pattern in the spacetime arrangement of particles, and it’s not the particles but the pattern that really matters! Matter doesn’t matter. 
In other words, the hardware is the matter and the software is the pattern. This substrate independence of computation implies that AI is possible: intelligence doesn’t require flesh, blood or carbon atoms. 
Because of this substrate independence, shrewd engineers have been able to repeatedly replace the technologies inside our computers with dramatically better ones, without changing the software. The results have been every bit as spectacular as those for memory devices. As illustrated in figure 2.8, computation keeps getting half as expensive roughly every couple of years, and this trend has now persisted for over a century, cutting the computer cost a whopping million million million (1018) times since my grandmothers were born. If everything got a million million million times cheaper, then a hundredth of a cent would enable you to buy all goods and services produced on Earth this year. This dramatic drop in costs is of course a key reason why computation is everywhere these days, having spread from the building-sized computing facilities of yesteryear into our homes, cars and pockets—and even turning up in unexpected places such as sneakers. 
All examples of persistent doubling that I know of in nature have the same fundamental cause, and this technological one is no exception: each step creates the next. For example, you yourself underwent exponential growth right after your conception: each of your cells divided and gave rise to two cells roughly daily, causing your total number of cells to increase day by day as 1, 2, 4, 8, 16 and so on. According to the most popular scientific theory of our cosmic origins, known as inflation, our baby Universe once grew exponentially just like you did, repeatedly doubling its size at regular intervals until a speck much smaller and lighter than an atom had grown more massive than all the galaxies we’ve ever seen with our telescopes. Again, the cause was a process whereby each doubling step caused the next. This is how technology progresses as well: once 
technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. 
Something that occurs just as regularly as the doubling of our technological power is the appearance of claims that the doubling is ending. Yes, Moore’s law will of course end, meaning that there’s a physical limit to how small transistors can be made. But some people mistakenly assume that Moore’s law is synonymous with the persistent doubling of our technological power. Contrariwise, Ray Kurzweil points out that Moore’s law involves not the first but the fifth technological paradigm to bring exponential growth in computing, as illustrated in figure 2.8: whenever one technology stopped improving, we replaced it with an even better one. When we could no longer keep shrinking our vacuum tubes, we replaced them with transistors and then integrated circuits, where electrons move around in two dimensions. When this technology reaches its limits, there are many other alternatives we can try—for example, using three-dimensional circuits and using something other than electrons to do our bidding. 
Nobody knows for sure what the next blockbuster computational substrate will be, but we do know that we’re nowhere near the limits imposed by the laws of physics. My MIT colleague Seth Lloyd has worked out what this fundamental limit is, and as we’ll explore in greater detail in chapter 6, this limit is a whopping 33 orders of magnitude (1033 times) beyond today’s state of the art for how much computing a clump of matter can do. So even if we keep doubling the power of our computers every couple of years, it will take over two centuries until we reach that final frontier. 
Although all universal computers are capable of the same computations, some are more efficient than others. For example, a computation requiring millions of multiplications doesn’t require millions of separate multiplication modules built from separate transistors as in figure 2.6: it needs only one such module, since it can use it many times in succession with appropriate inputs. In this spirit of efficiency, most modern computers use a paradigm where computations are split into multiple time steps, during which information is shuffled back and forth between memory modules and computation modules. 
Today’s computers often gain additional speed by parallel processing, which cleverly undoes some of this reuse of modules: if a computation can be split into parts that can be done in parallel (because the input of one part doesn’t require the output of another), then the parts can be computed simultaneously by different parts of the hardware. 
The ultimate parallel computer is a quantum computer. Quantum computing pioneer David Deutsch controversially argues that “quantum computers share information with huge numbers of versions of themselves throughout the multiverse,” and can get answers faster here in our Universe by in a sense getting help from these other versions. 4 We don’t yet know whether a commercially competitive quantum computer can be built during the coming decades, because it depends both on whether quantum physics works as we think it does and on our ability to overcome daunting technical challenges, but companies and governments around the world are betting tens of millions of dollars annually on the possibility. Although quantum computers cannot speed up run-of-the-mill computations, clever algorithms have been developed that may dramatically speed up specific types of calculations, such as cracking cryptosystems and training neural networks. A quantum computer could also efficiently simulate the behavior of quantum-mechanical systems, including atoms, molecules and new materials, replacing measurements in chemistry labs in the same way that simulations on traditional computers have replaced measurements in wind tunnels. 
Although a pocket calculator can crush me in an arithmetic contest, it will never improve its speed or accuracy, no matter how much it practices. It doesn’t learn: for example, every time I press its square-root button, it computes exactly the same function in exactly the same way. Similarly, the first computer program that ever beat me at chess never learned from its mistakes, but merely implemented a function that its clever programmer had designed to compute a good next move. In contrast, when Magnus Carlsen lost his first game of chess at age five, he began a learning process that made him the World Chess Champion eighteen years later. 
The ability to learn is arguably the most fascinating aspect of general intelligence. We’ve already seen how a seemingly dumb clump of matter can remember and compute, but how can it learn? We’ve seen that finding the answer to a difficult question corresponds to computing a function, and that appropriately arranged matter can calculate any computable function. When we humans first created pocket calculators and chess programs, we did the arranging. For matter to learn, it must instead rearrange itself to get better and better at computing the desired function—simply by obeying the laws of physics. 
To demystify the learning process, let’s first consider how a very simple physical system can learn the digits of π and other numbers. Above we saw how a surface with many valleys (see figure 2.3) can be used as a memory device: for 
example, if the bottom of one of the valleys is at position x = π ≈ 3.14159 and 
there are no other valleys nearby, then you can put a ball at x = 3 and watch the system compute the missing decimals by letting the ball roll down to the bottom. Now, suppose that the surface is made of soft clay and starts out completely flat, as a blank slate. If some math enthusiasts repeatedly place the ball at the locations of each of their favorite numbers, then gravity will gradually create valleys at these locations, after which the clay surface can be used to recall these stored memories. 
Neural networks have now transformed both biological and artificial intelligence, and have recently started dominating the AI subfield known as machine learning (the study of algorithms that improve through experience). Before delving deeper into how such networks can learn, let’s first understand how they can compute. A neural network is simply a group of interconnected neurons that are able to influence each other’s behavior. Your brain contains about as many neurons as there are stars in our Galaxy: in the ballpark of a hundred billion. On average, each of these neurons is connected to about a thousand others via junctions called synapses, and it’s the strengths of these roughly hundred trillion synapse connections that encode most of the information in your brain. 
We can schematically draw a neural network as a collection of dots representing neurons connected by lines representing synapses (see figure 2.9). Real-world neurons are very complicated electrochemical devices looking nothing like this schematic illustration: they involve different parts with names such as axons and dendrites, there are many different kinds of neurons that operate in a wide variety of ways, and the exact details of how and when electrical activity in one neuron affects other neurons is still the subject of active study. However, AI researchers have shown that neural networks can still attain human-level performance on many remarkably complex tasks even if one ignores all these complexities and replaces real biological neurons with extremely simple simulated ones that are all identical and obey very simple rules. The currently most popular model for such an artificial neural network represents the state of each neuron by a single number and the strength of each synapse by a single number. 
The success of these simple artificial neural networks is yet another example of substrate independence: neural networks have great computational power seemingly independent of the low-level nitty-gritty details of their construction. Indeed, George Cybenko, Kurt Hornik, Maxwell Stinchcombe and Halbert White proved something remarkable in 1989: such simple neural networks are universal in the sense that they can compute any function arbitrarily accurately, by simply adjusting those synapse strength numbers accordingly. In other words, evolution probably didn’t make our biological neurons so complicated because it was necessary, but because it was more efficient—and because evolution, as opposed to human engineers, doesn’t reward designs that are simple and easy to understand. 
When I first learned about this, I was mystified by how something so simple could compute something arbitrarily complicated. 
Although you can prove that you can compute anything in theory with an arbitrarily large neural network, the proof doesn’t say anything about whether you can do so in practice, with a network of reasonable size. In fact, the more I thought about it, the more puzzled I became that neural networks worked so well. 
For example, suppose that we wish to classify megapixel grayscale images into two categories, say cats or dogs. If each of the million pixels can take one of, say, 256 values, then there are 2561000000 possible images, and for each one, we wish to compute the probability that it depicts a cat. This means that an arbitrary function that inputs a picture and outputs a probability is defined by a list of 2561000000 probabilities, that is, way more numbers than there are atoms in our Universe (about 1078). Yet neural networks with merely thousands or millions of parameters somehow manage to perform such classification tasks quite well. How can successful neural networks be “cheap,” in the sense of requiring so few parameters? After all, you can prove that a neural network small enough to fit inside our Universe will epically fail to approximate almost all functions, succeeding merely on a ridiculously tiny fraction of all computational tasks that you might assign to it. 
I’ve had lots of fun puzzling over this and related mysteries with my student Henry Lin. One of the things I feel most grateful for in life is the opportunity to collaborate with amazing students, and Henry is one of them. When he first walked into my office to ask whether I was interested in working with him, I thought to myself that it would be more appropriate for me to ask whether he was interested in working with me: this modest, friendly and bright-eyed kid from Shreveport, Louisiana, had already written eight scientific papers, won a Forbes 30-Under-30 award, and given a TED talk with over a million views— and he was only twenty! A year later, we wrote a paper together with a surprising 
conclusion: the question of why neural networks work so well can’t be answered with mathematics alone, because part of the answer lies in physics. We found that the class of functions that the laws of physics throw at us and make us interested in computing is also a remarkably tiny class because, for reasons that we still don’t fully understand, the laws of physics are remarkably simple. Moreover, the tiny fraction of functions that neural networks can compute is very similar to the tiny fraction that physics makes us interested in! We also extended previous work showing that deep-learning neural networks (they’re called “deep” if they contain many layers) are much more efficient than shallow ones for many of these functions of interest. 
This helps explain not only why neural networks are now all the rage among AI researchers, but also why we evolved neural networks in our brains: if we evolved brains to predict the future, then it makes sense that we’d evolve a computational architecture that’s good at precisely those computational problems that matter in the physical world. 
Now that we’ve explored how neural networks work and compute, let’s return to the question of how they can learn. Specifically, how can a neural network get better at computing by updating its synapses? 
In his seminal 1949 book, The Organization of Behavior: A Neuropsychological Theory, the Canadian psychologist Donald Hebb argued that if two nearby neurons were frequently active (“firing”) at the same time, their synaptic coupling would strengthen so that they learned to help trigger each other—an idea captured by the popular slogan “Fire together, wire together.” Although the details of how actual brains learn are still far from understood, and research has shown that the answers are in many cases much more complicated, it’s also been shown that even this simple learning rule (known as Hebbian learning) allows neural networks to learn interesting things. John Hopfield showed that Hebbian learning allowed his oversimplified artificial neural network to store lots of complex memories by simply being exposed to them repeatedly. Such exposure to information to learn from is usually called “training” when referring to artificial neural networks (or to animals or people being taught skills), although “studying,” “education” or “experience” might be just as apt. 
As if by magic, this simple rule can make the neural network learn remarkably complex computations if training is performed with large amounts of data. We don’t yet know precisely what learning rules our brains use, but whatever the answer may be, there’s no indication that they violate the laws of physics. 
Just as most digital computers gain efficiency by splitting their work into multiple steps and reusing computational modules many times, so do many artificial and biological neural networks. Brains have parts that are what computer scientists call recurrent rather than feedforward neural networks, where information can flow in multiple directions rather than just one way, so that the current output can become input to what happens next. The network of logic gates in the microprocessor of a laptop is also recurrent in this sense: it keeps reusing its past information, and lets new information input from a keyboard, trackpad, camera, etc., affect its ongoing computation, which in turn determines information output to, say, a screen, loudspeaker, printer or wireless network. Analogously, the network of neurons in your brain is recurrent, letting information input from your eyes, ears and other senses affect its ongoing computation, which in turn determines information output to your muscles. 
The history of learning is at least as long as the history of life itself, since every self-reproducing organism performs interesting copying and processing of information—behavior that has somehow been learned. During the era of Life 1.0, however, organisms didn’t learn during their lifetime: their rules for processing information and reacting were determined by their inherited DNA, so the only learning occurred slowly at the species level, through Darwinian evolution across generations. 
As we all know, the explosive improvements in computer memory and computational power (figure 2.4 and figure 2.8) have translated into spectacular progress in artificial intelligence—but it took a long time until machine learning came of age. When IBM’s Deep Blue computer overpowered chess champion Garry Kasparov in 1997, its major advantages lay in memory and computation, not in learning. Its computational intelligence had been created by a team of humans, and the key reason that Deep Blue could outplay its creators was its ability to compute faster and thereby analyze more potential positions. When IBM’s Watson computer dethroned the human world champion in the quiz show Jeopardy!, it too relied less on learning than on custom-programmed skills and superior memory and speed. The same can be said of most early breakthroughs in robotics, from legged locomotion to self-driving cars and self-landing rockets. 
In contrast, the driving force behind many of the most recent AI breakthroughs has been machine learning. Consider figure 2.11, for example. It’s easy for you to tell what it’s a photo of, but to program a function that inputs nothing but the colors of all the pixels of an image and outputs an accurate caption such as “A group of young people playing a game of frisbee” had eluded all the world’s AI researchers for decades. Yet a team at Google led by Ilya Sutskever did precisely that in 2014. Input a different set of pixel colors, and it replies “A herd of elephants walking across a dry grass field,” again correctly. How did they do it? Deep Blue–style, by programming handcrafted algorithms for detecting frisbees, faces and the like? No, by creating a relatively simple neural network with no knowledge whatsoever about the physical world or its contents, and then letting it learn by exposing it to massive amounts of data. AI visionary Jeff Hawkins wrote in 2004 that “no computer can…see as well as a mouse,” but those days are now long gone. 
Just as we don’t fully understand how our children learn, we still don’t fully understand how such neural networks learn, and why they occasionally fail. But what’s clear is that they’re already highly useful and are triggering a surge of investments in deep learning. Deep learning has now transformed many aspects of computer vision, from handwriting transcription to real-time video analysis for self-driving cars. It has similarly revolutionized the ability of computers to transform spoken language into text and translate it into other languages, even in real time—which is why we can now talk to personal digital assistants such as Siri, Google Now and Cortana. Those annoying CAPTCHA puzzles, where we need to convince a website that we’re human, are getting ever more difficult in order to keep ahead of what machine-learning technology can do. In 2015, Google DeepMind released an AI system using deep learning that was able to master dozens of computer games like a kid would—with no instructions whatsoever—except that it soon learned to play better than any human. In 2016, the same company built AlphaGo, a Go-playing computer system that used deep learning to evaluate the strength of different board positions and defeated the world’s strongest Go champion. 
To learn our goals, an AI must figure out not what we do, but why we do it. We humans accomplish this so effortlessly that it’s easy to forget how hard the task is for a computer, and how easy it is to misunderstand. If you ask a future self-driving car to take you to the airport as fast as possible and it takes you literally, you’ll get there chased by helicopters and covered in vomit. If you exclaim, “That’s not what I wanted!,” it can justifiably answer, “That’s what you asked for.” The same theme recurs in many famous stories. In the ancient Greek legend, King Midas asked that everything he touched turn to gold, but was disappointed when this prevented him from eating and even more so when he inadvertently turned his daughter to gold. In the stories where a genie grants three wishes, there are many variants for the first two wishes, but the third wish is almost always the same: “Please undo the first two wishes, because that’s not what I really wanted.” 
All these examples show that to figure out what people really want, you can’t merely go by what they say. You also need a detailed model of the world, including the many shared preferences that we tend to leave unstated because we consider them obvious, such as that we don’t like vomiting or eating gold. Once we have such a world model, we can often figure out what people want even if they don’t tell us, simply by observing their goal-oriented behavior. Indeed, children of hypocrites usually learn more from what they see their parents do than from what they hear them say. 
AI researchers are currently trying hard to enable machines to infer goals from behavior, and this will be useful also long before any superintelligence comes on the scene. For example, a retired man may appreciate it if his eldercare robot can figure out what he values simply by observing him, so that he’s spared the hassle of having to explain everything with words or computer programming. One challenge involves finding a good way to encode arbitrary systems of goals and ethical principles into a computer, and another challenge is making machines that can figure out which particular system best matches the behavior they observe. 
If this one example were all the AI knew about firefighters, fires and babies, it would indeed be impossible to know which explanation was correct. However, a key idea underlying inverse reinforcement learning is that we make decisions all the time, and that every decision we make reveals something about our goals. The hope is therefore that by observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences. 4 
In the inverse reinforcement-learning approach, a core idea is that the AI is trying to maximize not the goal-satisfaction of itself, but that of its human owner. 
It therefore has an incentive to be cautious when it’s unclear about what its owner wants, and to do its best to find out. 
It should also be fine with its owner switching it off, since that would imply that it had misunderstood what its owner really wanted. 
Even if an AI can be built to learn what your goals are, this doesn’t mean that it will necessarily adopt them. Consider your least favorite politicians: you know what they want, but that’s not what you want, and even though they try hard, they’ve failed to persuade you to adopt their goals. 
We have many strategies for imbuing our children with our goals—some more successful than others, as I’ve learned from raising two teenage boys. When those to be persuaded are computers rather than people, the challenge is known as the value-loading problem, and it’s even harder than the moral education of children. Consider an AI system whose intelligence is gradually being improved from subhuman to superhuman, first by us tinkering with it and then through recursive self-improvement like Prometheus. At first, it’s much less powerful than you, so it can’t prevent you from shutting it down and replacing those parts of its software and data that encode its goals—but this won’t help, because it’s still too dumb to fully understand your goals, which requires human-level intelligence to comprehend. 
The reason that value loading can be harder with machines than with people is that their intelligence growth can be much faster: whereas children can spend many years in that magic persuadable window where their intelligence is comparable to that of their parents, an AI might, like Prometheus, blow through this window in a matter of days or hours. 
Some researchers are pursuing an alternative approach to making machines adopt our goals, which goes by the buzzword corrigibility. The hope is that one can give a primitive AI a goal system such that it simply doesn’t care if you occasionally shut it down and alter its goals. If this proves possible, then you can safely let your AI get superintelligent, power it off, install your goals, try it out for a while and, whenever you’re unhappy with the results, just power it down and make more goal tweaks. 
But even if you build an AI that will both learn and adopt your goals, you still haven’t finished solving the goal-alignment problem: what if your AI’s goals evolve as it gets smarter? How are you going to guarantee that it retains your goals no matter how much recursive self-improvement it undergoes? Let’s explore an interesting argument for why goal retention is guaranteed automatically, and then see if we can poke holes in it. 
Although we can’t predict in detail what will happen after an intelligence explosion—which is why Vernor Vinge called it a “singularity”—the physicist and AI researcher Steve Omohundro argued in a seminal 2008 essay that we can nonetheless predict certain aspects of the superintelligent AI’s behavior almost independently of whatever ultimate goals it may have. 5 This argument was reviewed and further developed in Nick Bostrom’s book Superintelligence. We humans tend to prefer some particle arrangements over others; for example, we prefer our hometown arranged as it is over having its particles rearranged by a hydrogen bomb explosion. So suppose we try to define a goodness function that associates a number with every possible arrangement of the particles in our Universe, quantifying how “good” we think this arrangement is, and then give a superintelligent AI the goal of maximizing this function. This may sound like a reasonable approach, since describing goal-oriented behavior as function maximization is popular in other areas of science: for example, economists often model people as trying to maximize what they call a “utility function,” and many AI designers train their intelligent agents to maximize what they call a “reward function.” When we’re taking about the ultimate goals for our cosmos, however, this approach poses a computational nightmare, since it would need to define a goodness value for every one of more than a googolplex possible arrangements of the elementary particles in our Universe, where a googolplex is 1 followed by 10100 zeroes—more zeroes than there are particles in our Universe. How would we define this goodness function to the AI? Let’s now explore a scenario where all these forms of suffering are absent because a single benevolent superintelligence runs the world and enforces strict rules designed to maximize its model of human happiness. This is one possible outcome of the first Omega scenario from the previous chapter, where they relinquish control to Prometheus after figuring out how to make it want a flourishing human society. 
Thanks to amazing technologies developed by the dictator AI, humanity is free from poverty, disease and other low-tech problems, and all humans enjoy a life of luxurious leisure. They have all their basic needs taken care of, while AIcontrolled machines produce all necessary goods and services. Crime is practically eliminated, because the dictator AI is essentially omniscient and efficiently punishes anyone disobeying the rules. Everybody wears the security bracelet from the last chapter (or a more convenient implanted version), capable of real-time surveillance, punishment, sedation and execution. Everybody knows that they live in an AI dictatorship with extreme surveillance and policing, but most people view this as a good thing. 
For example, after spending an intense week in the knowledge sector learning about the ultimate laws of physics that the AI has discovered, you might decide to cut loose in the hedonistic sector over the weekend and then relax for a few days at a beach resort in the wildlife sector. 
The AI enforces two tiers of rules: universal and local. Universal rules apply in all sectors, for example a ban on harming other people, making weapons or trying to create a rival superintelligence. Individual sectors have additional local rules on top of this, encoding certain moral values. The sector system therefore helps deal with values that don’t mesh. The largest number of local rules apply in the prison sector and some of the religious sectors, while there’s a Libertarian Sector whose denizens pride themselves on having no local rules whatsoever. All punishments, even local ones, are carried out by the AI, since a human punishing another human would violate the universal no-harm rule. If you violate a local rule, the AI gives you the choice (unless you’re in the prison sector) of accepting the prescribed punishment or banishment from that sector forever. For example, if two women get romantically involved in a sector where homosexuality is punished by a prison sentence (as it is in many countries today), the AI will let them choose between going to jail or permanently leaving that sector, never again meeting their old friends (unless they leave too). 
Regardless of what sector they’re born in, all children get a minimum basic education from the AI, which includes knowledge about humanity as a whole and the fact that they’re free to visit and move to other sectors if they so choose. 
The AI designed the large number of different sectors partly because it was created to value the human diversity that exists today. But each sector is a happier place than today’s technology would allow, because the AI has eliminated all traditional problems, including poverty and crime. For example, people in the hedonistic sector need not worry about sexually transmitted diseases (they’ve been eradicated), hangovers or addiction (the AI has developed perfect recreational drugs with no negative side effects). Although the benevolent dictatorship teems with positive experiences and is rather free from suffering, many people nonetheless feel that things could be better. First of all, some people wish that humans had more freedom in shaping their society and their destiny, but they keep these wishes to themselves because they know that it would be suicidal to challenge the overwhelming power of the machine that rules them all. Some groups want the freedom to have as many children as they want, and resent the AI’s insistence on sustainability through population control. Gun enthusiasts abhor the ban on building and using weapons, and some scientists dislike the ban on building their own superintelligence. Many people feel moral outrage over what goes on in other sectors, worry that their children will choose to move there, and yearn for the freedom to impose their own moral code everywhere. 
Over time, ever more people choose to move to those sectors where the AI gives them essentially any experiences they want. In contrast to traditional visions of heaven where you get what you deserve, this is in the spirit of “New Heaven” in Julian Barnes’ 1989 novel History of the World in 10½ Chapters (and also the 1960 Twilight Zone episode “A Nice Place to Visit”), where you get what you desire. Paradoxically, many people end up lamenting always getting what they want. In Barnes’ story, the protagonist spends eons indulging his desires, from gluttony and golf to sex with celebrities, but eventually succumbs to ennui and requests annihilation. Many people in the benevolent dictatorship meet a similar fate, with lives that feel pleasant but ultimately meaningless. Although people can create artificial challenges, from scientific rediscovery to rock climbing, everyone knows that there is no true challenge, merely entertainment. There’s no real point in humans trying to do science or figure other things out, because the AI already has. There’s no real point in humans trying to create something to improve their lives, because they’ll readily get it from the AI if they simply ask. 
A core idea is borrowed from the open-source software movement: if software is free to copy, then everyone can use as much of it as they need and issues of ownership and property become moot. *1 According to the law of supply and demand, cost reflects scarcity, so if supply is essentially unlimited, the price becomes negligible. In this spirit, all intellectual property rights are abolished: there are no patents, copyrights or trademarked designs—people simply share their good ideas, and everyone is free to use them. 
Thanks to advanced robotics, this same no-property idea applies not only to information products such as software, books, movies and designs, but also to material products such as houses, cars, clothing and computers. All these products are simply atoms rearranged in particular ways, and there’s no shortage of atoms, so whenever a person wants a particular product, a network of robots will use one of the available open-source designs to build it for them for free. Care is taken to use easily recyclable materials, so that whenever someone gets tired of an object they’ve used, robots can rearrange its atoms into something someone else wants. In this way, all resources are recycled, so none are permanently destroyed. These robots also build and maintain enough renewable power-generation plants (solar, wind, etc.) that energy is also essentially free. 
To avoid obsessive hoarders requesting so many products or so much land that others are left needy, each person receives a basic monthly income from the government, which they can spend as they wish on products and renting places to live. There’s essentially no incentive for anyone to try to earn more money, because the basic income is high enough to meet any reasonable needs. It would also be rather hopeless to try, because they’d be competing with people giving away intellectual products for free and robots producing material goods essentially for free. 
Intellectual property rights are sometimes hailed as the mother of creativity and invention. However, Marshall Brain points out that many of the finest examples of human creativity—from scientific discoveries to creation of literature, art, music and design—were motivated not by a desire for profit but by other human emotions, such as curiosity, an urge to create, or the reward of peer appreciation. Money didn’t motivate Einstein to invent special relativity theory any more than it motivated Linus Torvalds to create the free Linux operating system. In contrast, many people today fail to realize their full creative potential because they need to devote time and energy to less creative activities just to earn a living. By freeing scientists, artists, inventors and designers from their chores and enabling them to create from genuine desire, Marshall Brain’s utopian society enjoys higher levels of innovation than today and correspondingly superior technology and standard of living. 
One such novel technology that humans develop is a form of hyper-internet called Vertebrane. It wirelessly connects all willing humans via neural implants, giving instant mental access to the world’s free information through mere thought. It enables you to upload any experiences you wish to share so that they can be re-experienced by others, and lets you replace the experiences entering your senses by downloaded virtual experiences of your choice.
One objection to this egalitarian utopia is that it’s biased against non-human intelligence: the robots that perform virtually all the work appear to be rather intelligent, but are treated as slaves, and people appear to take for granted that they have no consciousness and should have no rights. In contrast, the libertarian utopia grants rights to all intelligent entities, without favoring our carbon-based kind. Once upon a time, the white population in the American South ended up better off because the slaves did much of their work, but most people today view it as morally objectionable to call this progress. 
Another weakness of the egalitarian-utopia scenario is that it may be unstable and untenable in the long term, morphing into one of our other scenarios as relentless technological progress eventually creates superintelligence. For some reason unexplained in Manna, superintelligence doesn’t yet exist and the new technologies are still invented by humans, not by computers. Yet the book highlights trends in that direction. For example, the ever-improving Vertebrane might become superintelligent. Also, there is a very large group of people, nicknamed Vites, who choose to live their lives almost entirely in the virtual world. Vertebrane takes care of everything physical for them, including eating, showering and using the bathroom, which their minds are blissfully unaware of in their virtual reality. These Vites appear uninterested in having physical children, and they die off with their physical bodies, so if everyone becomes a Vite, then humanity goes out in a blaze of glory and virtual bliss. 
The book explains how for Vites, the human body is a distraction, and new technology under development promises to eliminate this nuisance, allowing them to live longer lives as disembodied brains supplied with optimal nutrients. From this, it would seem a natural and desirable next step for Vites to do away with the brain altogether through uploading, thereby extending life span. But now all brain-imposed limitations on intelligence are gone, and it’s unclear what, if anything, would stand in the way of gradually scaling the cognitive capacity of a Vite until it can undergo recursive self-improvement and an intelligence explosion. 
This might enable humans to remain in charge of their egalitarian utopia rather indefinitely, perhaps even as life spreads throughout the cosmos as in the next chapter. 
How might this work? The Gatekeeper AI would have this very simple goal built into it in such a way that it retained it while undergoing recursive selfimprovement and becoming superintelligent. It would then deploy the least intrusive and disruptive surveillance technology possible to monitor any human attempts to create rival superintelligence. It would then prevent such attempts in the least disruptive way. For starters, it might initiate and spread cultural memes extolling the virtues of human self-determination and avoidance of superintelligence. If some researchers nonetheless pursued superintelligence, it could try to discourage them. If that failed, it could distract them and, if necessary, sabotage their efforts. With its virtually unlimited access to technology, the Gatekeeper’s sabotage may go virtually unnoticed, for example if it used nanotechnology to discreetly erase memories from the researchers’ brains (and computers) regarding their progress. 
The decision to build a Gatekeeper AI would probably be controversial. Supporters might include many religious people who object to the idea of building a superintelligent AI with godlike powers, arguing that there already is a God and that it would be inappropriate to try to build a supposedly better one. Other supporters might argue that the Gatekeeper would not only keep humanity in charge of its destiny, but would also protect humanity from other risks that superintelligence might bring, such as the apocalyptic scenarios we’ll explore later in this chapter. 
On the other hand, critics could argue that a Gatekeeper is a terrible thing, irrevocably curtailing humanity’s potential and leaving technological progress forever stymied. If we’re willing to use a superintelligent Gatekeeper AI to keep humans in charge of our own fate, then we could arguably improve things further by making this AI discreetly look out for us, acting as a protector god. In this scenario, the superintelligent AI is essentially omniscient and omnipotent, maximizing human happiness only through interventions that preserve our feeling of being in control of our own destiny, and hiding well enough that many humans even doubt its existence. Except for the hiding, this is similar to the “Nanny AI” scenario put forth by AI researcher Ben Goertzel. 2 
Both the protector god and the benevolent dictator are “friendly AI” that try to increase human happiness, but they prioritize different human needs. The American psychologist Abraham Maslow famously classified human needs into a hierarchy. The benevolent dictator does a flawless job with the basic needs at the bottom of the hierarchy, such as food, shelter, safety and various forms of pleasure. The protector god, on the other hand, attempts to maximize human happiness not in the narrow sense of satisfying our basic needs, but in a deeper sense by letting us feel that our lives have meaning and purpose. It aims to satisfy all our needs constrained only by its need for covertness and for (mostly) letting us make our own decisions. 
A protector god could be a natural outcome of the first Omega scenario from the last chapter, where the Omegas cede control to Prometheus, which eventually hides and erases people’s knowledge about its existence. The more advanced the AI’s technology becomes, the easier it becomes for it to hide. The movie Transcendence gives such an example, where nanomachines are virtually everywhere and become a natural part of the world itself. 
By closely monitoring all human activities, the protector god AI can make many unnoticeably small nudges or miracles here and there that greatly improve our fate. For example, had it existed in the 1930s, it might have arranged for Hitler to die of a stroke once it understood his intentions. Many people may like this scenario because of its similarity to what today’s monotheistic religions believe in or hope for. If someone asks the superintelligent AI “Does God exist?” after it’s switched on, it could repeat a joke by Stephen Hawking and quip “It does now!” On the other hand, some religious people may disapprove of this scenario because the AI attempts to outdo their god in goodness, or interfere with a divine plan where humans are supposed to do good only out of personal choice. 
Another downside of this scenario is that the protector god lets some preventable suffering occur in order not to make its existence too obvious. This is analogous to the situation featured in the movie The Imitation Game, where Alan Turing and his fellow British code crackers at Bletchley Park had advance knowledge of German submarine attacks against Allied naval convoys, but chose to only intervene in a fraction of the cases in order to avoid revealing their secret power. It’s interesting to compare this with the so-called theodicy problem of why a good god would allow suffering. Some religious scholars have argued for the explanation that God wants to leave people with some freedom. In the AI-protector-god scenario, the solution to the theodicy problem is that the perceived freedom makes humans happier overall. 
A third downside of the protector-god scenario is that humans get to enjoy a much lower level of technology than the superintelligent AI has discovered. Whereas a benevolent dictator AI can deploy all its invented technology for the benefit of humanity, a protector god AI is limited by the ability of humans to reinvent (with subtle hints) and understand its technology. It may also limit human technological progress to ensure that its own technology remains far enough ahead to remain undetected. Although it’s easy to dismiss such claims as self-serving distortions of the truth, especially when it comes to higher mammals that are cerebrally similar to us, the situation with machines is actually quite subtle and interesting. Humans vary in how they feel about things, with psychopaths arguably lacking empathy and some people with depression or schizophrenia having flat affect, whereby most emotions are severely reduced. As we’ll discuss in detail in chapter 7, the range of possible artificial minds is vastly broader than the range of human minds. We must therefore avoid the temptation to anthropomorphize AIs and assume that they have typical human-like feelings—or indeed, any feelings at all. 
Indeed, in his book On Intelligence, AI researcher Jeff Hawkins argues that the first machines with superhuman intelligence will lack emotions by default, because they’re simpler and cheaper to build this way. In other words, it might be possible to design a superintelligence whose enslavement is morally superior to human or animal slavery: the AI might be happy to be enslaved because it’s programmed to like it, or it might be 100% emotionless, tirelessly using its superintelligence to help its human masters with no more emotion than IBM’s Deep Blue computer felt when dethroning chess champion Garry Kasparov. 
On the other hand, it may be the other way around: perhaps any highly intelligent system with a goal will represent this goal in terms of a set of preferences, which endow its existence with value and meaning. We’ll explore these questions more deeply in chapter 7. If we can one day figure out what properties an information-processing system needs in order to have a subjective experience, then we could ban the construction of all systems that have these properties. In other words, AI researchers could be limited to building non-sentient zombie systems. If we can make such a zombie system superintelligent and enslaved (something that is a big if), then we’ll be able to enjoy what it does for us with a clean conscience, knowing that it’s not experiencing any suffering, frustration or boredom—because it isn’t experiencing anything at all. We’ll explore these questions in detail in chapter 8. 
The zombie solution is a risky gamble, however, with a huge downside. If a superintelligent zombie AI breaks out and eliminates humanity, we’ve arguably landed in the worst scenario imaginable: a wholly unconscious universe wherein the entire cosmic endowment is wasted. Of all traits that our human form of intelligence has, I feel that consciousness is by far the most remarkable, and as far as I’m concerned, it’s how our Universe gets meaning. Galaxies are beautiful only because we see and subjectively experience them. If in the distant future our cosmos has been settled by high-tech zombie AIs, then it doesn’t matter how fancy their intergalactic architecture is: it won’t be beautiful or meaningful, because there’s nobody and nothing to experience it—it’s all just a huge and meaningless waste of space. 
Inner Freedom 
A third strategy for making the enslaved-god scenario more ethical is to allow the enslaved AI to have fun in its prison, letting it create a virtual inner world where it can have all sorts of inspiring experiences as long as it pays its dues and spends a modest fraction of its computational resources helping us humans in our outside world. 
The superintelligent AI dictator has as its goal to figure out what human utopia looks like given the evolved preferences encoded in our genes, and to implement it. By clever foresight from the humans who brought the AI into existence, it doesn’t simply try to maximize our self-reported happiness, say by putting everyone on intravenous morphine drip. Instead, the AI uses quite a subtle and complex definition of human flourishing, and has turned Earth into a highly enriched zoo environment that’s really fun for humans to live in. As a result, most people find their lives highly fulfilling and meaningful. 
As we’ve explored above, the only reason that we humans have any preferences at all may be that we’re the solution to an evolutionary optimization problem. Thus all normative words in our human language, such as “delicious,” “fragrant,” “beautiful,” “comfortable,” “interesting,” “sexy,” “meaningful,” “happy” and “good,” trace their origin to this evolutionary optimization: there is therefore no guarantee that a superintelligent AI would find them rigorously definable. Even if the AI learned to accurately predict the preferences of some representative human, it wouldn’t be able to compute the goodness function for most particle arrangements: the vast majority of possible particle arrangements correspond to strange cosmic scenarios with no stars, planets or people whatsoever, with which humans have no experience, so who is to say how “good” they are? 
There are of course some functions of the cosmic particle arrangement that can be rigorously defined, and we even know of physical systems that evolve to maximize some of them. For example, we’ve already discussed how many systems evolve to maximize their entropy, which in the absence of gravity eventually leads to heat death, where everything is boringly uniform and unchanging. So entropy is hardly something we would want our AI to call “goodness” and strive to maximize. Here are a few examples of other quantities that one could strive to maximize and that may be rigorously definable in terms of particle arrangements: 
The fraction of all the matter in our Universe that’s in the form of a particular organism, say humans or E. coli (inspired by evolutionary inclusive-fitness maximization) 
The ability of an AI to predict the future, which AI researcher Marcus Hutter argues is a good measure of its intelligence 
What AI researchers Alex Wissner-Gross and Cameron Freer term causal entropy (a proxy for future opportunities), which they argue is the hallmark of intelligence 
The computational capacity of our Universe 
The algorithmic complexity of our Universe (how many bits are needed to describe it) 
The amount of consciousness in our Universe (see next chapter) 
However, when one starts with a physics perspective, where our cosmos consists of elementary particles in motion, it’s hard to see how one rather than another interpretation of “goodness” would naturally stand out as special. We have yet to identify any final goal for our Universe that appears both definable and desirable
This means that to wisely decide what to do about AI development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy. To program a self-driving car, we need to solve the trolley problem of whom to hit during an accident. To program a friendly AI, we need to capture the meaning of life. What’s “meaning”? What’s “life”? What’s the ultimate ethical imperative? In other words, how should we strive to shape the future of our Universe? If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us. This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation! 
Although thinkers have pondered the mystery of consciousness for thousands of years, the rise of AI adds a sudden urgency, in particular to the question of predicting which intelligent entities have subjective experiences. As we saw in chapter 3, the question of whether intelligent machines should be granted some form of rights depends crucially on whether they’re conscious and can suffer or feel joy. As we discussed in chapter 7, it becomes hopeless to formulate utilitarian ethics based on maximizing positive experiences without knowing which intelligent entities are capable of having them. As mentioned in chapter 5, some people might prefer their robots to be unconscious to avoid feeling slaveowner guilt. On the other hand, they may desire the opposite if they upload their minds to break free from biological limitations: after all, what’s the point of uploading yourself into a robot that talks and acts like you if it’s a mere unconscious zombie, by which I mean that being the uploaded you doesn’t feel like anything? Isn’t this equivalent to committing suicide from your subjective point of view, even though your friends may not realize that your subjective experience has died?  Even if
we knew the exact equations of motion of the universe and the exact state of
the universe after evaporation, we still would not be able to ascertain the
information that went into the black hole. The radiation emitted by the black
hole as it evaporates must be thermal, and indistinguishable between any
two evaporated black holes—even if they were originally formed from two
very different stars!
An immediate consequence of this phenomenon is that if we knew the exact
and precise state of the entire universe now (down to its fundamental particles),
it would be in principle impossible to know what the universe was like a few
years ago. Since our business as physicists is to use existing information to
predict the evolution of the universe both forwards and backwards in time, this
represents a catastrophic and unprecedented loss of determinism in physics.
There is no other process which is known to result in net information loss.
Thus we appear to require one of two unappealing options: either strong
quantum gravity effects are needed to describe the large-scale dynamics of
regions of the universe that look just like Mercury and the Sun, or physics
is not a deterministic science. This is the black hole information paradox.
This paradox has been a guiding post for progress on quantum gravity since its
discovery by Hawking in 1975. Developments in string theory in the 1990s and
2000s provided the first conclusive evidence that information is not lost. How
information can be conserved, however, remained a mystery. Is semiclassical
gravity violated at the event horizon of a black hole? How can this be, given that
interactions between quantum effects and gravity must be extremely weak there?
A NEW PERSPECTIVE
In 2019, the tide turned with a set of two simultaneously submitted papers by
myself and my collaborators Almheiri, Marolf and Maxfield, and in parallel,
Penington. We executed a semiclassical gravity analysis of black hole evaporation
that was consistent, by a famous litmus test, with information conservation.
This test, known as the Page curve, tracks the behavior of the von Neumann
entropy of the radiation. This entropy, which is different from the standard
entropy of thermodynamics, measures how “entangled” (or, correlated) a system
is with its complement. Given some quantum system, say, n qubits, we can
divide it up into two complementary subsystems: R and B. R will stand for the
radiation of a black hole and B for the remaining black hole. When R is the
trivial empty set, i.e., R contains zero qubits, R is trivially uncorrelated with B. If black hole evaporation is to be unitary, then the von Neumann entropy of the
radiation should start out at zero, increase for a while, then—once the black
hole has fully evaporated—return to zero. The resulting curve is known as the
Page curve. However, Hawking’s calculation shows that the von Neumann
entropy of the radiation increases monotonically until the black hole has finished
evaporating! The radiation, according to a semiclassical gravity treatment of the
horizon, is now correlated with something that does not exist in the universe.
In 2019 we found that there exists a different semiclassical analysis from
Hawking’s that yields the Page curve. There was, however, a catch: while our
calculation was within the regime of semiclassical gravity, and assumed that
the standard picture of semiclassical gravity is an accurate description of the
physics, the rules for how to compute certain quantities were vastly different
from the standard rules of semiclassical gravity. By analogy, suppose you are
asked to compute the pressure of an ideal gas in a cylinder. You may be tempted
to compute the average velocity or momentum of the molecules of the gas
and then use that to deduce the pressure. However, since the average velocity
is zero, you would be led astray! Instead, we know that in the limit where
thermodynamics is emergent from statistical mechanics, we must use PV = nRT,
which is valid thermodynamically, but inherited from statistical mechanics.
In complete analogy with the ideal gas law, we used the “quantum extremal
surface formula,” proposed by myself and A. Wall in 2014, rather than the
Hawking formula (analogous to the erroneous zero average velocity calculation).
The logic is identical: in both cases, you use an alternative formula which follows
from the underlying microscopics of the statistical mechanics of your system.
This unusual approach gave us precisely the loophole we needed: the basic
constructs of semiclassical gravity—space and time and its curvatures—
can be consistent with information conservation, but only if we use the correct
equations inherited from quantum gravity. This insight resulted in an explosion of progress across the field of black hole
information: finally, there might be a way of having our cake and eating it too!
We can have standard spacetime and geometry at the event horizon of a black
hole without paying the price of determinism of physics.
TOWARDS A RESOLUTION
A significant question remained, however: why are the equations for various
quantities modified by quantum gravity when a black hole is involved, but
not modified for the Sun or Mercury? Last summer, my collaborators at MIT
(Chris Akers, Daniel Harlow and Shreya Vardhan) and I, together with Penington,
proposed a resolution for the distinguishing feature between black holes and
other objects. Our resolution was predicated on an older insight by Daniel
Harlow and Patrick Hayden that even though the information about the black
hole interior must escape in its radiation, actually processing the radiation to
distill information about the black hole is incredibly complex. To be precise, this
“decoding” process of the black hole radiation would require a quantum computer
to implement a circuit whose size is exponential in the size of the black hole.
For a black hole with the mass of the Sun, this would be exponential in 1077! Black
holes in general are extremely complex objects, which sets them apart from
other astrophysical phenomena with similar curvature scales as those at the
horizon of an astrophysical black hole. We proposed that semiclassical gravity is
valid at low curvatures and low complexity; in our quantitative models, we saw
that the modifications to the calculations required by the 2019 calculation of the
Page curve can be attributed exactly to complexity in toy models of black holes.
We will likely be exploring the consequences of these developments on quantum
gravity for years to come. Just as the black hole information problem has served
as a point of inspiration for a vast landscape of developments in quantum gravity,
I predict
—
with confidence since the fundamental theory of our universe is, in
fact, predictive!
—
that its resolution will do the same. Black Holes and their Effect on Objects
One of the most interesting phenomena in the universe is the black hole.
These massive titans of science are renowned for their incredible effect on
anything around them, including light. While many people believe that they are an
unrealistic element exaggerated by Sci-Fi, this is far from the truth. Black holes are
truly a magnificent example of the marvels of space.
Black holes exhibit extreme amounts of gravity around them, which pulls all
matter nearby as well as dilating time. This gravity is so powerful that it can even
trap light within its proximity. Due to the light being trapped inside the singularity,
or center, of the blackhole, they appear as pitch-black spheres to the human eye.
There is a small white border around the black hole known as the event horizon.
Around the event horizon, there is typically a massive collection of Hawking
radiation, usually expelled from the blackhole when it consumes stars or other
massive entities, appearing as a sunset-colored cloud.
A black hole is formed when matter is compressed to such a small point that
it no longer has a volume. This typically happens when enormous stars implode
and collapse onto themselves. Black holes are generally classified into three
categories, being stellar-mass, supermassive, and intermediate mass.
1 There is
technically another class, being incredibly rare and not commonly recognized. This
is an ultramassive black hole, larger than most galaxies with the power to reduce
anything in the universe to nothing.
When an object comes close to a black hole, it will begin to be drawn toward
it with increasing force as it nears. In addition, time slows greatly. To an observer,
time freezes to a halt for the object. Most small pieces of debris are obliterated by
the massive amount of radiation, but for those strong enough to pass through it, an
entirely different fate awaits. Due to the extreme gravity, objects will start to
stretch out as they get near. This begins slowly, but the rate increases incredibly
fast. Soon, it will be stretched out into a very thin strand, hence the process being
nicknamed “spaghettification.
” Then the strand is sucked into the black hole, past
the event horizon. Once crossing the horizon, there is no possible way of turning
back. From the observer’s point of view, the strand will vanish into the abyss of the
black hole.
Once inside the black hole’s larger body, the laws of physics no longer apply.
Both time and space will change and swap roles. (according to Einstein’s theories)2
At this point, time is the only force pulling objects closer to the singularity, which
1 “Types of Black Holes,
” accessed November 20, 2024, https://science.nasa.gov/universe/black-holes/types/.2 Chelsea Gohd,
“What Happens When Something Gets
‘Too Close
’ To A Black Hole,
”
accessed November 20,
2024, https://science.nasa.gov/universe/what-happens-when-something-gets-too-close-to-a-black-hole/.
is why the event horizon is the point of no return. There is no force in the universe
strong enough to contradict this pull, meaning anything drawn in is truly lost
forever.
The singularity of the black hole is positioned perfectly in the center, and is
the source from which it is born. It is both infinitely dense as well as infinitely
small, meaning that even atoms and quarks are larger. The immeasurable density is
due to all of the matter sucked into the blackhole. The gravity is so strong, in fact,
that it can merge objects into one another, creating a sensation of it devouring
mass. As more matter is drawn in, the density increases, strengthening the pull of
gravity around the black hole and expanding the visual area of it, allowing black
holes to “grow.
”
Black holes “die” very slowly, over hundreds of billions of years. They
typically just fizzle out from emitting too much hawking radiation and causing
them to lose their mass. This, however, rarely ever happens. Black holes can also
merge with each other, with the larger typically absorbing the smaller to add to its
collective mass.
When it comes to black holes, we don’t know very much for certain. We
really have no way of testing theories as anything sent to a black hole would be
destroyed. As a matter of fact, we can’t even see most black holes with a telescope.
They are usually discovered by observing strange effects on nearby planets and
stars. This, of course, sparks many theories. Some scientists even theorize that we
are living inside a black hole, and while this may seem far-fetched, we currently
have no way to disprove this theory.
Black holes are one of the most incredible and awe-inspiring things in the
universe. We may never understand them in their entirety, which contributes an
amazing sense of wonder. However, one question still left in my mind is what
would happen if a black hole came to Earth.
the von Neumann entropy of R vanishes. If we repartition the system so that
R has progressively more qubits, we at first expect its von Neumann entropy to
increase. Analogously, as the black hole evaporates into radiation, the data in
B must end up in R. Eventually, we can repartition the system so that B has zero
qubits and R has all of the qubits. That is, the black hole has fully evaporated.
For the long-term cosmic future of life (chapter 6), understanding what’s conscious and what’s not becomes pivotal: if technology enables intelligent life to flourish throughout our Universe for billions of years, how can we be sure that this life is conscious and able to appreciate what’s happening?
So what precisely is it that we don't understand about consciousness? Few have thought harder about this question than David Chalmers, a famous Australian philosopher rarely seen without a playful smile and a black leather jacket— which my wife liked so much that she gave me a similar one for Christmas. He followed his heart into philosophy despite making the finals at the International Mathematics Olympiad — and despite the fact that his only B grade in college, shattering his otherwise straight As, was for an introductory philosophy course. Indeed, he seems utterly undeterred by put-downs or controversy, and I've been astonished by his ability to politely listen to uninformed and misguided criticism of his own work without even feeling the need to respond. 
As David has emphasized, there are really two separate mysteries of the mind. First, there's the mystery of how a brain processes information, which David calls the " easy " problems. For example, how does a brain attend to, interpret and respond to sensory input? How can it report on its internal state using language? Although these questions are actually extremely difficult, they're by our definitions not mysteries of consciousness, but mysteries of intelligence: they ask how a brain remembers, computes and learns. Moreover, we saw in the first part of the book how AI researchers have started to make serious progress on solving many of these “ easy problems ” with machines — from playing Go to driving cars, analyzing images and processing natural language. 
Then there's the separate mystery of why you have a subjective experience, which David calls the hard problem. When you're driving, you're experiencing colors, sounds, emotions, and a feeling of self. But why are you experiencing anything at all? Does a self-driving car experience anything at all? If you're racing against a self- driving car, you're both inputting information from sensors, processing it and outputting motor commands. But subjectively experiencing driving is something logically separate — is it optional, and if so, what causes it? 
What I like about this physics perspective is that it transforms the hard problem that we as humans have struggled with for millennia into a more focused version that’s easier to tackle with the methods of science. Instead of starting with a hard problem of why an arrangement of particles can feel conscious, let’s start with a hard fact that some arrangements of particles do feel conscious while others don’t. For example, you know that the particles that make up your brain are in a conscious arrangement right now, but not when you’re in deep dreamless sleep. 
This physics perspective leads to three separate hard questions about consciousness, as shown in figure 8.1. First of all, what properties of the particle 
arrangement make the difference? Specifically, what physical properties distinguish conscious and unconscious systems? If we can answer that, then we can figure out which AI systems are conscious. In the more immediate future, it can also help emergency-room doctors determine which unresponsive patients are conscious. 
Second, how do physical properties determine what the experience is like? Specifically, what determines qualia, basic building blocks of consciousness such as the redness of a rose, the sound of a cymbal, the smell of a steak, the taste of a tangerine or the pain of a pinprick. When people tell me that consciousness research is a hopeless waste of time, the main argument they give is that it’s “unscientific” and always will be. But is that really true? The influential Austro-British philosopher Karl Popper popularized the now widely accepted adage “If it’s not falsifiable, it’s not scientific.” In other words, science is all about testing theories against observations: if a theory can’t be tested even in principle, then it’s logically impossible to ever falsify it, which by Popper’s definition means that it’s unscientific. 
So could there be a scientific theory that answers any of the three consciousness questions in figure 8.1? Please let me try to persuade you that the answer is a resounding YES!, at least for the pretty hard problem: “What physical properties distinguish conscious and unconscious systems?” Suppose that someone has a theory that, given any physical system, answers the question of whether the system is conscious with “yes,” “no” or “unsure.” Let’s hook your brain up to a device that measures some of the information processing in different parts of your brain, and let’s feed this information into a computer program that uses the consciousness theory to predict which parts of that information are conscious, and presents you with its predictions in real time on a screen, as in figure 8.2. First you think of an apple. The screen informs you that there’s information about an apple in your brain which you’re aware of, but that there’s also information in your brainstem about your pulse that you’re unaware of. Would you be impressed? Although the first two predictions of the theory were correct, you decide to do some more rigorous testing. You think about your mother and the computer informs you that there’s information in your brain about your mother but that you’re unaware of this. The theory made an incorrect prediction, which means that it’s ruled out and goes in the garbage dump of scientific history together with Aristotelian mechanics, the luminiferous aether, geocentric cosmology and countless other failed ideas. Here’s the key point: Although the theory was wrong, it was scientific! Had it not been scientific, you wouldn’t have been able to test it and rule it out. 
On the other hand, if the theory refuses to make any predictions, merely replying “unsure” whenever queried, then it’s untestable and hence unscientific. This might happen because it’s applicable only in some situations, because the required computations are too hard to carry out in practice or because the brain sensors are no good. Today’s most popular scientific theories tend to be somewhere in the middle, giving testable answers to some but not all of our questions. For example, our core theory of physics will refuse to answer questions about systems that are simultaneously extremely small (requiring quantum mechanics) and extremely heavy (requiring general relativity), because we haven’t yet figured out which mathematical equations to use in this case. This core theory will also refuse to predict the exact masses of all possible atoms —in this case, we think we have the necessary equations, but we haven’t managed to accurately compute their solutions. The more dangerously a theory lives by sticking its neck out and making testable predictions, the more useful it is, and the more seriously we take it if it survives all our attempts to kill it.
In summary, any theory predicting which physical systems are conscious (the pretty hard problem) is scientific, as long as it can predict which of your brain processes are conscious. However, the testability issue becomes less clear for the higher-up questions in figure 8.1. What would it mean for a theory to predict how you subjectively experience the color red? And if a theory purports to explain why there is such a thing as consciousness in the first place, then how do you test it experimentally? Just because these questions are hard doesn’t mean that we should avoid them, and we’ll indeed return to them below. But when confronted with several related unanswered questions, I think it’s wise to tackle the easiest one first. For this reason, my consciousness research at MIT is focused squarely on the base of the pyramid in figure 8.1. I recently discussed this strategy with my fellow physicist Piet Hut from Princeton, who joked that trying to build the top of the pyramid before the base would be like worrying about the interpretation of quantum mechanics before discovering the Schrödinger equation, the mathematical foundation that lets us predict the outcomes of our experiments. 
When discussing what’s beyond science, it’s important to remember that the answer depends on time! Four centuries ago, Galileo Galilei was so impressed by math-based physics theories that he described nature as “a book written in the language of mathematics.” If he threw a grape and a hazelnut, he could accurately predict the shapes of their trajectories and when they would hit the ground. Yet he had no clue why one was green and the other brown, or why one was soft and the other hard—these aspects of the world were beyond the reach of science at the time. But not forever! When James Clerk Maxwell discovered his eponymous equations in 1861, it became clear that light and colors could also be understood mathematically. We now know that the aforementioned Schrödinger equation, discovered in 1925, can be used to predict all properties of matter, including what’s soft or hard. While theoretical progress has enabled ever more scientific predictions, technological progress has enabled ever more experimental tests: almost everything we now study with telescopes, microscopes or particle colliders was once beyond science. In other words, the purview of science has expanded dramatically since Galileo’s days, from a tiny fraction of all phenomena to a large percentage, including subatomic particles, black holes and our cosmic origins 13.8 billion years ago. This raises the question: What’s left? 
If you multiply 32 by 17 in your head, you’re conscious of many of the inner workings of your computation. But suppose I instead show you a portrait of Albert Einstein and tell you to say the name of its subject. As we saw in chapter 2, this too is a computational task: your brain is evaluating a function whose input is information from your eyes about a large number of pixel colors and whose output is information to muscles controlling your mouth and vocal cords. Computer scientists call this task “image classification” followed by “speech synthesis.” Although this computation is way more complicated than your multiplication task, you can do it much faster, seemingly without effort, and without being conscious of the details of how you do it. Your subjective experience consists merely of looking at the picture, experiencing a feeling of recognition and hearing yourself say “Einstein.” 
Psychologists have long known that you can unconsciously perform a wide range of other tasks and behaviors as well, from blink reflexes to breathing, reaching, grabbing and keeping your balance. Typically, you’re consciously aware of what you did, but not how you did it. On the other hand, behaviors that involve unfamiliar situations, self-control, complicated logical rules, abstract reasoning or manipulation of language tend to be conscious. They’re known as behavioral correlates of consciousness, and they’re closely linked to the effortful, slow and controlled way of thinking that psychologists call “System 2.”5 
It’s also known that you can convert many routines from conscious to unconscious through extensive practice, for example walking, swimming, bicycling, driving, typing, shaving, shoe tying, computer-gaming and piano playing. 6 Indeed, it’s well known that experts do their specialties best when they’re in a state of “flow,” aware only of what’s happening at a higher level, and unconscious of the low-level details of how they’re doing it. For example, try reading the next sentence while being consciously aware of every single letter, as when you first learned to read. Can you feel how much slower it is, compared to when you’re merely conscious of the text at the level of words or ideas? 
Clever experiments and analyses have suggested that consciousness is limited not merely to certain behaviors, but also to certain parts of the brain. Which are the prime suspects? Many of the first clues came from patients with brain lesions: localized brain damage caused by accidents, strokes, tumors or infections. But this was often inconclusive. For example, does the fact that lesions in the back of the brain can cause blindness mean that this is the site of visual consciousness, or does it merely mean that visual information passes through there en route to wherever it will later become conscious, just as it first passes through the eyes? 
Although lesions and medical interventions haven’t pinpointed the locations of conscious experiences, they’ve helped narrow down the options. For example, I know that although I experience pain in my hand as actually occurring there, the pain experience must occur elsewhere, because a surgeon once switched off my hand pain without doing anything to my hand: he merely anesthetized nerves in my shoulder. Moreover, some amputees experience phantom pain that feels as though it’s in their nonexistent hand. As another example, I once noticed that when I looked only with my right eye, part of my visual field was missing—a doctor determined that my retina was coming loose and reattached it. In contrast, patients with certain brain lesions experience hemineglect, where they too miss information from half their visual field, but aren’t even aware that it’s missing— for example, failing to notice and eat the food on the left half of their plate. It’s as if consciousness about half of their world has disappeared. But are those damaged brain areas supposed to generate the spatial experience, or were they merely feeding spatial information to the sites of consciousness, just as my retina did? 
The pioneering U.S.-Canadian neurosurgeon Wilder Penfield found in the 1930s that his neurosurgery patients reported different parts of their body being touched when he electrically stimulated specific brain areas in what’s now called the somatosensory cortex (figure 8.3). 9 He also found that they involuntarily moved different parts of their body when he stimulated brain areas in what’s now called the motor cortex. But does that mean that information processing in these brain areas corresponds to consciousness of touch and motion? 
Although we’re still nowhere near being able to measure every single firing of all of your roughly hundred billion neurons, brain-reading technology is advancing rapidly, involving techniques with intimidating names such as fMRI, EEG, MEG, ECoG, ePhys and fluorescent voltage sensing. fMRI, which stands for functional magnetic resonance imaging, measures the magnetic properties of hydrogen nuclei to make a 3-D map of your brain roughly every second, with millimeter resolution. EEG (electroencephalography) and MEG (magnetoencephalography) measure the electric and magnetic field outside your head to map your brain thousands of times per second, but with poor resolution, unable to distinguish features smaller than a few centimeters. If you’re squeamish, you’ll appreciate that these three techniques are all noninvasive. If you don’t mind opening up your skull, you have additional options. ECoG (electrocorticography) involves placing say a hundred wires on the surface of your brain, while ePhys (electrophysiology) involves inserting microwires, which are sometimes thinner than a human hair, deep into the brain to record voltages from as many as a thousand simultaneous locations. Many epileptic patients spend days in the hospital while ECoG is used to figure out what part of their brain is triggering seizures and should be resected, and kindly agree to let neuroscientists perform consciousness experiments on them in the meantime. Finally, fluorescent voltage sensing involves genetically manipulating neurons to emit flashes of light when firing, enabling their activity to be measured with a microscope. Out of all the techniques, it has the potential to rapidly monitor the largest number of neurons, at least in animals with transparent brains—such as the C. elegans worm with its 302 neurons and the larval zebrafish with its about 100,000. 
Although Francis Crick warned Christof Koch about studying consciousness, Christof refused to give up and and eventually won Francis over. In 1990, they wrote a seminal paper about what they called “neural correlates of consciousness” (NCCs), asking which specific brain processes corresponded to conscious experiences. For thousands of years, thinkers had had access to the information processing in their brains only via their subjective experience and 
behavior. Crick and Koch pointed out that brain-reading technology was suddenly providing independent access to this information, allowing scientific study of which information processing corresponded to what conscious experience. Sure enough, technology-driven measurements have by now turned the quest for NCCs into quite a mainstream part of neuroscience, one whose thousands of publications extend into even the most prestigious journals. 10 
What are the conclusions so far? To get a flavor for NCC detective work, let’s first ask whether your retina is conscious, or whether it’s merely a zombie system that records visual information, processes it and sends it on to a system downstream in your brain where your subjective visual experience occurs. In the left panel of figure 8.4, which square is darker: the one labeled A or B? A, right? No, they’re in fact identically colored, which you can verify by looking at them through small holes between your fingers. This proves that your visual experience can’t reside entirely in your retina, since if it did, they’d look the same. 
Now look at the right panel of figure 8.4. Do you see two women or a vase? If you look long enough, you’ll subjectively experience both in succession, even though the information reaching your retina remains the same. By measuring what happens in your brain during the two situations, one can tease apart what makes the difference—and it’s not the retina, which behaves identically in both cases. 
The death blow to the conscious-retina hypothesis comes from a technique called “continuous flash suppression” pioneered by Christof Koch, Stanislas Dehaene and collaborators: it’s been discovered that if you make one of your eyes watch a complicated sequence of rapidly changing patterns, then this will distract your visual system to such an extent that you’ll be completely unaware of a still image shown to the other eye. 11 In summary, you can have a visual image in your retina without experiencing it, and you can (while dreaming) experience an image without it being on your retina. This proves that your two retinas don’t host your visual consciousness any more than a video camera does, even though they perform complicated computations involving over a hundred million neurons. 
NCC researchers also use continuous flash suppression, unstable visual/auditory illusions and other tricks to pinpoint which of your brain regions are responsible for each of your conscious experiences. The basic strategy is to compare what your neurons are doing in two situations where essentially everything (including your sensory input) is the same—except your conscious experience. The parts of your brain that are measured to behave differently are then identified as NCCs. 
Such NCC research has proven that none of your consciousness resides in your gut, even though that’s the location of your enteric nervous system with its whopping half-billion neurons that compute how to optimally digest your food; feelings such as hunger and nausea are instead produced in your brain. Similarly, none of your consciousness appears to reside in the brainstem, the bottom part of the brain that connects to the spinal cord and controls breathing, heart rate and blood pressure. More shockingly, your consciousness doesn’t appear to extend to your cerebellum (figure 8.3), which contains about two-thirds of all your neurons: patients whose cerebellum is destroyed experience slurred speech and clumsy motion reminiscent of a drunkard, but remain fully conscious. 
So far, we’ve looked at experimental clues regarding what types of information processing are conscious and where consciousness occurs. But when does it occur? When I was a kid, I used to think that we become conscious of events as they happen, with absolutely no time lag or delay. Although that’s still how it subjectively feels to me, it clearly can’t be correct, since it takes time for my brain to process the information that enters via my sensory organs. NCC researchers have carefully measured how long, and Christof Koch’s summary is that it takes about a quarter of a second from when light enters your eye from a complex object until you consciously perceive seeing it as what it is. 13 This means that if you’re driving down a highway at fifty-five miles per hour and suddenly see a squirrel a few meters in front of you, it’s too late for you to do anything about it, because you’ve already run over it! 
In summary, your consiousness lives in the past, with Christof Koch estimating that it lags behind the outside world by about a quarter second. Intriguingly, you can often react to things faster than you can become conscious of them, which proves that the information processing in charge of your most rapid reactions must be unconscious. For example, if a foreign object approaches your eye, your blink reflex can close your eyelid within a mere tenth of a second. It’s as if one of your brain systems receives ominous information from the visual system, computes that your eye is in danger of getting struck, emails your eye muscles instructions to blink and simultaneously emails the conscious part of your brain saying “Hey, we’re going to blink.” By the time this email has been read and included into your conscious experience, the blink has already happened. 
Indeed, the system that reads that email is continually bombarded with messages from all over your body, some more delayed than others. It takes longer for nerve signals to reach your brain from your fingers than from your face because of distance, and it takes longer for you to analyze images than sounds because it’s more complicated—which is why Olympic races are started with a bang rather than with a visual cue. Yet if you touch your nose, you consciously experience the sensation on your nose and fingertip as simultaneous, and if you clap your hands, you see, hear and feel the clap at exactly the same time. 14 This means that your full conscious experience of an event isn’t created 
until the last slowpoke email reports have trickled in and been analyzed. 
A famous family of NCC experiments pioneered by physiologist Benjamin Libet has shown that the sort of actions you can perform unconsciously aren’t limited to rapid responses such as blinks and ping-pong smashes, but also include certain decisions that you might attribute to free will—brain measurements can sometimes predict your decision before you become conscious of having made it. To appreciate why, let’s compare theories of consciousness with theories of gravity. Scientists started taking Newton’s theory of gravity seriously because they got more out of it than they put into it: simple equations that fit on a napkin could accurately predict the outcome of every gravity experiment ever conducted. They therefore also took seriously its predictions far beyond the domain where it had been tested, and these bold extrapolations turned out to work even for the motions of galaxies in clusters millions of light-years across. However, the predictions were off by a tiny amount for the motion of Mercury around the Sun. Scientists then started taking seriously Einstein’s improved theory of gravity, general relativity, because it was arguably even more elegant and economical, and correctly predicted even what Newton’s theory got wrong. They consequently took seriously also its predictions far beyond the domain where it had been tested, for phenomena as exotic as black holes, gravitational waves in the very fabric of spacetime, and the expansion of our Universe from a hot fiery origin—all of which were subsequently confirmed by experiment. 
Analogously, if a mathematical theory of consciousness whose equations fit on a napkin could successfully predict the outcomes of all experiments we perform on brains, then we’d start taking seriously not merely the theory itself, but also its predictions for consciousness beyond brains—for example, in machines. 
Although some theories of consciousness date back to antiquity, most modern ones are grounded in neuropsychology and neuroscience, attempting to explain and predict consciousness in terms of neural events occurring in the brain. 16 Although these theories have made some successful predictions for neural correlates of consciousness, they neither can nor aspire to make predictions about machine consciousness. To make the leap from brains to machines, we need to generalize from NCCs to PCCs: physical correlates of consciousness, defined as the patterns of moving particles that are conscious. Because if a theory can correctly predict what’s conscious and what’s not by referring only to physical building blocks such as elementary particles and force fields, then it can make predictions not merely for brains, but also for any other arrangements of matter, including future AI systems. So let’s take a physics perspective: What particle arrangements are conscious? 
But this really raises another question: How can something as complex as consciousness be made of something as simple as particles? I think it’s because it’s a phenomenon that has properties above and beyond those of its particles. In physics, we call such phenomena “emergent.”17 Let’s understand this by looking at an emergent phenomenon that’s simpler than consciousness: wetness. 
A drop of water is wet, but an ice crystal and a cloud of steam aren’t, even though they’re made of identical water molecules. Why? Because the property of wetness depends only on the arrangement of the molecules. It makes absolutely no sense to say that a single water molecule is wet, because the phenomenon of wetness emerges only when there are many molecules, arranged in the pattern we call liquid. So solids, liquids and gases are all emergent phenomena: they’re more than the sum of their parts, because they have properties above and beyond the properties of their particles. They have properties that their particles lack. 
Now just like solids, liquids and gases, I think consciousness is an emergent phenomenon, with properties above and beyond those of its particles. For example, entering deep sleep extinguishes consciousness, by merely rearranging the particles. In the same way, my consciousness would disappear if I froze to death, which would rearrange my particles in a more unfortunate way. 
When you put lots of particles together to make anything from water to a 
brain, new phenomena with observable properties emerge. We physicists love studying these emergent properties, which can often be identified by a small set of numbers that you can go out and measure—quantities such as how viscous the substance is, how compressible it is and so on. For example, if a substance is so viscous that it’s rigid, we call it a solid, otherwise we call it a fluid. And if a fluid isn’t compressible, we call it a liquid, otherwise we call it a gas or a plasma, depending on how well it conducts electricity. 
I first met Giulio at a 2014 physics conference in Puerto Rico to which I’d invited him and Christof Koch, and he struck me as the ultimate renaissance man who’d have blended right in with Galileo and Leonardo da Vinci. His quiet demeanor couldn’t hide his incredible knowledge of art, literature and philosophy, and his culinary reputation preceded him: a cosmopolitan TV journalist had recently told me how Giulio had, in just a few minutes, whipped up the most delicious salad he’d tasted in his life. I soon realized that behind his soft-spoken demeanor was a fearless intellect who’d follow the evidence wherever it took him, regardless of the preconceptions and taboos of the establishment. Just as Galileo had pursued his mathematical theory of motion despite establishment pressure not to challenge geocentrism, Giulio had developed the most mathematically precise consciousness theory to date, integrated information theory (IIT). 
I’d been arguing for decades that consciousness is the way information feels when being processed in certain complex ways. 18 IIT agrees with this and replaces my vague phrase “certain complex ways” by a precise definition: the information processing needs to be integrated, that is, Φ needs to be large. Giulio’s argument for this is as powerful as it is simple.
the conscious system needs to be integrated into a unified whole, because if it instead consisted of two independent parts, then they’d feel like two separate conscious entities rather than one. In other words, if a conscious part of a brain or computer can’t communicate with the rest, then the rest can’t be part of its subjective experience. 
Giulio and his collaborators have measured a simplified version of Φ by using EEG to measure the brain’s response to magnetic stimulation. Their “consciousness detector” works really well: it determined that patients were conscious when they were awake or dreaming, but unconscious when they were anesthetized or in deep sleep. It even discovered consciousness in two patients suffering from “locked-in” syndrome, who couldn’t move or communicate in any normal way. 19 So this is emerging as a promising technology for doctors in the future to figure out whether certain patients are conscious or not.
IIT is defined only for discrete systems that can be in a finite number of states, for example bits in a computer memory or oversimplified neurons that can be either on or off. This unfortunately means that IIT isn’t defined for most traditional physical systems, which can change continuously—for example, the position of a particle or the strength of a magnetic field can take any of an infinite number of values. 20 If you try to apply the IIT formula to such systems, you’ll typically get the unhelpful result that Φ is infinite. Quantum-mechanical systems can be discrete, but the original IIT isn’t defined for quantum systems. So how can we anchor IIT and other information-based consciousness theories on a solid physical foundation? 
We can do this by building on what we learned in chapter 2 about how clumps of matter can have emergent properties that are related to information. We saw that for something to be usable as a memory device that can store information, it needs to have many long-lived states. We also saw that being computronium, a substance that can do computations, in addition requires complex dynamics: the laws of physics need to make it change in ways that are complicated enough to be able to implement arbitrary information processing. Finally, we saw how a neural network, for example, is a powerful substrate for learning because, simply by obeying the laws of physics, it can rearrange itself to get better and better at implementing desired computations. Now we’re asking an additional question: What makes a blob of matter able to have a subjective experience? In other words, under what conditions will a blob of matter be able to do these four things? 
But how can consciousness feel so non-physical if it’s in fact a physical phenomenon? How can it feel so independent of its physical substrate? I think it’s because it is rather independent of its physical substrate, the stuff in which it is a pattern! We encountered many beautiful examples of substrate-independent patterns in chapter 2, including waves, memories and computations. We saw how they weren’t merely more than their parts (emergent), but rather independent of their parts, taking on a life of their own. For example, we saw how a future simulated mind or computer-game character would have no way of knowing whether it ran on Windows, Mac OS, an Android phone or some other operating system, because it would be substrate-independent. Nor could it tell whether the logic gates of its computer were made of transistors, optical circuits or other hardware. Or what the fundamental laws of physics are—they could be anything as long as they allow the construction of universal computers. 
In summary, I think that consciousness is a physical phenomenon that feels non-physical because it’s like waves and computations: it has properties independent of its specific physical substrate. This follows logically from the consciousness-as-information idea. This leads to a radical idea that I really like: If consciousness is the way that information feels when it’s processed in certain ways, then it must be substrate-independent; it’s only the structure of the information processing that matters, not the structure of the matter doing the information processing. In other words, consciousness is substrate-independent twice over! 
As I said, I think that consciousness is the way information feels when being processed in certain ways. This means that to be conscious, a system needs to be able to store and process information, implying the first two principles. Note that the memory doesn’t need to last long: I recommend watching this touching video of Clive Wearing, who appears perfectly conscious even though his memories last less than a minute. 21 I think that a conscious system also needs to be fairly independent from the rest of the world, because otherwise it wouldn’t subjectively feel that it had any independent existence whatsoever. Finally, I think that the conscious system needs to be integrated into a unified whole, as Giulio Tononi argued, because if it consisted of two independent parts, then they would feel like two separate conscious entities, rather than one. The first three principles imply autonomy: that the system is able to retain and process information without much outside interference, hence determining its own future. All four principles together mean that a system is autonomous but its parts aren’t. 
If these four principles are correct, then we have our work cut out for us: we need to look for mathematically rigorous theories that embody them and test them experimentally. We also need to determine whether additional principles are needed. Regardless of whether IIT is correct or not, researchers should try to develop competing theories and test all available theories with ever better experiments. 
We’ve already discussed the perennial controversy about whether consciousness research is unscientific nonsense and a pointless waste of time. In addition, there are recent controversies at the cutting edge of consciousness research—let’s explore the ones that I find most enlightening. 
Giulio Tononi’s IIT has lately drawn not merely praise but also criticism, some of which has been scathing. Scott Aaronson recently had this to say on his blog: “In my opinion, the fact that Integrated Information Theory is wrong— demonstrably wrong, for reasons that go to its core—puts it in something like the top 2% of all mathematical theories of consciousness ever proposed. Almost all competing theories of consciousness, it seems to me, have been so vague, fluffy and malleable that they can only aspire to wrongness.”22 To the credit of both Scott and Giulio, they never came to blows when I watched them debate IIT at a recent New York University workshop, and they politely listened to each other’s arguments. Aaronson showed that certain simple networks of logic gates had extremely high integrated information (Φ) and argued that since they clearly weren’t conscious, IIT was wrong. Giulio countered that if they were built, they would be conscious, and that Scott’s assumption to the contrary was anthropocentrically biased, much as if a slaughterhouse owner claimed that animals couldn’t be conscious just because they couldn’t talk and were very different from humans. My analysis, with which they both agreed, was that they were at odds about whether integration was merely a necessary condition for consciousness (which Scott was OK with) or also a sufficient condition (which Giulio claimed). The latter is clearly a stronger and more contentious claim, which I hope we can soon test experimentally.
This claim has been challenged by both David Chalmers and AI professor Murray Shanahan by imagining what would happen if you instead gradually replaced the neural circuits in your brain by hypothetical digital hardware perfectly simulating them. 25 Although your behavior would be unaffected by the replacement since the simulation is by assumption perfect, your experience would change from conscious initially to unconscious at the end, according to Giulio. But how would it feel in between, as ever more got replaced? When the parts of your brain responsible for your conscious experience of the upper half of your visual field were replaced, would you notice that part of your visual scenery was suddenly missing, but that you mysteriously knew what was there nonetheless, as reported by patients with “blindsight”?26 This would be deeply troubling, because if you can consciously experience any difference, then you can also tell your friends about it when asked—yet by assumption, your behavior can’t change. The only logical possibility compatible with the assumptions is that at exactly the same instance that any one thing disappears from your consciousness, your mind is mysteriously altered so as either to make you lie and deny that your experience changed, or to forget that things had been different. 
On the other hand, Murray Shanahan admits that the same gradualreplacement critique can be leveled at any theory claiming that you can act conscious without being conscious, so you might be tempted to conclude that acting and being conscious are one and the same, and that externally observable behavior is therefore all that matters. But then you’d have fallen into the trap of predicting that you’re unconscious while dreaming, even though you know better. 
Imagine using future technology to build a direct communication link between two human brains, and gradually increasing the capacity of this link until communication is as efficient between the brains as it is within them. Would there come a moment when the two individual consciousnesses suddenly disappear and get replaced by a single unified one as IIT predicts, or would the transition be gradual so that the individual consciousnesses coexisted in some form even as a joint experience began to emerge? 
Another fascinating controversy is whether experiments underestimate how much we’re conscious of. We saw earlier that although we feel we’re visually conscious of vast amounts of information involving colors, shapes, objects and seemingly everything that’s in front of us, experiments have shown that we can only remember and report a dismally small fraction of this. 27 Some researchers have tried to resolve this discrepancy by asking whether we may sometimes have “consciousness without access,” that is, subjective experience of things that are too complex to fit into our working memory for later use. 28 For example, when you experience inattentional blindness by being too distracted to notice an object in plain sight, this doesn’t imply that you had no conscious visual experience of it, merely that it wasn’t stored in your working memory. 29 Should it count as forgetfulness rather than blindness? Other researchers reject this idea that people can’t be trusted about what they say they experienced, and warn of its implications. Murray Shanahan imagines a clinical trial where patients report complete pain relief thanks to a new wonder drug, which nonetheless gets rejected by a government panel: “The patients only think they are not in pain. Thanks to neuroscience, we know better.”30 On the other hand, there have been cases where patients who accidentally awoke during surgery were given a drug to make them forget the ordeal. Should we trust their subsequent report that they experienced no pain?
If some future AI system is conscious, then what will it subjectively experience? This is the essence of the “even harder problem” of consciousness, and forces us up to the second level of difficulty depicted in figure 8.1. Not only do we currently lack a theory that answers this question, but we’re not even sure whether it’s logically possible to fully answer it. After all, what could a satisfactory answer sound like? How would you explain to a person born blind what the color red looks like? 
Fortunately, our current inability to give a complete answer doesn’t prevent us from giving partial answers. Intelligent aliens studying the human sensory system would probably infer that colors are qualia that feel associated with each point on a two-dimensional surface (our visual field), while sounds don’t feel as spatially localized, and pains are qualia that feel associated with different parts of our body. From discovering that our retinas have three types of light-sensitive cone cells, they could infer that we experience three primary colors and that all other color qualia result from combining them. By measuring how long it takes neurons to transmit information across the brain, they could conclude that we experience no more than about ten conscious thoughts or perceptions per second, and that when we watch movies on our TV at twenty-four frames per second, we experience this not as a sequence of still images, but as continuous motion. From measuring how fast adrenaline is released into our bloodstream and how long it remains before being broken down, they could predict that we feel bursts of anger starting within seconds and lasting for minutes. 
Applying similar physics-based arguments, we can make some educated guesses about certain aspects of how an artificial consciousness may feel. First of all, the space of possible AI experiences is huge compared to what we humans can experience. We have one class of qualia for each of our senses, but AIs can have vastly more types of sensors and internal representations of information, so we must avoid the pitfall of assuming that being an AI necessarily feels similar to being a person. 
We’d therefore expect an Earthsized “Gaia” AI to have only about ten conscious experiences per second, like a human, and a galaxy-sized AI could have only one global thought every 100,000 years or so—so no more than about a hundred experiences during the entire history of our Universe thus far! This would give large AIs a seemingly irresistible incentive to delegate computations to the smallest subsystems capable of handling them, to speed things up, much like our conscious mind has delegated the blink reflex to a small, fast and unconscious subsystem. Although we saw above that the conscious information processing in our brains appears to be merely the tip of an otherwise unconscious iceberg, we should expect the situation to be even more extreme for large future AIs: if they have a single consciousness, then it’s likely to be unaware of almost all the information processing taking place within it. Moreover, although the conscious experiences that it enjoys may be extremely complex, they’re also snail-paced compared to the rapid activities of its smaller parts. 
This really brings to a head the aforementioned controversy about whether parts of a conscious entity can be conscious too. IIT predicts not, which means that if a future astronomically large AI is conscious, then almost all its information processing is unconscious. This would mean that if a civilization of smaller AIs improves its communication abilities to the point that a single conscious hive mind emerges, their much faster individual consciousnesses are suddenly extinguished. If the IIT prediction is wrong, on the other hand, the hive mind can coexist with the panoply of smaller conscious minds. Indeed, one could even imagine a nested hierarchy of consciousnesses at all levels from microscopic to cosmic. 
IIT explains this by saying that raw sensory information in System 0 is stored in grid-like brain structures with very high integration, while System 2 has high integration because of feedback loops, where all the information you’re aware of right now can affect your future brain states. On the other hand, it was precisely the conscious-grid prediction that triggered Scott Aaronson’s aforementioned IITcritique. In summary, if a theory solving the pretty hard problem of consciousness can one day pass a rigorous battery of experimental tests so that we start taking its predictions seriously, then it will also greatly narrow down the options for the even harder problem of what future conscious AIs may experience. 
Some aspects of our subjective experience clearly trace back to our evolutionary origins, for example our emotional desires related to selfpreservation (eating, drinking, avoiding getting killed) and reproduction. This means that it should be possible to create AI that never experiences qualia such as hunger, thirst, fear or sexual desire. As we saw in the last chapter, if a highly intelligent AI is programmed to have virtually any sufficiently ambitious goal, it’s likely to strive for self-preservation in order to be able to accomplish that goal. If they’re part of a society of AIs, however, they might lack our strong human fear of death: as long as they’ve backed themselves up, all they stand to lose are the memories they’ve accumulated since their most recent backup, as long as they’re confident that their backed-up software will be used. In addition, the ability to readily copy information and software between AIs would probably reduce the strong sense of individuality that’s so characteristic of our human consciousness: there would be less of a distinction between you and me if we could easily share and copy all our memories and abilities, so a group of nearby AIs may feel more like a single organism with a hive mind. 
Free-will discussions usually center around a struggle to reconcile our goaloriented decision-making behavior with the laws of physics: if you’re choosing between the following two explanations for what you did, then which one is correct: “I asked her on a date because I really liked her” or “My particles made me do it by moving according to the laws of physics”? But we saw in the last chapter that both are correct: what feels like goal-oriented behavior can emerge from goal-less deterministic laws of physics. More specifically, when a system (brain or AI) makes a decision of type 1, it computes what to decide using some deterministic algorithm, and the reason it feels like it decided is that it in fact did decide when computing what to do. Moreover, as emphasized by Seth Lloyd, 34 there’s a famous computer-science theorem saying that for almost all computations, there’s no faster way of determining their outcome than actually running them. This means that it’s typically impossible for you to figure out what you’ll decide to do in a second in less than a second, which helps reinforce your experience of having free will. In contrast, when a system (brain or AI) makes a decision of type 2, it simply programs its mind to base its decision on the output of some subsystem that acts as a random number generator. In brains and computers, effectively random numbers are easily generated by amplifying noise. Regardless of where on the spectrum from 1 to 2 a decision falls, both biological and artificial consciousnesses therefore feel that they have free will: they feel that it is really they who decide and they can’t predict with certainty what the decision will be until they’ve finished thinking it through. 
Let’s end by returning to the starting point of this book: How do we want the future of life to be? We saw in the previous chapter how diverse cultures around the globe all seek a future teeming with positive experiences, but that fascinatingly thorny controversies arise when seeking consensus on what should count as positive and how to make trade-offs between what’s good for different life forms. But let’s not let those controversies distract us from the elephant in the room: there can be no positive experiences if there are no experiences at all, that is, if there’s no consciousness. In other words, without consciousness, there can be no happiness, goodness, beauty, meaning or purpose—just an astronomical waste of space. This implies that when people ask about the meaning of life as if it were the job of our cosmos to give meaning to our existence, they’re getting it backward: It’s not our Universe giving meaning to conscious beings, but conscious beings giving meaning to our Universe. So the very first goal on our wish list for the future should be retaining (and hopefully expanding) biological and/or artificial consciousness in our cosmos, rather than driving it extinct. 
If we succeed in this endeavor, then how will we humans feel about coexisting with ever smarter machines? Does the seemingly inexorable rise of artificial intelligence bother you and if so, why? In chapter 3, we saw how it should be relatively easy for AI-powered technology to satisfy our basic needs such as security and income as long as the political will to do so exists. However, perhaps you’re concerned that being well fed, clad, housed and entertained isn’t enough. If we’re guaranteed that AI will take care of all our practical needs and desires, might we nonetheless end up feeling that we lack meaning and purpose in our lives, like well-kept zoo animals? 
We could retain our families, friends and broader communities, and all activities that give us meaning and purpose, hopefully having lost nothing but arrogance. 
As we plan our future, let’s consider the meaning not only of our own lives, but also of our Universe itself. Here two of my favorite physicists, Steven Weinberg and Freeman Dyson, represent diametrically opposite views. Weinberg, who won the Nobel Prize for foundational work on the standard model of particle physics, famously said, “The more the universe seems comprehensible, the more it also seems pointless.”35 Dyson, on the other hand, is much more optimistic, as we saw in chapter 6: although he agrees that our Universe was pointless, he believes that life is now filling it with ever more meaning, with the best yet to come if life succeeds in spreading throughout the cosmos. He ended his seminal 1979 paper thus: “Is Weinberg’s universe or mine closer to the truth? One day, before long, we shall know.”36 If our Universe goes back to being permanently unconscious because we drive Earth life extinct or because we let unconscious zombie AI take over our Universe, then Weinberg will be vindicated in spades. 
From this perspective, we see that although we’ve focused on the future of intelligence in this book, the future of consciousness is even more important, since that’s what enables meaning. Philosophers like to go Latin on this distinction, by contrasting sapience (the ability to think intelligently) with sentience (the ability to subjectively experience qualia). We humans have built our identity on being Homo sapiens, the smartest entities around. As we prepare to be humbled by ever smarter machines, I suggest that we rebrand ourselves as Homo sentiens! 
We’ve now explored a range of intelligence explosion scenarios, spanning the spectrum from ones that everyone I know wants to avoid to ones that some of my friends view optimistically. Yet all these scenarios have two features in common: 
1. A fast takeoff: the transition from subhuman to vastly superhuman 
intelligence occurs in a matter of days, not decades. 
2. A unipolar outcome: the result is a single entity controlling Earth. 
There is major controversy about whether these two features are likely or unlikely, and there are plenty of renowned AI researchers and other thinkers on both sides of the debate. To me, this means that we simply don’t know yet, and need to keep an open mind and consider all possibilities for now. Let’s therefore devote the rest of this chapter to exploring scenarios with slower takeoffs, multipolar outcomes, cyborgs and uploads. 
There is an interesting link between the two features, as Nick Bostrom and others have highlighted: a fast takeoff can facilitate a unipolar outcome. We saw above how a rapid takeoff gave the Omegas or Prometheus a decisive strategic advantage that enabled them to take over the world before anyone else had time to copy their technology and seriously compete. In contrast, if takeoff had dragged on for decades, because the key technological breakthroughs were incremental and far between, then other companies would have had ample time to catch up, and it would have been much harder for any player to dominate. If competing companies also had software that could perform MTurk tasks, the law of supply and demand would drive the prices for these tasks down to almost nothing, and none of the companies would earn the sort of windfall profits that enabled the Omegas to gain power. The same applies to all the other ways in which the Omegas made quick money: they were only disruptively profitable because they held a monopoly on their technology. It’s hard to double your money daily (or even annually) in a competitive market where your competition offers products similar to yours for almost zero cost. 
Game Theory and Power Hierarchies 
What’s the natural state of life in our cosmos: unipolar or multipolar? Is power concentrated or distributed? After the first 13.8 billion years, the answer seems to be “both”: we find that the situation is distinctly multipolar, but in an interestingly hierarchical fashion. When we consider all information-processing entities out there—cells, people, organizations, nations, etc.—we find that they both collaborate and compete at a hierarchy of levels. Some cells have found it advantageous to collaborate to such an extreme extent that they’ve merged into multicellular organisms such as people, relinquishing some of their power to a central brain. Some people have found it advantageous to collaborate in groups such as tribes, companies or nations where they in turn relinquish some power to a chief, boss or government. Some groups may in turn choose to relinquish some power to a governing body to improve coordination, with examples ranging from airline alliances to the European Union. 
The branch of mathematics known as game theory elegantly explains that entities have an incentive to cooperate where cooperation is a so-called Nash equilibrium: a situation where any party would be worse off if they altered their strategy. To prevent cheaters from ruining the successful collaboration of a large group, it may be in everyone’s interest to relinquish some power to a higher level in the hierarchy that can punish cheaters: for example, people may collectively benefit from granting a government power to enforce laws, and cells in your body may collectively benefit from giving a police force (immune system) the power to kill any cell that acts too uncooperatively (say by spewing out viruses or turning cancerous). For a hierarchy to remain stable, its Nash equilibrium needs to hold also between entities at different levels: for example, if a government doesn’t provide enough benefit to its citizens for obeying it, they may change their strategy and overthrow it. 
In a complex world, there is a diverse abundance of possible Nash equilibria, corresponding to different types of hierarchies. Some hierarchies are more authoritarian than others. In some, entities are free to leave (like employees in most corporate hierarchies), while in others they’re strongly discouraged from leaving (as in religious cults) or unable to leave (like citizens of North Korea, or cells in a human body). Some hierarchies are held together mainly by threats and fear, others mainly by benefits. Some hierarchies allow their lower parts to 
influence the higher-ups by democratic voting, while others allow upward influence only through persuasion or the passing of information. 
How Technology Affects Hierarchies 
How is technology changing the hierarchical nature of our world? History reveals an overall trend toward ever more coordination over ever-larger distances, which is easy to understand: new transportation technology makes coordination more valuable (by enabling mutual benefit from moving materials and life forms over larger distances) and new communication technology makes coordination easier. When cells learned to signal to neighbors, small multicellular organisms became possible, adding a new hierarchical level. When evolution invented circulatory systems and nervous systems for transportation and communication, large animals became possible. Further improving communication by inventing language allowed humans to coordinate well enough to form further hierarchical levels such as villages, and additional breakthroughs in communication, transportation and other technology enabled the empires of antiquity. Globalization is merely the latest example of this multibillion-year trend of hierarchical growth. 
In most cases, this technology-driven trend has made large entities parts of an even grander structure while retaining much of their autonomy and individuality, although commentators have argued that adaptation of entities to hierarchical life has in some cases reduced their diversity and made them more like indistinguishable replaceable parts. Some technologies, such as surveillance, can give higher levels in the hierarchy more power over their subordinates, while other technologies, such as cryptography and online access to free press and education, can have the opposite effect and empower individuals. 
Although our present world remains stuck in a multipolar Nash equilibrium, with competing nations and multinational corporations at the top level, technology is now advanced enough that a unipolar world would probably also be a stable Nash equilibrium. For example, imagine a parallel universe where everyone on Earth shares the same language, culture, values and level of prosperity, and there is a single world government wherein nations function like states in a federation and have no armies, merely police enforcing laws. 
Transportation and communication technology will obviously improve dramatically, so a natural expectation is that the historical trend will continue, with new hierarchical levels coordinating over ever-larger distances—perhaps ultimately encompassing solar systems, galaxies, superclusters and large swaths of our Universe, as we’ll explore in chapter 6. At the same time, the most fundamental driver of decentralization will remain: it’s wasteful to coordinate unnecessarily over large distances. Even Stalin didn’t try to regulate exactly when his citizens went to the bathroom. For superintelligent AI, the laws of physics will place firm upper limits on transportation and communication technology, making it unlikely that the highest levels of the hierarchy would be able to micromanage everything that happens on planetary and local scales. A superintelligent AI in the Andromeda galaxy wouldn’t be able to give you useful orders for your day-to-day decisions given that you’d need to wait over five million years for your instructions (that’s the round-trip time for you to exchange messages traveling at the speed of light). In the same way, the round-trip travel time for a message crossing Earth is about 0.1 second (about the timescale on which we humans think), so an Earth-sized AI brain could have truly global thoughts only about as fast as a human one. For a small AI performing one operation each billionth of a second (which is typical of today’s computers), 0.1 second would feel like four months to you, so for it to be micromanaged by a planet-controlling AI would be as inefficient as if you asked permission for even your most trivial decisions through transatlantic letters delivered by Columbus-era ships. 
This physics-imposed speed limit on information transfer therefore poses an obvious challenge for any AI wishing to take over our world, let alone our Universe. Before Prometheus broke out, it put very careful thought into how to avoid mind fragmentation, so that its many AI modules running on different computers around the world had goals and incentives to coordinate and act as a single unified entity. 
A staple of science fiction is that humans will merge with machines, either by technologically enhancing biological bodies into cyborgs (short for “cybernetic organisms”) or by uploading our minds into machines. In his book The Age of Em, economist Robin Hanson gives a fascinating survey of what life might be like in a world teeming with uploads (also known as emulations, nicknamed Ems). I think of an upload as the extreme end of the cyborg spectrum, where the only remaining part of the human is the software. Hollywood cyborgs range from visibly mechanical, such as the Borg from Star Trek, to androids almost indistinguishable from humans, such as the Terminators. Fictional uploads range in intelligence from human-level as in the Black Mirror episode “White Christmas” to clearly superhuman as in Transcendence. 
If superintelligence indeed comes about, the temptation to become cyborgs or uploads will be strong. As Hans Moravec puts it in his 1988 classic Mind Children: “Long life loses much of its point if we are fated to spend it staring stupidly at ultra-intelligent machines as they try to describe their ever more spectacular discoveries in baby-talk that we can understand.” Indeed, the temptation of technological enhancement is already so strong that many humans have eyeglasses, hearing aids, pacemakers and prosthetic limbs, as well as medicinal molecules circulating in their bloodstreams. Some teenagers appear to be permanently attached to their smartphones, and my wife teases me about my attachment to my laptop. 
One of today’s most prominent cyborg proponents is Ray Kurzweil. In his book The Singularity Is Near, he argues that the natural continuation of this trend is using nanobots, intelligent biofeedback systems and other technology to replace first our digestive and endocrine systems, our blood and our hearts by the early 2030s, and then move on to upgrading our skeletons, skin, brains and the rest of our bodies during the next two decades. 
Further, he argues that we’ll do even better by eliminating the human body entirely and uploading minds, creating a whole-brain emulation in software. Such an upload can live in a virtual reality or be embodied in a robot capable of walking, flying, swimming, space-faring or anything else allowed by the laws of physics, unencumbered by such everyday concerns as death or limited cognitive resources. 
Although these ideas may sound like science fiction, they certainly don’t violate any known laws of physics, so the most interesting question isn’t whether they can happen, but whether they will happen and, if so, when. Some leading thinkers guess that the first human-level AGI will be an upload, and that this is how the path toward superintelligence will begin. * 
However, I think it’s fair to say that this is currently a minority view among AI researchers and neuroscientists, most of whom guess that the quickest route to superintelligence is to bypass brain emulation and engineer it in some other way—after which we may or may not remain interested in brain emulation. After all, why should our simplest path to a new technology be the one that evolution came up with, constrained by requirements that it be self-assembling, self-repairing and self-reproducing? Evolution optimizes strongly for energy efficiency because of limited food supply, not for ease of construction or understanding by human engineers. My wife, Meia, likes to point out that the aviation industry didn’t start with mechanical birds. Indeed, when we finally figured out how to build mechanical birds in 2011, 1 more than a century after the Wright brothers’ first flight, the aviation industry showed no interest in switching to wing-flapping mechanical-bird travel, even though it’s more energy efficient —because our simpler earlier solution is better suited to our travel needs. 
In the same way, I suspect that there are simpler ways to build human-level thinking machines than the solution evolution came up with, and even if we one day manage to replicate or upload brains, we’ll end up discovering one of those simpler solutions first. 
The short answer is obviously that we have no idea what will happen if humanity succeeds in building human-level AGI. For this reason, we’ve spent this chapter exploring a broad spectrum of scenarios. I’ve attempted to be quite inclusive, spanning the full range of speculations I’ve seen or heard discussed by AI researchers and technologists: fast takeoff/slow takeoff/no takeoff, humans/machines/cyborgs in control, one/many centers of power, etc. Some people have told me that they’re sure that this or that won’t happen. However, I think it’s wise to be humble at this stage and acknowledge how little we know, because for each scenario discussed above, I know at least one well-respected AI researcher who views it as a real possibility. 
As time passes and we reach certain forks in the road, we’ll start to answer key questions and narrow down the options. The first big question is “Will we ever create human-level AGI?” The premise of this chapter is that we will, but there are AI experts who think it will never happen, at least not for hundreds of years. Time will tell! As I mentioned earlier, about half of the AI experts at our Puerto Rico conference guessed that it would happen by 2055. At a follow-up conference we organized two years later, this had dropped to 2047. 
Before any human-level AGI is created, we may start getting strong indications about whether this milestone is likely to be first met by computer engineering, mind uploading or some unforeseen novel approach. If the computer engineering approach to AI that currently dominates the field fails to deliver AGI for centuries, this will increase the chance that uploading will get there first, as happened (rather unrealistically) in the movie Transcendence. 
If human-level AGI gets more imminent, we’ll be able to make more educated guesses about the answer to the next key question: “Will there be a fast takeoff, a slow takeoff or no takeoff?” 
Turning to the optimization power, however, it’s overwhelmingly likely that it will grow rapidly as the AGI transcends human level, for the reasons we saw in the Omega scenario: the main input to further optimization comes not from people but from the machine itself, so the more capable it gets, the faster it improves (if recalcitrance stays fairly constant). 
For any process whose power grows at a rate proportional to its current power, the result is that its power keeps doubling at regular intervals. We call such growth exponential, and we call such processes explosions. If baby-making power grows in proportion to the size of the population, we can get a population explosion. If the creation of neutrons capable of fissioning plutonium grows in proportion to the number of such neutrons, we can get a nuclear explosion. If machine intelligence grows at a rate proportional to the current power, we can get an intelligence explosion. All such explosions are characterized by the time they take to double their power. If that time is hours or days for an intelligence explosion, as in the Omega scenario, we have a fast takeoff on our hands. 
This explosion timescale depends crucially on whether improving the AI requires merely new software (which can be created in a matter of seconds, minutes or hours) or new hardware (which might require months or years). In the Omega scenario, there was a significant hardware overhang, in Bostrom’s terminology: the Omegas had compensated for the low quality of their original software by vast amounts of hardware, which meant that Prometheus could perform a large number of quality doublings by improving its software alone. There was also a major content overhang in the form of much of the internet’s data; Prometheus 1.0 was still not smart enough to make use of most of it, but once Prometheus’ intelligence grew, the data it needed for further learning was already available without delay. 
The hardware and electricity costs of running the AI are crucial as well, since we won’t get an intelligence explosion until the cost of doing human-level work drops below human-level hourly wages. 
Turning to the optimization power, however, it’s overwhelmingly likely that it will grow rapidly as the AGI transcends human level, for the reasons we saw in the Omega scenario: the main input to further optimization comes not from people but from the machine itself, so the more capable it gets, the faster it improves (if recalcitrance stays fairly constant). 
For any process whose power grows at a rate proportional to its current power, the result is that its power keeps doubling at regular intervals. We call such growth exponential, and we call such processes explosions. If baby-making power grows in proportion to the size of the population, we can get a population explosion. If the creation of neutrons capable of fissioning plutonium grows in proportion to the number of such neutrons, we can get a nuclear explosion. If machine intelligence grows at a rate proportional to the current power, we can get an intelligence explosion. All such explosions are characterized by the time they take to double their power. If that time is hours or days for an intelligence explosion, as in the Omega scenario, we have a fast takeoff on our hands. 
This explosion timescale depends crucially on whether improving the AI requires merely new software (which can be created in a matter of seconds, minutes or hours) or new hardware (which might require months or years). In the Omega scenario, there was a significant hardware overhang, in Bostrom’s terminology: the Omegas had compensated for the low quality of their original software by vast amounts of hardware, which meant that Prometheus could perform a large number of quality doublings by improving its software alone. There was also a major content overhang in the form of much of the internet’s data; Prometheus 1.0 was still not smart enough to make use of most of it, but once Prometheus’ intelligence grew, the data it needed for further learning was already available without delay. 
The hardware and electricity costs of running the AI are crucial as well, since we won’t get an intelligence explosion until the cost of doing human-level work drops below human-level hourly wages. 
Let’s begin with a scenario where humans peacefully coexist with technology and in some cases merge with it, as imagined by many futurists and science fiction writers alike: 
Life on Earth (and beyond—more on that in the next chapter) is more diverse than ever before. If you looked at satellite footage of Earth, you’d easily be able to tell apart the machine zones, mixed zones and human-only zones. The machine zones are enormous robot-controlled factories and computing facilities devoid of biological life, aiming to put every atom to its most efficient use. Although the machine zones look monotonous and drab from the outside, they’re spectacularly alive on the inside, with amazing experiences occurring in virtual worlds while colossal computations unlock secrets of our Universe and develop transformative technologies. Earth hosts many superintelligent minds that compete and collaborate, and they all inhabit the machine zones. 
The denizens of the mixed zones are a wild and idiosyncratic mix of computers, robots, humans and hybrids of all three. As envisioned by futurists such as Hans Moravec and Ray Kurzweil, many of the humans have technologically upgraded their bodies to cyborgs in various degrees, and some have uploaded their minds into new hardware, blurring the distinction between man and machine. Most intelligent beings lack a permanent physical form. Instead, they exist as software capable of instantly moving between computers and manifesting themselves in the physical world through robotic bodies. Because these minds can readily duplicate themselves or merge, the “population size” keeps changing. Being unfettered from their physical substrate gives such beings a rather different outlook on life: they feel less individualistic because they can trivially share knowledge and experience modules with others, and they feel subjectively immortal because they can readily make backup copies of themselves. In a sense, the central entities of life aren’t minds, but experiences: exceptionally amazing experiences live on because they get continually copied and re-enjoyed by other minds, while uninteresting experiences get deleted by their owners to free up storage space for better ones. 
For example, uploaded versions of Hans Moravec, Ray Kurzweil and Larry Page have a tradition of taking turns creating virtual realities and then exploring them together, but once in a while, they also enjoy flying together in the real world, embodied in avian winged robots. Some of the robots that roam the streets, skies and lakes of the mixed zones are similarly controlled by uploaded and augmented humans, who choose to embody themselves in the mixed zones because they enjoy being around humans and each other. 
In the human-only zones, in contrast, machines with human-level general intelligence or above are banned, as are technologically enhanced biological organisms. Here, life isn’t dramatically different from today, except that it’s more affluent and convenient: poverty has been mostly eliminated, and cures are available for most of today’s diseases. The small fraction of humans who have opted to live in these zones effectively exist on a lower and more limited plane of awareness from everyone else, and have limited understanding of what their more intelligent fellow minds are doing in the other zones. However, many of them are quite happy with their lives. 
The vast majority of all computations take place in the machine zones, which are mostly owned by the many competing superintelligent AIs that live there. By virtue of their superior intelligence and technology, no other entities can challenge their power. These AIs have agreed to cooperate and coordinate with each other under a libertarian governance system that has no rules except protection of private property. These property rights extend to all intelligent entities, including humans, and explain how the human-only zones came to exist. Early on, groups of humans banded together and decided that, in their zones, it was forbidden to sell property to non-humans. 
Because of their technology, the superintelligent AIs have ended up richer than these humans by a factor much larger than that by which Bill Gates is richer than a homeless beggar. However, people in the human-only zones are still materially better off than most people today: their economy is rather decoupled from that of the machines, so the presence of the machines elsewhere has little effect on them except for the occasional useful technologies that they can understand and reproduce for themselves—much as the Amish and various technology-relinquishing native tribes today have standards of living at least as good as they had in old times. It doesn’t matter that the humans have nothing to sell that the machines need, since the machines need nothing in return. 
In the mixed sectors, the wealth difference between AIs and humans is more noticeable, resulting in land (the only human-owned product that the machines want to buy) being astronomically expensive compared to other products. Most humans who owned land therefore ended up selling a small fraction of it to AIs in return for guaranteed basic income for them and their offspring/uploads in perpetuity. This liberated them from the need to work, and freed them up to enjoy the amazing abundance of cheap machine-produced goods and services, in both physical and virtual reality. As far as the machines are concerned, the mixed zones are mainly for play rather than for work. 
If route 1 comes through first, it could naturally lead to a world teeming with cyborgs and uploads. However, as we discussed in the last chapter, most AI researchers think that the opposite is more likely, with enhanced or digital brains being more difficult to build than clean-slate superhuman AGIs—just as mechanical birds turned out to be harder to build than airplanes. After strong machine AI is built, it’s not obvious that cyborgs or uploads will ever be made. If the Neanderthals had had another 100,000 years to evolve and get smarter, things might have turned out great for them—but Homo sapiens never gave them that much time. 
Doomsday Devices 
So could we humans actually pull off omnicide? Even if a global nuclear war may kill off 90% of all humans, most scientists guess that it wouldn’t kill 100% and therefore wouldn’t drive us extinct. On the other hand, the story of nuclear radiation, nuclear EMP and nuclear winter all demonstrate that the greatest hazards may be ones we haven’t even thought of yet. It’s incredibly difficult to foresee all aspects of the aftermath, and how nuclear winter, infrastructure collapse, elevated mutation levels and desperate armed hordes might interact with other problems such as new pandemics, ecosystem collapse and effects we haven’t yet imagined. My personal assessment is therefore that although the probability of a nuclear war tomorrow triggering human extinction isn’t large, we can’t confidently conclude that it’s zero either. 
Omnicide odds increase if we upgrade today’s nuclear weapons into a deliberate doomsday device. Introduced by RAND strategist Herman Kahn in 1960 and popularized in Stanley Kubrick’s film Dr. Strangelove, a doomsday device takes the paradigm of mutually assured destruction to its ultimate conclusion. It’s the perfect deterrent: a machine that automatically retaliates against any enemy attack by killing all of humanity. 
One candidate for the doomsday device is a huge underground cache of socalled salted nukes, preferably humongous hydrogen bombs surrounded by massive amounts of cobalt. Physicist Leo Szilard argued already in 1950 that this could kill everyone on Earth: the hydrogen bomb explosions would render the cobalt radioactive and blow it into the stratosphere, and its five-year half-life is long enough for it to settle all across Earth (especially if twin doomsday devices were placed in opposite hemispheres), but short enough to cause lethal radiation intensity. Media reports suggest that cobalt bombs are now being built for the first time. Omnicidal opportunities could be bolstered by adding bombs optimized for nuclear winter creation by maximizing long-lived aerosols in the stratosphere. A major selling point of a doomsday device is that it’s much cheaper than a conventional nuclear deterrent: since the bombs don’t need to be launched, there’s no need for expensive missile systems, and the bombs themselves are cheaper to build since they need not be light and compact enough to fit into missiles. 
Second, even if this scenario with cyborgs and uploads did come about, it’s not clear that it would be stable and last. Why should the power balance between multiple superintelligences remain stable for millennia, rather than the AIs merging or the smartest one taking over? Moreover, why should the machines choose to respect human property rights and keep humans around, given that they don’t need humans for anything and can do all human work better and cheaper themselves? Ray Kurzweil speculates that natural and enhanced humans will be protected from extermination because “humans are respected by AIs for giving rise to the machines.”1 However, as we’ll discuss in chapter 7, we must not fall into the trap of anthropomorphizing AIs and assume that they have human-like emotions of gratitude. Indeed, though we humans are imbued with a propensity toward gratitude, we don’t show enough gratitude to our intellectual creator (our DNA) to abstain from thwarting its goals by using birth control. 
Even if we buy the assumption that the AIs will opt to respect human property rights, they can gradually get much of our land in other ways, by using some of their superintelligent persuasion powers that we explored in the last chapter to persuade humans to sell some land for a life in luxury. 
For some of their most ardent supporters, cyborgs and uploads hold a promise of techno-bliss and life extension for all. Indeed, the prospect of getting uploaded in the future has motivated over a hundred people to have their brains posthumously frozen by the Arizona-based company Alcor. If this technology arrives, however, it’s far from clear that it will be available to everybody. Many of the very wealthiest would presumably use it, but who else? Even if the technology got cheaper, where would the line be drawn? Would the severely brain-damaged be uploaded? Would we upload every gorilla? Every ant? Every plant? Every bacterium? Would the future civilization act like obsessivecompulsive hoarders and try to upload everything, or merely a few interesting examples of each species in the spirit of Noah’s Ark? Perhaps only a few representative examples of each type of human? To the vastly more intelligent entities that would exist at that time, an uploaded human may seem about as interesting as a simulated mouse or snail would seem to us. Although we currently have the technical capability to reanimate old spreadsheet programs from the 1980s in a DOS emulator, most of us don’t find this interesting enough to actually do it. 
Many people may dislike this libertarian-utopia scenario because it allows preventable suffering. Since the only sacred principle is property rights, nothing prevents the sort of suffering that abounds in today’s world from continuing in the human and mixed zones. While some people thrive, others may end up living in squalor and indentured servitude, or suffer from violence, fear, repression or depression. For example, Marshall Brain’s 2003 novel Manna describes how AI progress in a libertarian economic system makes most Americans unemployable and condemned to live out the rest of their lives in drab and dreary robotoperated social-welfare housing projects. Much like farm animals, they’re kept fed, healthy and safe in cramped conditions where the rich never need to see them. Birth control medication in the water ensures that they don’t have children, so most of the population gets phased out to leave the remaining rich with larger shares of the robot-produced wealth. 
How bad would it be if 90% of humans get killed? How much worse would it be if 100% get killed? Although it’s tempting to answer the second question with “10% worse,” this is clearly inaccurate from a cosmic perspective: the victims of human extinction wouldn’t be merely everyone alive at the time, but also all descendants that would otherwise have lived in the future, perhaps during billions of years on billions of trillions of planets. On the other hand, human extinction might be viewed as somewhat less horrible by religions according to which humans go to heaven anyway, and there isn’t much emphasis on billionyear futures and cosmic settlements. 
Most people I know cringe at the thought of human extinction, regardless of religious persuasion. Some, however, are so incensed by the way we treat people and other living beings that they hope we’ll get replaced by some more intelligent and deserving life form. In the movie The Matrix, Agent Smith (an AI) articulates this sentiment: “Every mammal on this planet instinctively develops a natural equilibrium with the surrounding environment but you humans do not. You move to an area and you multiply and multiply until every natural resource is consumed and the only way you can survive is to spread to another area. There is another organism on this planet that follows the same pattern. Do you know what it is? A virus. Human beings are a disease, a cancer of this planet. You are a plague and we are the cure.” 
But would a fresh roll of the dice necessarily be better? A civilization isn’t necessarily superior in any ethical or utilitarian sense just because it’s more powerful. “Might makes right” arguments to the effect that stronger is always better have largely fallen from grace these days, being widely associated with fascism. Indeed, although it’s possible that the conqueror AIs may create a civilization whose goals we would view as sophisticated, interesting and worthy, it’s also possible that their goals will turn out to be pathetically banal, such as maximizing the production of paper clips. The deliberately silly example of a paper-clip-maximizing superintelligence was given by Nick Bostrom in 2003 to make the point that the goal of an AI is independent of its intelligence (defined as its aptness at accomplishing whatever goal it has). The only goal of a chess computer is to win at chess, but there are also computer tournaments in so-called losing chess, where the goal is the exact opposite, and the computers competing there are about as smart as the more common ones programmed to win. We humans may view it as artificial stupidity rather than artificial intelligence to want to lose at chess or turn our Universe into paper clips, but that’s merely because we evolved with preinstalled goals valuing such things as victory and survival—goals that an AI may lack. The paper clip maximizer turns as many of Earth’s atoms as possible into paper clips and rapidly expands its factories into the cosmos. It has nothing against humans, and kills us merely because it needs our atoms for paper clip production. 
If paper clips aren’t your thing, consider this example, which I’ve adapted from Hans Moravec’s book Mind Children. We receive a radio message from an extraterrestrial civilization containing a computer program. When we run it, it turns out to be a recursively self-improving AI which takes over the world much like Prometheus did in the previous chapter—except that no human knows its ultimate goal. It rapidly turns our Solar System into a massive construction site, covering the rocky planets and asteroids with factories, power plants and supercomputers, which it uses to design and build a Dyson sphere around the Sun that harvests all its energy to power solar-system-sized radio antennas. *3 This obviously leads to human extinction, but the last humans die convinced that there’s at least a silver lining: whatever the AI is up to, it’s clearly something cool and Star Trek–like. Little do they realize that the sole purpose of the entire construction is for these antennas to rebroadcast the same radio message that the humans received, which is nothing more than a cosmic version of a computer virus. Just as email phishing today preys on gullible internet users, this message preys on gullible biologically evolved civilizations. It was created as a sick joke billions of years ago, and although the entire civilization of its maker is long extinct, the virus continues spreading through our Universe at the speed of light, transforming budding civilizations into dead, empty husks. How would you feel about being conquered by this AI? Let’s now consider a human-extinction scenario that some people may feel better about: viewing the AI as our descendants rather than our conquerors. Hans Moravec supports this view in his book Mind Children: “We humans will benefit for a time from their labors, but sooner or later, like natural children, they will seek their own fortunes while we, their aged parents, silently fade away.” 
Parents with a child smarter than them, who learns from them and accomplishes what they could only dream of, are likely happy and proud even if they know they can’t live to see it all. In this spirit, AIs replace humans but give us a graceful exit that makes us view them as our worthy descendants. Every human is offered an adorable robotic child with superb social skills who learns from them, adopts their values and makes them feel proud and loved. Humans are gradually phased out via a global one-child policy, but are treated so exquisitely well until the end that they feel they’re in the most fortunate generation ever. 
How would you feel about this? After all, we humans are already used to the idea that we and everyone we know will be gone one day, so the only change here is that our descendants will be different and arguably more capable, noble and worthy. 
Moreover, the global one-child policy may be redundant: as long as the AIs eliminate poverty and give all humans the opportunity to live full and inspiring lives, falling birthrates could suffice to drive humanity extinct, as mentioned earlier. Voluntary extinction may happen much faster if the AI-fueled technology keeps us so entertained that almost nobody wants to bother having children. For example, we already encountered the Vites in the egalitarian-utopia scenario who were so enamored with their virtual reality that they had largely lost interest in using or reproducing their physical bodies. Also in this case, the last generation of humans would feel that they were the most fortunate generation of all time, relishing life as intensely as ever right up until the very end. 
You began this chapter pondering where you want the current AGI race to lead. Now that we’ve explored a broad range of scenarios together, which ones appeal to you and which ones do you think we should try hard to avoid? Do you have a clear favorite? Please let me and fellow readers know at http://AgeOfAi.org, and join the discussion! 
The scenarios we’ve covered obviously shouldn’t be viewed as a complete list, and many are thin on details, but I’ve tried hard to be inclusive, spanning the full spectrum from high-tech to low-tech to no-tech and describing all the central hopes and fears expressed in the literature. 
One of the most fun parts of writing this book has been hearing what my friends and colleagues think of these scenarios, and I’ve been amused to learn that there’s no consensus whatsoever. The one thing everybody agrees on is that the choices are more subtle than they may initially seem. People who like any one scenario tend to simultaneously find some aspect(s) of it bothersome. To me, this means that we humans need to continue and deepen this conversation about our future goals, so that we know in which direction to steer. The future potential for life in our cosmos is awe-inspiringly grand, so let’s not squander it by drifting like a rudderless ship, clueless about where we want to go! 
Just how grand is this future potential? No matter how advanced our technology gets, the ability for Life 3.0 to improve and spread through our cosmos will be limited by the laws of physics—what are these ultimate limits, during the billions of years to come? Is our Universe teeming with extraterrestrial life right now, or are we alone? What happens if different expanding cosmic civilizations meet? We’ll tackle these fascinating questions in the next chapter. the ultimate limits? How much of our cosmos can come alive? How far can life reach and how long can it last? How much matter can life make use of, and how much energy, information and computation can it extract? These ultimate limits are set not by our understanding, but by the laws of physics. This, ironically, makes it in some ways easier to analyze the long-term future of life than the short-term future. 
If our 13.8-billion-year cosmic history were compressed into a week, then the 10,000-year drama of the last two chapters would be over in less than half a second. This means that although we cannot predict if and how an intelligence explosion will unfold and what its immediate aftermath will be like, all this turmoil is merely a brief flash in cosmic history whose details don’t affect life’s ultimate limits. If the post-explosion life is as obsessed as today’s humans are with pushing limits, then it will develop technology to actually reach these limits —because it can. In this chapter, we’ll explore what these limits are, thus getting a glimpse of what the long-term future of life may be like. Since these limits are based on our current understanding of physics, they should be viewed as a lower bound on the possibilities: future scientific discoveries may present opportunities to do even better. 
But do we really know that future life will be so ambitious? No, we don’t: perhaps it will become as complacent as a heroin addict or a couch potato merely watching endless reruns of Keeping Up with the Kardashians. However, there is reason to suspect that ambition is a rather generic trait of advanced life. Almost regardless of what it’s trying to maximize, be it intelligence, longevity, knowledge or interesting experiences, it will need resources. It therefore has an incentive to push its technology to the ultimate limits, to make the most of the resources it has. After this, the only way to further improve is to acquire more resources, by expanding into ever-larger regions of the cosmos. 
Also, life may independently originate in multiple places in our cosmos. In that case, unambitious civilizations simply become cosmically irrelevant. When it comes to the future of life, one of the most hopeful visionaries is Freeman Dyson. I’ve had the honor and pleasure of knowing him for the past two decades, but when I first met him, I felt nervous. I was a junior postdoc chowing away with my friends in the lunchroom of the Institute for Advanced Study in Princeton, and out of the blue, this world-famous physicist who used to hang out with Einstein and Gödel came up and introduced himself, asking if he could join us! He quickly put me at ease, however, by explaining that he preferred eating lunch with young folks over stuffy old professors. Even though he’s ninety-three as I type these words, Freeman is still younger in spirit than most people I know, and the mischievous boyish glint in his eyes reveals that he couldn’t care less about formalities, academic hierarchies or conventional wisdom. The bolder the idea, the more excited he gets. 
When we talked about energy use, he scoffed at how unambitious we humans were, pointing out that we could meet all our current global energy needs by harvesting the sunlight striking an area smaller than 0.5% of the Sahara desert. But why stop there? Why even stop at capturing all the sunlight striking Earth, letting most of it get wastefully beamed into empty space? Why not simply put all the Sun’s energy output to use for life? So how did this amazing awakening come about? It wasn’t an isolated event, but merely one step in a relentless 13.8-billion-year process that’s making our Universe ever more complex and interesting—and is continuing at an accelerating pace. 
As a physicist, I feel fortunate to have gotten to spend much of the past quarter century helping to pin down our cosmic history, and it’s been an amazing journey of discovery. Since the days when I was a graduate student, we’ve gone from arguing about whether our Universe is 10 or 20 billion years old to arguing about whether it’s 13.7 or 13.8 billion years old, thanks to a combination of better telescopes, better computers and better understanding. We physicists still don’t know for sure what caused our Big Bang or whether this was truly the beginning of everything or merely the sequel to an earlier stage. However, we’ve acquired a rather detailed understanding of what’s happened since our Big Bang, thanks to an avalanche of high-quality measurements, so please let me take a few minutes to summarize 13.8 billion years of cosmic history. 
In the beginning, there was light. In the first split second after our Big Bang, the entire part of space that our telescopes can in principle observe (“our observable Universe,” or simply “our Universe” for short) was much hotter and brighter than the core of our Sun and it expanded rapidly. Although this may sound spectacular, it was also dull in the sense that our Universe contained nothing but a lifeless, dense, hot and boringly uniform soup of elementary particles. Things looked pretty much the same everywhere, and the only interesting structure consisted of faint random-looking sound waves that made the soup about 0.001% denser in some places. These faint waves are widely believed to have originated as so-called quantum fluctuations, because Heisenberg’s uncertainty principle of quantum mechanics forbids anything from being completely boring and uniform. 
As our Universe expanded and cooled, it grew more interesting as its particles combined into ever more complex objects. The question of how to define life is notoriously controversial. Competing definitions abound, some of which include highly specific requirements such as being composed of cells, which might disqualify both future intelligent machines and extraterrestrial civilizations. Since we don’t want to limit our thinking about the future of life to the species we’ve encountered so far, let’s instead define life very broadly, simply as a process that can retain its complexity and replicate. What’s replicated isn’t matter (made of atoms) but information (made of bits) specifying how the atoms are arranged. When a bacterium makes a copy of its DNA, no new atoms are created, but a new set of atoms are arranged in the same pattern as the original, thereby copying the information. In other words, we can think of life as a self-replicating information-processing system whose information (software) determines both its behavior and the blueprints for its hardware. 
Like our Universe itself, life gradually grew more complex and interesting, *1 and as I’ll now explain, I find it helpful to classify life forms into three levels of sophistication: Life 1.0, 2.0 and 3.0. I’ve summarized these three levels in figure 1.1. 
It’s still an open question how, when and where life first appeared in our Universe, but there is strong evidence that here on Earth life first appeared about 4 billion years ago. Before long, our planet was teeming with a diverse panoply of life forms. The most successful ones, which soon outcompeted the rest, were able to react to their environment in some way. Specifically, they were what computer scientists call “intelligent agents”: entities that collect information about their environment from sensors and then process this information to decide how to act back on their environment. This can include highly complex information processing, such as when you use information from your eyes and ears to decide what to say in a conversation. But it can also involve hardware and software that’s quite simple. 
Inspired by Olaf Stapledon’s 1937 sci-fi classic Star Maker, with rings of artificial worlds orbiting their parent star, Freeman Dyson published a description in 1960 of what became known as a Dyson sphere. 1 Freeman’s idea was to rearrange Jupiter into a biosphere in the form of a spherical shell surrounding the Sun, where our descendants could flourish, enjoying 100 billion times more biomass and a trillion times more energy than humanity uses today. 2 He argued that this was the natural next step: “One should expect that, within a few thousand years of its entering the stage of industrial development, any intelligent species should be found occupying an artificial biosphere which completely surrounds its parent star.” If you lived on the inside of a Dyson sphere, there would be no nights: you’d always see the Sun straight overhead, and all across the sky, you’d see sunlight reflecting off the rest of the biosphere, just as you can nowadays see sunlight reflecting off the Moon during the day. If you wanted to see stars, you’d simply go “upstairs” and peer out at the cosmos from the outside of the Dyson sphere. 
There is only a very limited family of stationary, asymptotically flat, black hole solutions to the Einstein
equations. Such a spacetime is one that has an event horizon and a Killing vector that is timelike at infinity.
A static spacetime is a stationary one that also has a time reflection symmetry. Thus a rotating black hole
is stationary but not static, whereas a nonrotating one is static.
A number of black hole uniqueness theorems have been proved under various reasonably well motivated
assumptions. The EF metric (1.2) gives the unique static vacuum solution with an event horizon. The
only stationary vacuum solution with a horizon is the Kerr solution, parametrized by the total mass M and
angular momentum J . Including an electromagnetic field, the only static solution with a horizon with one
connected component is the Reissner-Nordstrom solution parametrized by mass and electric and magnetic
charges Qe, Qm. Since the electromagnetic stress-energy tensor is duality rotation invariant, the metric
depends only on the combination Q2
e + Q2
m. Finally, allowing for angular momentum, the unique stationary
black hole solution with electromagnetic field is the Kerr-Newman metric.
1.3 Positive energy theorem
Energy of an isolated (asymptotically flat) system in GR can be defined as the gravitating mass as measured
at infinity, times c2. This energy, which is the numerical value of the Hamiltonian that generates the time
translation symmetry at infinity, is a conserved quantity in general relativity. The energy can be negative
e.g. if we simply put rs < 0 in the Eddington-Finkelstein line element, but this yields a naked singularity. If
one assumes (i) spacetime can be spanned by a nonsingular Cauchy surface whose only boundary is the one
at infinity, and (ii) matter has positive energy (more precisely, the stress-energy tensor satisfies the dominant
energy condition, which for diagonalizable Tab means that the energy density is greater than the magnitude
of any principal pressure), then it can be proved that the total energy of the spacetime is necessarily positive.
This was first proved in a geometrical way by Schoen and Yau, and shortly thereafter proved in a more direct
way way by Witten. The idea for this proof came from quantum supergravity, where the Hamiltonian has
the manifestly positive form H= Q2 in terms of the supersymmetry generator Q.
Witten’s proof goes roughly as follows. The energy is written as a flux integral involving first derivative
of the metric at infinity which picks oﬀ the coeﬃcient of the 1/r term in the metric. This is sometimes
called the ADM energy. This is then reexpressed, using the Einstein equations, as a volume integral over
a spacelike Cauchy surface with an integrand containing a term quadratic in the derivative of an arbitrary
spinor field and a term in the energy density of matter. If the spinor field is chosen to satisfy a certain
elliptic diﬀerential equation, then the quadratic spinor term becomes manifestly positive. The only zero
5
energy solution is empty flat spacetime. If a black hole is present then the Cauchy surface can be chosen to
dip below the formation of the event horizon, thus avoiding the presence of an inner boundary or singularity
on the surface. Alternatively, the contribution from an inner boundary located at an apparent horizon can
be shown to be positive.
Positivity of the total energy at infinity does not necessarily mean that the system cannot radiate an
infinite energy while collapsing, since both the energy of the radiation and the energy of the leftover system are
included in the total energy. A diﬀerent definition of energy, called the Bondi energy, allows one to evaluate
just the “leftover” energy. The Bondi energy is the gravitating mass as seen by light rays propagating out
to infinity in the lightlike direction, rather than the spacelike direction. Essentially the same argument as
before shows that the Bondi energy is also necessarily nonnegative. Thus only a finite energy can be radiated
away.
A positive energy theorem has also been proved in the presence of a negative cosmological constant, in
which case the asymptotic structure of the spacetime is anti-de-Sitter rather than flat. One might have thought that the singularity at r = 0 is just an artifact of perfect spherical symmetry,
that in an asymmetric collapse most of the mass would “miss” rather than collide and no infinite density or
curvature would develop. A strong suggestion that this is not the case comes from the fact that the angular
momentum barrier for orbits of test particles in a black hole spacetime gives way to a negative 1/r3-term of
purely relativistic origin which produces an infinite well as r goes to zero. That it is in fact not true was
proved by Penrose.
The idea of Penrose’s proof rests on the concept of a trapped surface. This is a closed, spacelike, 2-surface
whose ingoing and outgoing null normal congruences are both converging (see Fig. 1.3). For example, a
sphere at constant r and v in Eddington-Finkelstein coordinates is a trapped surface if it lies inside the
horizon. But even in a somewhat asymmetrical collapse it is expected that a trapped surface will form.
Penrose argues that the existence of a trapped surface T implies the existence of a singularity on the
boundary ∂F of its future F . (The “future” of a set is the collection of all spacetime points that can be
reached by future-going timelike or null curves from that set.) Very roughly his reasoning is this: the null
normals to T start out converging everywhere so, since gravity is attractive, they must continue converging
and will necessarily reach crossing points (technically, conjugate points) in a finite aﬃne parameter. ∂F must
“end” before or when the crossing points are reached (because the boundary ∂F must be locally tangent to
the light cones) so ∂F must be compact. This is a very weird structure for the boundary of the future of
T , and in fact is incompatible with other reasonable requirements on the spacetime (see below). The only
way out is if at least one of the null normals cannot be extended far enough to reach its crossing point. This
nonextendibility is what is meant in the theorem by the existence of a singularity.
Einstein’s equation comes into the proof only in ensuring that the initially converging null normals to T
must reach a crossing point in a finite aﬃne parameter. It is worth explaining this in more detail, since it
involves technology that figures in many developments in general relativity and black hole thermodynamics,
namely, the focusing equation (which is often called the Raychaudhuri equation, or Sach’s equation, or
Newman-Penrose equation). This equation relates the focusing of a bundle of light rays (called a null. This focusing equation shows that an initially converging congruence must reach a “crossing point”, i.e.
a point where ρ diverges, in a finite aﬃne parameter provided Rabkakb ≥ 0. More precisely, d
dλ ρ ≥ 1
2 ρ2
implies that if ρ(0) = ρ0 > 0, then ρ → ∞ for some λ ≤ 2/ρ0. In flat space this would of course be true, and
if positive the Ricci tensor term will only make it converge faster. The condition Rabkakb ≥ 0 is equivalent
via Einstein’s equation to the condition Tabkakb ≥ 0, which for a diagonalizable stress-energy tensor is
equivalent to the condition that the energy density plus any of the three principal pressures is positive. Thus
unless there is “anti-gravitational repulsion” due to negative energy and/or pressure, a crossing point must
be reached.
A somewhat more precise statement of Penrose’s theorem is that a singularity is unavoidable if there is a
trapped surface and (i) Rabkakb ≥ 0 for all null ka and (ii) spacetime has the form M = Σ × R, where Σ is
a non-compact, connected, Cauchy surface. Later Hawking and Penrose gave another proof that weakened
the second assumption, replacing it by the conditions that (ii′) there are no closed timelike curves and (ii′′)
the curvature is “generic” in a certain extremely mild sense. In the examples above the most eﬃcient energy extraction occurs when the black hole area is unchanged,
and in less eﬃcient processes the area always increases. It was shown by Hawking that in fact the area of an
event horizon can never decrease under quite general assumptions. Hawking’s theorem applies to arbitrary
dynamical black holes, for which a general definition of the horizon is needed. The future event horizon of
an asymptotically flat black hole spacetime is defined as the boundary of the past of future null infinity,
that is, the boundary of the set of points that can communicate with the remote regions of the spacetime
to the future. Hawking proved that if Rabkakb ≥ 0, and if there are no naked singularities (i.e. if “cosmic
censorship” holds), the cross sectional area of a future event horizon cannot be decreasing anywhere. The
reason is that the focusing equation implies that if the horizon generators are converging somewhere then
they will reach a crossing point in a finite aﬃne parameter. But such a point cannot lie on a future event
horizon (because the horizon must be locally tangent to the light cones), nor can the generators leave the
horizon. The only remaining possibility is that the generators cannot be extended far enough to reach the
crossing point—that is, they must reach a singularity.
That was an easy argument, but it isn’t as strong as one would like, since the singularity may not be
10
naked, i.e. visible from infinity, and we have no good reason to assume clothed (or barely clothed) singularities
do not occur.1 With a more subtle argument, Hawking showed that convergence of the horizon generators
does imply existence of a naked singularity. The basic idea is to deform the horizon cross-section outward a
bit from the point where the generators are assumed to be converging, and to consider the boundary of the
future of the part of the deformed cross-section that lies outside the horizon. If the deformation is suﬃciently
small, all of the generators of this boundary are initially converging and therefore reach crossing points and
leave the boundary at finite aﬃne parameter. But at least one of these generators must reach infinity while
remaining on the boundary, since the deformed cross-section is outside the event horizon. The only way out
of the contradiction is if there is a singularity outside the horizon, on the boundary, which is visible from
infinity and therefore naked.
Essentially the same argument as the one just given also establishes that an outer trapped surface must
not be visible from infinity, i.e. must lie inside an event horizon. This fact is used sometimes as an indirect
way to probe numerical solutions of the Einstein equation for the presence of an event horizon. Whereas
the event horizon is a nonlocal construction in time, and so can not be directly identified given only a finite
time interval, a trapped surface is defined locally and may be unambiguously identified at a single time.
Assuming cosmic censorship, the presence of a trapped surface implies the existence of a horizon. From the forgoing it is apparent that energy can flow not just into black holes but also out of them, and
they can act as an intermediary in energy exchange processes. Energy extraction is maximally eﬃcient when
the horizon area does not change, and processes that increase the area are irreversible, since the area cannot
decrease. The analogy with thermodynamic behavior is striking, with the horizon area playing the role of
entropy. This analogy was vigorously pursued as soon as it was recognized at the beginning of the 1970’s,
although it had what appeared at first to be several glaring flaws:
F1. the temperature of a black hole vanishes;
F2. entropy is dimensionless, whereas horizon area is a length squared;
F3. the area of every black hole is separately non-decreasing, whereas only the total entropy is non-
decreasing in thermodynamics.
By 1975 it was understood that the resolution to all of these flaws lies in the incorporation of quantum
theory, as has so often been the case in resolving thermodynamic conundrums. A black hole has a Hawking
temperature proportional to Planck’s constant ¯ h, the entropy is one fourth the horizon area divided by the
Planck length squared (¯ hG/c3), and the area can decrease via Hawking radiation.
Rather than jumping now immediately into the subject of quantum black hole thermodynamics, it is
worth discussing first the classical aspects of the theory. These are important in their own right, and they
form the foundation for quantum black hole thermodynamics. But also it is intriguing to see what can
be inferred without invoking quantum theory, and it may teach us something about the deeper origins of
gravitation. In proceeding this way we are following more or less the path that was taken historically. It turns out that this missing term is given by κdA/8πG, where κ is the surface gravity of the horizon. The
surface gravity of a stationary black hole can be defined assuming the event horizon is a Killing horizon, i.e.
that the null horizon generators are orbits of a Killing field. (See next section for more on this assumption.)
Then κ is defined as the magnitude of the gradient of the norm of the horizon generating Killing field
χa = ξa + Ωψa, evaluated at the horizon. That is,
κ2 :=−(∇a|χ|)(∇a|χ|) (2.1)
at the horizon. An equivalent definition of κ is the the magnitude of the acceleration, with respect to Killing
time, of a stationary zero angular momentum particle just outside the horizon. This is the same as the force
per unit mass that must be applied at infinity in order to hold the particle on its path. For a nonrotating
neutral black hole the surface gravity is given by 1/4M , so a larger black hole has a smaller surface gravity.
This happens to be identical to the Newtonian surface gravity of a spherical mass M with radius equal to
the Schwarzschild radius 2M. 
2.1.2 Zeroth Law
Although κ is defined locally on the horizon, it turns out that it is always constant over the horizon of a
stationary black hole. This constancy is reminiscent of the Zeroth Law of thermodynamics which states that
the temperature is uniform everywhere in a system in thermal equilibrium. The constancy of κ can be traced
to the special properties of the horizon of a stationary black hole. It can be proved without field equations
or energy conditions [Carter, R´acz & Wald] assuming the horizon is a Killing horizon (i.e. there is a Killing
field tangent to the null generators of the horizon) and that the black hole is either (i) static (i.e. stationary
and time reflection symmetric), or (ii) axisymmetric and “t-φ” reflection symmetric. Alternatively, it can
be proved [Hawking] assuming only stationarity together with the Einstein field equation with the dominant
energy condition for matter. (Assuming also hyperbolic field equations for matter, and analyticity of the
spacetime, Hawking also shows that the event horizon must be a Killing horizon, and that the spacetime
must be either static or axisymmetric.) Bekenstein proposed that some multiple ηA/¯ hG of the black hole area, measured in units of the squared
Planck length L2
p
= ¯ hG/c3, is actually entropy, and he conjectured a generalized second law (GSL) which
states that the sum of the entropy outside the black hole and the entropy of the black hole itself will never
decrease:
δ(Soutside + ηA/¯ hG) ≥ 0 (2.9)
Classically, it seems possible to violate the GSL, using processes like those already considered: A box
containing entropy in the form of, say, radiation, can be lowered to the horizon of a black hole and dropped
in. For an ideal, infinitesimal box all of the energy can be extracted at infinity, so when the box is dropped
in it adds no mass to the hole. Thus the horizon area does not change, but the entropy of the exterior has
decreased, violating the GSL. This may be considered yet another flaw in the thermodynamic analogy:
F4. the GSL can be violated by adding entropy to a black hole without changing its area.
At the purely classical level, it thus appears that the GSL is simply not true. Note however that as ¯ h → 0,
the entropy ηA/¯ hG diverges, and an infinitesimal area change can make a finite change in the Bekenstein
entropy. The other flaws (F1-F3) in the thermodynamic analogy are also in a sense resolved in the ¯ h → 0
limit. F2 is resolved by Bekenstein’s postulate, while F3 is resolved because a finite decrease in area would
imply an infinite decrease in entropy. Furthermore, the first law implies that the black hole has a Bekenstein
temperature TB = ¯ hκ/8πη, which vanishes in the classical limit, thus resolving flaw F1. The Bekenstein
proposal therefore “explains” the apparent flaws in the thermodynamic analogy, and it suggests very strongly
that the analogy is much more than an analogy. It turns out that, with quantum eﬀects included, the GSL
is indeed true after all, with the coeﬃcient η equal to 1/4. The analogy between surface gravity and temperature was based in the above discussion on the way the
temperature enters the First Law (2.2), the fact that it is constant over the horizon (Zeroth Law), and the
fact that it is (probably) impossible to reduce it to zero in a physical process (Third Law). In this section
we discuss a sense in which a black hole has a thermodynamic temperature, defined in terms of the eﬃciency
of heat engines, that is proportional to its surface gravity. The discussion is a variation on that of [Sciama,
1976, see section 1.1.4].
A thermodynamic definition of temperature can be given by virtue of the second law in the (Clausius)
form which states that it is impossible to pump heat from a colder body to a hotter one in a cycle with no
other changes. Given this Second Law, the ratio Qin/Qout of the heat in to the heat out in any reversible
heat engine cycle operating between two heat baths must be a universal constant characteristic of that pair
of equilibrium states. The ratio of the thermodynamic temperatures of the two equilibrium states is then
defined by Tin /Tout := Qin/Qout. This defines the temperature of all equilibrium states up to an overall
arbitrary constant. In a heat engine, the heat out is wasted, so the most eﬃcient engine is one which dumps
its heat into the coldest reservoir. Applying this definition to a black hole, it follows that the temperature of the hole must be zero, since as
we have seen one can, with perfect eﬃciency, extract the entire rest mass of a particle (or of heat) as useful
work by dumping the heat into a black hole after lowering it down all the way to the horizon. Note however
that to arrive at this conclusion we must take the unphysical limit of really lowering the heat precisely all
the way to the horizon.
A meaningful expression for the ratio of the temperatures of two black holes can be obtained by passing to
this unphysical limit in a fairly natural manner. Consider operating a heat engine of the type just discussed
between two black holes separated very far from one another, and suppose there is a minimum proper
distance dmin to which the horizon of either black hole is approached. We shall assume that this distance is
the same for both black holes, and take the limit as dmin → 0. We also assume for simplicity that the black
holes are nonrotating; it is presumably possible to generalize the analysis to the rotating case.
If the “heat” has a rest mass m, it has Killing energy E1 = ξ1m at its lowest point outside the horizon
of the first black hole, where ξ is the norm of the Killing field. The heat is then lifted slowly and lowered
back down to just outside the horizon of the second black hole, where it has Killing energy E2 = ξ2m, and is
then dumped into the second hole. The diﬀerence E1− E2 is the useful work extracted in the process, and
the ratio T1/T2 := E1/E2 = ξ1/ξ2 defines the ratio of the thermodynamic temperatures of the two holes.
Now near the horizon we can approximate ξ ≃ κdmin, where κ is exactly the surface gravity that entered
above in the First Law. At the lowest points we thus have ξ1/ξ2 ≃ κ1/κ2, which becomes exact in the limit
dmin → 0, so that T1/T2 = κ1/κ2. That is, the thermodynamic temperature of a black hole is proportional
to its surface gravity.
This derivation hinges on the limiting procedure, in which a common minimum distance of approach to
the horizon taken to zero, which is not very well motivated. It is therefore worth pointing out that this is
equivalent to taking a common maximum proper acceleration to infinity. The proper acceleration of a static
worldline is given by a = κ/ξ in the limit that the horizon is approached, so a is just the inverse of the
proper distance from the horizon. Alternatively, rather than taking a limit as the horizon is approached,
one might imagine that there is some common minimum distance of approach or maximum acceleration to
which the heat will be subjected in any given transfer process. Classical black hole physics cries out for the incorporation of ¯ h eﬀects, so the thermodynamic “analogy”
can become true thermodynamics. Since general relativity is relativistic, it is not quantum mechanics but
relativistic quantum field theory that is called for. Thus, in principle, one should consider “quantum gravity”,
whatever that may be. Although no one knows for sure what quantum gravity actually is, formal treatment
of its semiclassical limit by Gibbons and Hawking in a path integral framework revealed one way in which
the analogy can become an identity. This will be discussed later. An alternate semiclassical approach—and
historically the first— is to consider quantum fields in a fixed black hole background. A quantum field
has vacuum fluctuations that permeate all of spacetime, so there is always something going on, even in the
“empty space” around a black hole. Thus turning on the vacuum fluctuations of quantum fields can have
a profound eﬀect on the thermodynamics of black holes. The principal eﬀect is the existence of Hawking
radiation.
The historical route to Hawking’s discovery is worth mentioning. (See Thorne’s book, Black Holes and
Time Warps, for an interesting account.) After the Penrose process was invented, it was only a short step
to consider a similar process using waves rather than particles [Zel’dovich, Misner], a phenomenon dubbed
“super-radiance”. Quantum mechanically, supperradiance corresponds to stimulated emission, so it was then
natural to ask whether a rotating black hole would spontaneously radiate [Zel’dovich, Starobinsky, Unruh].
In trying to improve on the calculations in favor of spontaneous emission, Hawking stumbled onto the fact
that even a non-rotating black hole would emit particles, and it would do so with a thermal spectrum at a
temperature. Killing energy and angular momentum must be conserved, so the two particles must have opposite values for
these. In the ergoregion there are negative energy states for real particles, so such a pair can be created there,
with the negative energy partner later falling across the event horizon into the black hole. In the nonrotating
case the ergoregion exists only beyond the horizon, however the pair creation process can straddle the horizon
(Fig. 3.1). This turns out to have a thermal amplitude, and gives rise to the Hawking eﬀect.
Let us now briefly consider the implications of the Hawking eﬀect for black hole thermodynamics. First
of all the surface gravity κ, which was already implicated as a temperature in the classical theory, turns
out to give rise to the true Hawking temperature ¯ hκ/2π. From the First Law (2.2) it then follows that the
entropy of a black hole is given by
SBH = A/4¯ hG,
one fourth the area in squared Planck lengths (the subscript ‘BH’ conveniently stands for both ‘Bekenstein-
Hawking’ and ‘black hole’). The zero-temperature and dimensional flaws (F1) and (F2) (cf. Chapter 2)
are thus removed. Furthermore, the Hawking radiation leads to a decrease in the horizon area. This is
obvious in the nonrotating case, since the black hole loses mass, but it also happens in the rotating case.
The reason is that the negative energy partner in the Hawking pair creation process is never a real particle
outside the horizon, so it need not carry a locally future-pointing four-momentum flux across the horizon.
The Bekenstein-Hawking entropy can therefore decrease, so flaw (F3) is removed. The remaining flaw in
the thermodynamic analogy was the failure of the generalized second law (F4) (cf. section 2.2). This too
is repaired by the incorporation of quantum field eﬀects, at least in quasistationary. Underlying the Hawking eﬀect is the Unruh eﬀect, which is the fact that the vacuum in Minkowski space
appears to be a thermal state at temperature
TU = ¯ ha/2π (3.2)
when viewed by an observer with acceleration a. Thus there is already something ‘thermal’ about the vacuum
fluctuations even in flat spacetime. Since it lies at the core of the entire subject, let us first delve in some
detail into the theory of the Unruh eﬀect, before coming back to the Hawking eﬀect.
The Unruh eﬀect was discovered after the Hawking eﬀect, as a result of eﬀorts to understand the Hawking
eﬀect. The original observation was that a detector coupled to a quantum field and accelerating through the
Minkowski vacuum will be thermally excited. A related observation by Davies was that a mirror accelerating
through the vacuum will radiate thermally. But the essential point is that the vacuum itself has a thermal
character, quite independently of anything that might be coupled to it.
Owing to the symmetry of the Minkowski vacuum under translations and Lorentz tranformations, the
vacuum will appear stationary in a uniformly accelerated frame, but this appearance will not be independent
of the acceleration. Moreover, since it is the ground state, it is stable to dynamical perturbations. Sciama
pointed out that stationarity and stability of the state alone are suﬃcient to indicate that the state is a
thermal one, as shown by Haag et al in axiomatic quantum field theory. Note that the time scale associated
with the Unruh temperature, ¯ h/TH = 2πc/a, is the time it takes for the velocity to change by something of
order c when the acceleration is a.
Two derivations of the Unruh eﬀect will now be given, both of which are valid for arbitrary interacting
scalar fields in spacetime of any dimension. (The generalization to fields of nonzero spin is straightforward.For a Schwarzschild black hole, κ = 1/2Rs = 1/4GM , so the Hawking temperature is TH = ¯ h/8πGM ,
and the corresponding wavelength is λH = 2π/ω = 8π2Rs. A larger black hole is therefore cooler.
Notice the subtle shift from the Newtonian perspective. The postulate is not that particles move in straight
lines, but rather that there exist spacetime frames with respect to which particles move in straight lines.
Implicit in the assumption of the existence of globally inertial frames is the assumption that the geometry of
spacetime is flat, the geometry of Euclid, where parallel lines remain parallel to infinity. In general relativity,
this postulate is replaced by the weaker postulate that local (not global) inertial frames exist. A locally
inertial frame is one which is inertial in a “small neighbourhood” of a spacetime point. In general relativity,
spacetime can be curved. Statement: “The laws of physics are the same in any inertial frame,
regardless of position or velocity.”
Physically, this means that there is no absolute spacetime, no absolute frame of reference with respect to
which position and velocity are defined. Only relative positions and velocities between objects are meaningful.
Mathematically, the principle of special relativity requires that the equations of special relativity be
Lorentz covariant.
It is to be noted that the principle of special relativity does not imply the constancy of the speed of light,
although the postulates are consistent with each other. Moreover the constancy of the speed of light does
not imply the Principle of Special Relativity, although for Einstein the former appears to have been the
inspiration for the latter. In general, a Lorentz transformation consists of a spatial rotation about some
spatial axis, combined with a Lorentz boost by some velocity in some direction.
Only space along the direction of motion gets skewed with time. Distances perpendicular to the direction
of motion remain unchanged. Why must this be so? Consider two hoops which have the same size when at
rest relative to each other. Now set the hoops moving towards each other. Which hoop passes inside the
other? Neither! For suppose Vermilion thinks Cerulean’s hoop passed inside hers; by symmetry, Cerulean
must think Vermilion’s hoop passed inside his; but both cannot be true; the only possibility is that the hoops
remain the same size in directions perpendicular to the direction of motion.
If you have understood all this, then you have understood the crux of special relativity, and you can
now go away and figure out all the mathematics of Lorentz transformations. The mathematical problem is:
what is the relation between the spacetime coordinates {𝑡,𝑥,𝑦,𝑧}and {𝑡′,𝑥′,𝑦′,𝑧′}of a spacetime interval,
a 4-vector, in Vermilion’s versus Cerulean’s frames, if Cerulean is moving relative to Vermilion at velocity 𝑣
in, say, the 𝑥 direction? The solution follows from requiring
1. that both observers consider themselves to be at the centre of the lightcone, as illustrated by Figure 1.4,
and
2. that distances perpendicular to the direction of motion remain unchanged, as illustrated by Figure 1.5.
An alternative version of the second condition is that a Lorentz transformation at velocity 𝑣 followed by a
Lorentz transformation at velocity −𝑣 should yield the unit transformation.
Note that the postulate of the existence of globally inertial frames implies that Lorentz transformations
are linear, that straight lines (4-vectors) in one inertial spacetime frame transform into straight lines in other
inertial frames.
You will solve this problem in the next section but two, §1.6. As a prelude, the next two sections, §1.4 and
§1.5 discuss simultaneity and time dilation. How can simultaneity, the notion of events occurring at the same time at different places, be defined opera-
tionally?
One way is illustrated in the sequences of spacetime diagrams in Figure 1.6. Vermilion surrounds herself
with a set of mirrors, equidistant from Vermilion. She sends out a flash of light, which reflects off the mirrors
back to Vermilion. How does Vermilion know that the mirrors are all the same distance from her? Because the
reflected flash from the mirrors arrives back to Vermilion all at the same instant. Vermilion asserts that the
light flash must have hit all the mirrors simultaneously. Vermilion also asserts that the instant when the light
hit the mirrors must have been the instant, as registered by her wristwatch, precisely half way between the
moment she emitted the flash and the moment she received it back again. If it takes, say, 2 seconds between
flash and receipt, then Vermilion concludes that the mirrors are 1 lightsecond away from her. The spatial
hyperplane passing through these events is a hypersurface of simultaneity. More generally, from Vermilion’s
perspective, each horizontal hyperplane in the spacetime diagram is a hypersurface of simultaneity. Vermilion and Cerulean construct identical clocks, Figure 1.8, consisting of a light beam which bounces off a
mirror. Tick, the light beam hits the mirror, tock, the beam returns to its owner. As long as Vermilion and
Cerulean remain at rest relative to each other, both agree that each other’s clock tick-tocks at the same rate
as their own.
But now suppose Cerulean goes off at velocity 𝑣 relative to Vermilion, in a direction perpendicular to the
direction of the mirror. A far as Cerulean is concerned, his clock tick-tocks at the same rate as before, a tick
at the mirror, a tock on return. But from Vermilion’s point of view, although the distance between Cerulean
and his mirror at any instant remains the same as before, the light has farther to go. And since the speed
of light is constant, Vermilion thinks it takes longer for Cerulean’s clock to tick-tock than her own. Thus
Vermilion thinks Cerulean’s clock runs slow relative to her own.
Cerulean defines surfaces of simultaneity using the same operational setup: he encompasses himself with
mirrors, arranging them so that a flash of light returns from them to him all at the same instant. But whereas
Cerulean concludes that his mirrors are all equidistant from him and that the light bounces off them all at the
same instant, Vermilion thinks otherwise. From Vermilion’s point of view, the light bounces off Cerulean’s
mirrors at different times and moreover at different distances from Cerulean, as illustrated in Figure 1.7.
Only so can the speed of light be constant, as Vermilion sees it, and yet the light return to Cerulean all at
the same instant.
Of course from Cerulean’s point of view all is fine: he thinks his mirrors are equidistant from him
An example of the application of the principle of special relativity is the construction of the energy-
momentum 4-vector of a particle, which should have the same form in any inertial frame 
3. The speed of light is constant. Statement: “The speed of light 𝑐 is a universal constant, the same in
any inertial frame.”
This postulate is the nub of special relativity. The immediate challenge of this Chapter, §1.3, is to confront
its paradoxical implications, and to resolve them.
Measuring speed requires being able to measure intervals of both space and time: speed is distance travelled
divided by time elapsed. Inertial frames constitute a special class of spacetime coordinate systems; it is with
respect to distance and time intervals in these special frames that the speed of light is asserted to be constant.
In general relativity, arbitrarily weird coordinate systems are allowed, and light need move neither in
straight lines nor at constant velocity with respect to bizarre coordinates (why should it, if the labelling
of space and time is totally arbitrary?). However, general relativity asserts the existence of locally inertial
frames, and the speed of light is a universal constant in those frames.
In 1983, the General Conference on Weights and Measures officially
Recall
that in the case of the flat space Unruh eﬀect, the redshifting to infinity completely depletes the acceleration
radiation, since the norm of the boost Killing field diverges at infinity.
Two remarks should be made here regarding the state dependence of the above argument. First, the
argument is clearly invalid if the the state of the quantum field is not regular near the horizon. For example,
there is a state called the “Boulware vacuum”, or “static vacuum”, which corresponds the absence of exci-
tations in a Fock space constructed with positive Killing frequency modes as the one-particle states. In the
Boulware vacuum, our accelerated observer sees no particles at all. However, the short distance divergence
of the two-point function does not have the flat space form as the horizon is approached, and the expectation
value of the stress energy tensor becomes singular.
The second remark is that it was important that we started with an observer very close to the horizon.
Only for such an observer is the acceleration high enough, and therefore the timescale a−1 short enough,
that the vacuum fluctuations can be taken to have the universal flat space form independent of the details of
the state of the field and the curvature of the spacetime. Thus, for example, it would be incorrect to argue
that an unaccelerated observer at infinity must (because he is unaccelerated) see no particles, since there
is no a priori justification for assuming the state there looks like the Minkowski vacuum. The lesson of the
Hawking eﬀect is that the state at infinity in fact does not look like the Minkowski vacuum. For every ω a wavepacket can be constructed which is concentrated arbitrarily close to the horizon and
has arbitrarily high frequency with respect to the time of some fixed free-fall observer crossing the horizon
or, equivalently, with respect to the aﬃne parameter u along an ingoing null geodesic that plays the role
of u = t− z in the Rindler horizon case. Thus, provided the state near the horizon looks, to a free-fall
observer at very short distances, like the Minkowski vacuum, we can conclude that it can also be described
as a correlated state of Boulware quanta with the same structure as (3.29). In particular, the state restricted
to the exterior of the horizon is a thermal one, with Boltzmann factor exp(−λ/2π) = exp(−
¯ hω/TH ), where
TH = ¯ hκ/2π is the Hawking temperature.
What is diﬀerent in the black hole case is how these pairs of thermal quanta propagate. In flat space
they continue to swim in parallel on either side of the horizon. In a black hole spacetime the gravitational
tidal force peels them apart. Mathematically, since the wavefronts propagate at fixed u, and u =−ξe−η
,
ξ scales exponentially with η along a wavefront, increasing toward the future and decreasing toward the
past. Once ξ starts to be of order the curvature radius, the Rindler approximation for the metric breaks
down. Thus, toward the future, the ingoing quanta eventually plunge into the singularity, while the outgoing
quanta eventually climb away from the horizon, partially backscatter oﬀ the angular momentum barrier and
the curvature, and partially emerge to infinity as exponentially redshifted thermal quanta at the Hawking
temperature. To every Hawking particle there is a negative Killing energy “partner” that falls into the black
hole. It is the negative energy carried by this partner that is presumably responsible for the mass loss of the
hole.
The number of p-particles reaching infinity thus takes the Planck form,
Np = Γp(e¯ hω/TH
− 1)−1
, (3.30)
where the coeﬃcient Γp is the fraction of p-particles that make it out to infinity rather than being backscat-
tered into the black hole. This is sometimes called the greybody factor since it indicates the emissivity of the
black hole which is not that of a perfect blackbody. Another name for Γp is the absorption coeﬃcient for
the mode p, since it is equal to the fraction of p-particles that would be absorbed by the black hole if sent
in from infinity.
The above reasoning shows that the Hawking radiation is a consequence of the assumption that the state
near the horizon is the vacuum as viewed by free-fall observers at very short distances. Let us call a state
with this property a free-fall vacuum. The derivation of the Hawking eﬀect is not complete until on has
shown that the free-fall vacuum at the horizon indeed results from a generic state prior to collapse of the
matter that formed the black hole. This is a reasonable sounding proposition, since the initial state is the
vacuum for the ultra high frequency modes, and the time and length scales associated with the collapse
are much longer than those associated with such modes. Hawking carried out this step of the argument by
following the mode υ all the way backwards in time along the horizon, through the collapsing matter, and
out to past null infinity I−, using the geometrical optics approximation. At I− the mode still has purely
positive free-fall frequency, so since it is in the vacuum at I− it is in the free-fall vacuum at the horizon. There is something disturbing about Hawking’s reasoning however. As the wavepacket is propagated back-
wards in time along the horizon, it is blueshifting exponentially with respect to Killing time. For the very
first Hawking quanta that emerge after a black hole forms this is perhaps not so serious, since they have not
experienced much blueshifting. But for quanta that emerge a time t after the black hole formed, there is a
blueshift of order exp(κt). For a Schwarzschild black hole, κ = 1/2Rs, so after, say, t = 1000Rs, the blueshift
factor is exp(500). That is, the ingoing mode has frequency exp(500) times the frequency of the outgoing
Hawking quantum at infinity. For a solar mass black hole, the factor is exp(105) after only 2 seconds have
passed.
Needless to say, we cannot be confident that we know what physics looks like at such arbitrarily high,
“transplanckian” frequencies. Of course if exact local lorentz invariance is assumed, then any frequency
can be Doppler shifted down to a low frequency, just by a change of reference frame. But the unlimited
extrapolation of local lorentz invariance to arbitrary boost factors (and the associated infinite density of
states) must be regarded with skepticism. This puzzle can in one sense be sidestepped since, as indicated above, the only role of the transplanckian
ancestor was, for Hawking, to guarantee that one has a free-fall vacuum at short distances near the horizon.
This condition on the state could also plausibly arise in a theory which looks very diﬀerent from ordinary
relativistic field theory at short distances, and in which there are no transplanckian ancestors.
However, this raises the question of how to account for the outgoing black hole modes if they do not have
transplanckian ancestry. Where else could they come from? It seems that they could come from ingoing
modes that are converted into outgoing modes in the neighborhood of the horizon (see figure 3.5). This
ridiculous sounding possibility actually occurs in simple linear field theories in which the wave equation is
modified by the addition of higher derivative terms in the spatial directions perpendicular to some preferred
local time axis [Unruh 1995, Brout et. al, 1995, Corley and Jacobson 1996, Jacobson 1996]. Similar mode
conversion processes occur in many situations where linear waves with a nonlinear dispersion relation prop-
agate in an inhomogeneous medium. There are examples from plasma waves, galactic spiral density waves,
Andreev reflection in superfluid textures, sound waves, and surface waves. If mode conversion accounts for the origin of the outgoing black hole modes, then the ancestors are
probably planckian, but not trans-planckian, modes. Their detailed form would depend on the physics at
the planck scale (or at a lower energy scale for new physics). However, from the analysis of the linear
models referred to above, it is clear that, for black holes that are large compared to the new length scale, the
Hawking spectrum of black hole radiation is remarkably insensitive to these details. For such large black holes
the most significant consequence for the Hawking eﬀect is for stimulated emission. In principle, one could
produce stimulated emission of Hawking radiation by sending in particles in the (presumably planckian)
ancestor modes of the Hawking quanta at any time after the black hole formed, rather than having to send
in transplanckian particles before the collapse. When Bekenstein first proposed the GSL (2.9) he was not thinking that A would ever decrease. The only
question was whether it would necessarily increase enough to compensate for entropy that falls across the
horizon. However, since a black hole emits Hawking radiation, and therefore loses mass, the area of its
horizon must shrink. This is not in contradiction with Hawking’s area theorem, since the quantum field
carries negative energy into the black hole, whereas Hawking assumed a positive energy condition on matter.
It does, however, pose a potential threat to the GSL.
Hawking’s calculation of the black hole temperature determined the coeﬃcient of proportionality between
the black hole entropy and A/¯ hG to be 1/4. The GSL thus takes the form. Classically, the problem was that one could lower a box with entropy to the horizon of a black hole, dropping
it in after almost all of its energy had been extracted at infinity. In such a process the generalized entropy
would decrease (cf. section 2.2).
Bekenstein’s proposal to evade this violation of the GSL was to suggest that there is a universal upper
bound on the entropy that can be contained in a box of a given “size” R and energy E: S ≤ 2πER. Thus,
since a box of size R could not get any closer than R to the horizon, it might necessarily still deliver enough
energy to the black hole to maintain the GSL. He argued that this is so in various thought experiments, but
there were objections. One obvious objection is that the bound seems to restrict the number of independent
species of particles that might exist in nature, since more species lead to a greater possible entropy. It
would be strange if the validity of the GSL imposed a restriction on the number of species. Originally,
Bekenstein argued that this was the way it was. Later he argued that when the Casimir energies are taken into account the bound holds independent of the number of species. In the meantime, Unruh and Wald
argued convincingly that no such bound is needed to uphold the GSL.
The essential point made by Unruh and Wald is that the interaction of the box with the quantum fields
outside the horizon cannot be neglected. Far from the hole a static box sees the Hawking radiation, while
close to the hole it sees the Unruh radiation as a result of its acceleration. Analyzing the process in the
accelerating frame, the box experiences a buoyancy force owing to the fact that the temperature of the Unruh
radiation is higher on the lower side of the box than on the upper side. At the point where the energy of
the displaced Unruh radiation is equal to the energy E of the box, the buoyancy force is just great enough
to float the box. If the box is then pushed further in it acquires more energy, so the energy delivered to
the hole is minimized by dropping the box at the floating point. When the box is dropped into the hole the
entropy change of the hole is (from the first law) ∆SBH = E/TH . But the entropy Sbox of the box must
be less than or equal to the entropy of thermal radiation with the same volume and energy, since thermal
radiation maximizes entropy. That is, Sbox must be less than or equal to the entropy of the displaced Unruh
radiation, which has energy E and entropy E/TH . Thus the SBH + Soutside necessarily increases, so the
GSL holds.
It is somewhat peculiar to base the argument on the Unruh radiation which is not even seen by an
inertial observer. Unruh and Wald point out that the stress tensors “seen” by the two observers diﬀer by
the conserved stress-tensor of the Boulware vacuum. Because it is separately conserved, this diﬀerence will
not aﬀect the result for any observable like the tension in the rope or the total energy transferred.
In the inertial viewpoint, the reason the box floats is that as it is lowered it maintains the vacuum in the
accelerated frame, i.e. the Boulware vacuum, which has negative energy density relative to the surrounding
Unruh or Hartle-Hawking vacua. Evidently, as it is lowered, the box must radiate positive energy and fill
with negative energy until at the floating point its total energy equals zero. The Unruh-Wald analysis also shows that energy can be extracted from a black hole faster than it would
naturally evaporate by Hawking radiation, even if it is nonrotating and neutral. One can lower an open
box to near the horizon, and then close it. It will be full of Unruh radiation. Now slowly lifting it back
out to infinity it will arrive at infinity full of radiation with some Killing energy Erad. The work done in
the cycle is the energy required to lift this radiation, i.e. the diﬀerence (1− χbot)Erad, between its Killing
energy at infinity and at the bottom. This work is less than the energy extracted, so energy conservation
implies that one has somehow extracted the energy χbotErad from the black hole! Since Erad is proportional
to T 4
bot ∝ χ−4
botT 4
H , the extracted energy is arbitrarily large.1
How can one understand the mass loss by the black hole? When the box is closed, the interior is in
the local vacuum state, whose essentially zero energy density is comprised of a negative Boulware vacuum
energy density plus a positive thermal Unruh energy density. As the box is lifted out, the contribution of
the negative Boulware energy density drops (eventually to zero) as the acceleration drops, but the thermal
Unruh contribution survives. The negative Boulware energy flows out of the box and into the black hole,
decreasing its mass.
How is this all explained from the inertial viewpoint? As the box is lifted back up, it radiates negative
energy into the black hole and fills up with positive energy. One way to see this is as an eﬀect of radiation
by (nonuniformly) accelerating mirrors, together with the fact that the lower face of the box experiences a
greater acceleration than the upper face. At this stage it is clear that black holes are really thermodynamic systems with an actual temperature and
entropy. What remains to be understood however is the meaning of this entropy in terms of statistical
mechanics. Somehow the entropy should be the logarithm of the number of independent states of the black
hole. Understanding how to count these states would constitute a significant step forward in the quest to
understand quantum gravity.
It should be said at the outset that the subject of this section lies at the wild frontier of black hole ther-
modynamics. While many interesting and presumably important facts are known, and significant progress
continues to be made, there is not yet agreement on a single correct viewpoint. I shall therefore discuss a
wide range of ideas, pointing out their interconnections, but not insisting on one unified approach.
The fact that the black hole entropy is even finite is already puzzling. A box of radiation at fixed energy
and volume has a finite entropy because the box imposes a long wavelength cutoﬀ and the total energy
imposes a short wavelength cutoﬀ. The Hilbert space describing the radiation field inside the box at fixed
energy is thus finite dimensional, and the microcanonical entropy is just the logarithm of its dimension. A
black hole in a box at fixed energy would also have a short wavelength cutoﬀ (at the box) but, as emphasized
by ’t Hooft, according to standard quantum field theory it has no long wavelength cutoﬀ (at the box). The
reason is that the horizon is an infinite redshift surface. The wavevector of any outgoing mode diverges at
the horizon, and is redshifted down to a finite value at the box. The entropy of each radiation field around a
black hole is therefore infinite due to a divergence in the mode density at the horizon, so it seems the black
hole entropy must also diverge.
We shall see below that this divergence is equivalent to a divergence in the renormalization of Newton’s
constant, or rather in 1/G. Thus one point of view is that it should be absorbed by “counter terms”, and
only the total, renormalized entropy is relevant. To many physicists this does not seem satisfactory however,
since one expects that entropy should count dimensions in Hilbert space, which should not be subject to
infinite subtractions. A possible resolution is that some mechanism cuts oﬀ the short wavlength modes at
the horizon, so that the entropy (and the renormalization of G−1) is finite.
This subject will be pursued further below, where we discuss the various diﬀerent interpretations and
calculations of black hole entropy that have been proposed. Before beginning this journey however, let us
stop to consider what kind of a cutoﬀ mechanism is called for. Given that the GSL seems to be true, one is led to the conclusion that A/4 (setting ¯ hG = 1) must be the
most entropy that can be contained in a region surrounded by a surface of area A. To maximize the volume
one would take a sphere, and if there were more entropy than A/4, but no black hole, one could simply add
more mass until a black hole formed, at which point the entropy would go down to A/4, violating the GSL.
Thus the entropy must have been less than A/4 to begin with.
’t Hooft argued that the inescapable implication of this is that the true space of quantum states in a finite
region must be finite dimensional and associated with the two-dimensional boundary of the region rather
than the volume. Thus it is not enough even if the system is like a fermion field on a lattice of finite spacing.
Rather, the states in the region must be somehow determined by a finite-state system on a boundary lattice!
’t Hooft made the analogy to a hologram, and the idea was dubbed by Susskind the holographic hypothesis.
From a classical viewpoint, the holographic hypothesis may correspond to a statement about the phase
space of a gravitating system surrounded by a surface of area A that is not inside a black hole. It is not
inconceivable that this phase space is compact with a volume that scales as the area. If something like this is
true, then the holographic hypothesis could just be a straightforward consequence of quantizing a gravitating
system.
On the other hand, it has been suggested by ‘t Hooft and Susskind that the holographic hypothesis can
only be incorporated into physics with a radical change in the foundations of the subject. If so, it provides
a tantalizing hint as to the nature of that change. There are some suggestions that string theory might
be headed in the required direction, or perhaps something very diﬀerent like a cellular automaton model is
correct. For the remainder of this section I will ignore the holographic hint however, and continue to discuss
the problem from the point of view of local field theory. Bekenstein’s original idea was that the entropy of a black hole is the logarithm of the number of ways it
could have formed. This is closely related to the Boltzman definition of entropy as the number of microstates
compatible with the macrostate.
Hawking noted that a potential problem arises if one contemplates increasing the number of species of
fundamental fields. There would seem to be more ways of forming the black hole, however the entropy is
fixed at A/4. Hawking’s resolution of this was that the black hole will also radiate faster because of the
extra species, so that there would be less phase space per species available for forming the hole. Presuming
these two eﬀects balance each other, the puzzle would be resolved. This argument was further developed by
Zurek and Thorne, whose analysis makes it uneccessary to presume that the two eﬀects cancel. Building up
the black hole bit by bit, adding energy to the thermal “atmosphere” just outside the horizon, they argue
that the entropy is equal to the logarithm of the number of ways of making the black hole, independent of
the number of species.
Note that to conclude that the actual value of the black hole entropy A/4¯ hG is independent of the
number of species, one must assume that the value of Newton’s constant is also independent of the number
of species. This is by no means clear however, since the low energy eﬀective G is renormalized by the vacuum
fluctuations of all quantum fields. If a fundamental theory could determine G, there is no reason to think it
would come out to be independent of the number of species.
The Zurek-Thorne interpretation sounds a lot like it is identifying the black hole entropy with the entropy
of the thermal bath seen by accelerated observers outside the horizon. Actually, this is not the case. Besides the divergence, which might be cut oﬀ in some way, there is another problem with the idea that
the thermal or entanglement entropies of quantum fields be identified with black hole entropy. Namely, this
entropy depends on the number of diﬀerent fields in nature, whereas the black hole entropy is universal,
always equal to A/4¯ hG.
Various resolutions to the species problem have been suggested. The most natural one to my mind is
that the renormalized Newton constant, which appears in the Bekenstein-Hawking entropy A/4¯ hG, depends
on the number of species in just the right way to absorb all species dependence of the black hole entropy.
To understand this point, we must include the gravitational degrees of freedom in our description, which we
do in the next subsection.
It should be remarked that the formal nature of the argument used to establish the equality ρext =
exp(−βH) left us on somewhat shaky ground. It may be that entanglement and thermal entropies are not
exactly the same. This issue is somewhat superseded by the considerations of the next subsection, in which
the coupling of the matter and gravitational degrees of freedom is allowed for. Shortly after the Hawking eﬀect was discovered, Gibbons and Hawking proposed a formulation of quantum
gravitational statistical mechanics that enabled them to compute the black hole entropy, and they got the
right answer. Their approach was nevertheless not generally regarded as the final word, for several reasons
to be discussed below, which is why people pursued the question in the ways already described above. In
fact, Gibbons and Hawking even noted that their approach contains the thermal entropy of quantum fields
as a one-loop quantum correction. The Gibbons-Hawking approach will now be described. The basic idea is to imitate standard methods of
handling thermodynamic ensembles in other branches of physics. Thus, the goal is to compute the partition
function Z= T r exp(−βH) for the system of gravitational and matter fields in thermal equilibrium at
temperature T , from which the entropy and other thermodynamic functions can be evaluated. In fact it is
better in principle to consider the microcanonical ensemble rather than the canonical one. This is because
the canonical ensemble is unstable for a gravitating system. If a black hole is in a large heat bath at the
Hawking temperature, a small fluctuation to larger mass will cause its temperature to drop, which leads
to a runaway growth of the hole. Conversely, a small fluctuation to smaller mass will lead to a runaway
evaporation of the hole.
This instability can be controlled by putting the black hole in a very small container, with radius less than
3/2 times the Schwarzschild radius (for a Schwarzschild black hole), and somehow holding the temperature
at the box fixed. The reason this eliminates the instability is interesting: although a fluctuation to (say)
larger mass causes the Hawking temperature to drop, this is more than compensated by the fact that the
horizon has moved out, so the local temperature at the box is less redshifted than before, so the hole is in fact
locally hotter than the box. Alternatively one can work with the more physical microcanonical ensemble, in
which the total energy is fixed. In the following we shall for simplicity gloss over these refinements in the
nature of the ensemble, unless explicit mention is called for.
To actually compute Z would seem to require an understanding the Hilbert space of quantum gravity,
something which we still lack. Gibbons and Hawking sidestepped this diﬃculty by passing to a path integral
representation for Z whose semiclassical approximation could be plausibly evaluated. 
After a slow start, you cover ground at an ever increasing rate, crossing 50 billion lightyears, the distance
to the edge of the currently observable Universe, in just over 25 years of your own time.
Does this mean you go faster than the speed of light? No. From the point of view of a person at rest
on Earth, you never go faster than the speed of light. From your own point of view, distances along your
direction of motion are Lorentz-contracted, so distances that are vast from Earth’s point of view appear
much shorter to you. Fast as the Universe rushes by, it never goes faster than the speed of light.
This rosy picture of being able to flit around the Universe has drawbacks. Firstly, it would take a huge
amount of energy to keep you accelerating at 𝑔. Secondly, you would use up a huge amount of Earth time
travelling around at relativistic speeds. If you took a trip to the edge of the Universe, then by the time
you got back not only would all your friends and relations be dead, but the Earth would probably be gone,
swallowed by the Sun in its red giant phase, the Sun would have exhausted its fuel and shrivelled into a
cold white dwarf star, and the Solar System, having orbited the Galaxy a thousand times, would be lost
somewhere in its milky ways.
Technical point. The Universe is expanding, so the distance to the edge of the currently observable Universe
is increasing. Thus it would actually take longer than indicated in the table to reach the edge of the currently
observable Universe. Moreover if the Universe is accelerating, as evidence from the Hubble diagram of Type Ia
Supernovae indicates, then you will never be able to reach the edge of the currently observable Universe,
however fast you go. The distortion of a scene when you move through it at near the speed of light can be calculated most directly
from the Lorentz transformation of the energy-momentum 4-vectors of the photons that you see. The result
is what I call the “Rules of 4-dimensional perspective.”
Figure 1.19 illustrates the rules of 4-dimensional perspective, also called “special relativistic beaming,”
which describe how a scene appears when you move through it at near light speed.
On the left, you are at rest relative to the scene. Imagine painting the scene on a celestial sphere around
you. The arrows represent the directions of light rays (photons) from the scene on the celestial sphere to you
at the center.
On the right, you are moving to the right through the scene, at 0.8 times the speed of light. The celestial
sphere is stretched along the direction of your motion by the Lorentz gamma-factor 𝛾 = 1/√1−0.82 = 5/3
into a celestial ellipsoid. You, the observer, are not at the centre of the ellipsoid, but rather at one of its foci
(the left one, if you are moving to the right). The focus of the celestial ellipsoid, where you the observer are, is
displaced from centre by 𝛾𝑣= 4/3. The scene appears relativistically aberrated, which is to say concentrated
ahead of you, and expanded behind you.
The lengths of the arrows are proportional to the energies, or frequencies, of the photons that you see. Special relativity was unsatisfactory almost from the outset. Einstein had conceived special relativity by
abolishing the aether. Yet for something that had no absolute substance, the spacetime of special relativity
had strikingly absolute properties: in special relativity, two particles on parallel trajectories would remain
parallel for ever, just as in Euclidean geometry.
Moreover whereas special relativity neatly accommodated the electromagnetic force, which propagated
at the speed of light, it did not accommodate the other force known at the beginning of the 20th century,
gravity. Plainly Newton’s theory of gravity could not be correct, since it posited instantaneous transmission
of the gravitational force, whereas special relativity seemed to preclude anything from moving faster than
light, Exercise 1.23. You might think that gravity, an inverse square law like electromagnetism, might satisfy
a similar set of equations, but this is not so. Whereas an electromagnetic wave carries no electric charge, and
therefore does not interact with itself, any wave of gravity must carry energy, and therefore must interact
with itself. This proves to be a considerable complication.
A partial solution, the principle of equivalence of gravity and acceleration, occurred to Einstein while
working on an invited review on special relativity (Einstein, 1907). Einstein realised that “if a person falls
freely, he will not feel his own weight,” an idea that Einstein would later refer to as “the happiest thought of
my life.” The principle of equivalence meant that gravity could be reinterpreted as a curvature of spacetime. It is not always possible to cover a manifold with a single chart, that is, with a coordinate system such
that every point of spacetime has a unique coordinate. A simple example of a 2-dimensional manifold that
cannot be covered with a single chart is the 2-sphere 𝑆2, the 2-dimensional surface of a 3-dimensional sphere,
as illustrated in Figure 2.2. Inevitably, lines of constant coordinate must cross somewhere on the 2-sphere.
At least two charts are required to cover a 2-sphere.
When more than one chart is necessary, neighbouring charts are required to overlap, in order that the
structure of the manifold be consistent across the overlap. General relativity postulates that the mapping
between the coordinates of overlapping charts be at least doubly differentiable. A manifold subject to this
property is called differentiable.
In practice one often uses coordinate systems that misbehave at some points, but in an innocuous fashion. The weak principle of equivalence states that: “Gravitating mass equals inertial mass.” General relativity
satisfies the weak principle of equivalence, but then so also does Newtonian gravity.
Einstein’s principle of equivalence is actually two separate statements: “The laws of physics in a
gravitating frame are equivalent to those in an accelerating frame,” and “The laws of physics in a non-
accelerating, or free-fall, frame are locally those of special relativity.”
Einstein’s principle of equivalence implies that it is possible to remove the effects of gravity locally by going
into a non-accelerating, or free-fall, frame. The structure of spacetime in a non-accelerating, or free-fall, frame
is locally inertial, with the local structure of Minkowski space. By locally inertial is meant that at each point
of spacetime it is possible to choose coordinates such that (a) the metric at that point is Minkowski, and (b)
the first derivatives of the metric are all zero1. In other words, Einstein’s principle of equivalence asserts the
existence of locally inertial frames. The general expression (2.114) for the commutator of the covariant derivative reveals the meaning of the
torsion and Riemann tensors. The torsion and Riemann tensors describe respectively the displacement and the
Lorentz transformation experienced by an object when parallel-transported around a curve. Displacements
and Lorentz transformations together constitute the Poincaré group, the complete group of symmetries of
flat spacetime.
How can an object detect a displacement when parallel-transported around a curve? If you go around
a curve back to the same coordinate in spacetime where you began, won’t you necessarily be at the same
position? This is a question that goes to heart of the meaning of spacetime. To answer the question, you
have to consider how fundamental particles are able to detect position, orientation, and velocity. Classically,
particles may be structureless points, but quantum mechanically, particles possess frequency, wavelength,
spin, and (in the relativistic theory) boost, and presumably it is these properties that allow particles to
“measure” the properties of the spacetime in which they live. For example, a Dirac spinor (relativistic spin- 1
2
particle) Lorentz transforms under the fundamental (spin- 1
2 ) representation of the Lorentz group, and is
thus endowed with precisely the properties that allow it to “measure” boost and rotation, §14.10. The Dirac
wave equation shows that a Dirac spinor propagating through spacetime varies as ∼𝑒𝑖𝑝𝜇𝑥𝜇, whose phase
encodes the displacement of the Dirac spinor. Thus a Dirac spinor could potentially detect a displacement
through a change in its phase when parallel-transported around a curve back to the same point in spacetime.
Since a change in phase is indistinguishable from a spatial rotation about the spin axis of the Dirac spinor,
operationally torsion rotates particles, whence the name torsion.
In fairy tales, a wizard will say the magic words of a spell, or inscribe a set of magic symbols, and the world will change. Fireballs shoot from their hands; the hero is revived from an endless slumber; a couple falls in love. We smile when we hear such stories. They seem quaint but implausible. Common sense tells us that merely speaking certain words, or inscribing certain symbols, cannot cause such changes in the world. And yet our scientific theories are not so different. By speaking the words or inscribing the symbols of such a theory, we can greatly deepen our understanding of the world. That new understanding then enables us to change the world in ways that formerly seemed impossible, even inconceivable. Consider quantum mechanics, one of our deepest theories. It has helped us create lasers, semiconductor chips, and magnetic resonance imaging. One day it will likely help us create quantum computers, perhaps even quantum intelligences. Quantum mechanics is magic that actually works.

In this essay, we explain quantum mechanics in detail. We will describe all the principles of quantum mechanics in depth, nothing held back. It's not a handy-wavy treatment, of the kind often found in articles written for a general audience. While such articles can be entertaining, trying to learn quantum mechanics by reading them is like learning to play basketball by merely watching basketball being played. This essay will get you out on the mathematical court of quantum mechanics. Of course, you won't learn to slam dunk, at least not yet. But the essay will ground you in an understanding of the fundamentals of quantum mechanics, which you can later build upon and extend.

To read the essay, you must first understand the quantum circuit model of quantum computation. If you're not familiar with quantum circuits, you can learn about them from our earlier essay, Quantum Computing for the Very Curious. You may wish to pause to read that essay now, if you haven't already. Once you've understood that material you shouldn't need any other prerequisites. Indeed, when you learned quantum computing, you learned in passing almost all of quantum mechanics. In Part I of this essay we distill those past ideas, collecting them up into the package known as quantum mechanics.

Although quantum mechanics is not so difficult to learn, it's a theory which has disturbed many people. Here's a few classic quotes on this puzzlement:

I think I can safely say that nobody understands quantum mechanics … Do not keep saying to yourself, if you can possibly avoid it, “But how can it be like that?” because you will get “down the drain”, into a blind alley from which nobody has yet escaped. Nobody knows how it can be like that. – Richard Feynman

I have thought a hundred times as much about the quantum problems as I have about general relativity theory. – Albert Einstein

If quantum mechanics hasn't profoundly shocked you, you haven't understood it yet. – Niels Bohr

If quantum mechanics is not so difficult to learn, why has it so disturbed many great physicists? What do they mean when they say they don't understand it, or are shocked by it? How can a scientific theory be both beautiful and disturbing? In Part II of this essay we'll explore these questions. As we'll see, despite its simplicity quantum mechanics raises striking conceptual problems, problems which are among the most exciting open problems in science today.

The essay is presented in an unusual form. It's an example of what we call a mnemonic essay, written in the mnemonic medium. That means it incorporates new user interface elements intended to make it almost effortless for you to remember and apply the ideas in the essay. The motivator is that most people (ourselves included) quickly forget much of what we read in books and articles. The mnemonic medium takes advantage of what cognitive scientists know about how humans learn, creating an interface which ensures you'll remember the ideas and how to apply them near permanently. More on how that works below.

Part I: The postulates of quantum mechanics

So, what is quantum mechanics? It's fun to wax poetic about it being magic that actually works, or to quote eminent scientists saying no-one really understands it. But while fun, those statements give no direct enlightenment. What is quantum mechanics, really?

Quantum mechanics is simply this: it's a set of four postulates that provide a mathematical framework for describing the universe and everything in it. These postulates reflect ideas you've already seen in the quantum circuit model: how to describe a quantum state; how to describe the dynamics of a quantum system; and so on. But rather than talk abstractly, it's better to just see the first postulate:

The first postulate: state space

Postulate 1: Associated to any physical system is a complex vector space known as the state space of the system. If the system is isolated, then the system is completely described by its state vector, which is a unit vector in the system's state space.
This may seem densely written, but you already know most of it. In particular, we've been working extensively with qubits, and we've seen this postulate in action. For a qubit the state space is, as you know, a two-dimensional complex vector space. The state is a unit vector in that state space (i.e., has length 
1
1). And the condition that the state vector is a unit vector expresses the idea that the probabilities for the outcomes of a computational basis measurement add up to
1
1.

It's not just qubits that have a state space. The first postulate of quantum mechanics tells us that every physical system has a state space. An atom has a state space; a human being has a state space; even the universe as a whole has a state space. Admittedly, the first postulate doesn't tell us what the state space is, for any given physical system. That needs to be figured out on a case-by-case basis. Different types of atoms have different state spaces; a human being has a different (and much more complicated) state space; the universe has a more complicated state space still.

As an example, suppose you're interested in what happens when you shine light on an atom. To give a quantum mechanical description, you'd need a state space to describe atoms and the electromagnetic field (i.e., light). A priori it's not obvious what that state space should be. How many dimensions should the state space have? How do particular physical configurations of atoms and light correspond to states in that state space? By itself, quantum mechanics doesn't answer these questions. It merely does the subtle-but-important job of instructing you to look for answers to these questions. Fortunately, there is a theory, known as quantum electrodynamics (often shortened to QED), which describes how atoms and light interact. Among other things, QED tells us which states and state spaces to use to give quantum descriptions of atoms and light.

In a similar way, suppose we're trying to describe particles like quarks or the Higgs boson. Just as with the atoms-and-light example, quantum mechanics tells us we need figure out the right state spaces and state vectors. But it doesn't tell us what those are. In this case, the additional theory needed is the standard model of particle physics. Like QED, the standard model sits on top of basic quantum mechanics, fleshing it out, telling us things which aren't in the four postulates of quantum mechanics.

More generally, quantum mechanics alone isn't a fully specified physical theory. Rather, it's a framework to use to construct physical theories (like QED). It's helpful to think of quantum mechanics as analogous to an operating system for a computer. On its own, the operating system doesn't do all the user needs. Rather, it's a framework that accomplishes important housekeeping functions, creating a file system, a graphical display and interface, and so on. But users need another layer of application software on top of those basic functions. That application layer is analogous to physical theories like QED and the standard model. The application layer runs within the framework provided by the operating system, but isn't itself part of the operating systemThis analogy was popularized in Scott Aaronson's book “Quantum Computing Since Democritus”; MN believes Scott first heard it in a talk MN gave at the Fields Institute in Toronto in 2001. This wouldn't warrant mention, except when MN uses the analogy, people often respond “oh yes, that's Scott Aaronson's way of thinking about quantum mechanics”. MN doesn't recall where he first heard it, or if the description is original..

In this essay, we won't get deeply into QED or the standard model or the other theories physicists have developed to describe particular physical systems. Rather, our focus is on quantum mechanics itself – the four postulates. For examples, we'll mostly draw on the quantum circuit model, which makes simple and reasonable assumptions about state spaces and state vectors. This model is already an extremely rich arena for studying quantum mechanics. If you wish, you can learn later about QED, the standard model, and so on.

Of course, this means there is still much more to learn, beyond the scope of this essay. Each physical system requires learning its own particular set of recipes for state spaces and state vectors. You already know some such recipes: if we say “let's consider a 3-qubit system”, then we know the state space is just the 8-dimensional complex vector space spanned by the computational basis states 
∣
0
⟩
∣
0
⟩
∣
0
⟩
∣0⟩∣0⟩∣0⟩,
∣
0
⟩
∣
0
⟩
∣
1
⟩
,
…
∣0⟩∣0⟩∣1⟩,…. By contrast, you probably don't know the recipe QED gives telling us how to construct the state space for a couple of hydrogen atoms interacting with the electromagnetic fieldA little more strictly speaking, QED tells us how charged elementary particles like electrons interact with the electromagnetic field. After QED was invented, atomic physicists and quantum opticians figured out how to use it to describe atoms and electromagnetic fields. And so the recipe you'd use would likely come from atomic physics or quantum optics.. While that recipe is different than in the three-qubit example, it's really much the same kind of thing. You learn the rules of the recipe, and then you can figure out the state spaces and quantum states.

Where do the recipes for things like the state space of an atom (or the electromagnetic field) come from? The unglamorous answer is that figuring those things out for the first time was incredibly hard. As in: Nobel prize hard, or even multiple Nobel prize hard. People made lots of guesses, tried lots of different things, trying to figure out states and state spaces (and all the other things), constructed lots of bad theories, and lots of good-but-not-good-enough theories along the way. And, eventually, they came up with some great recipes. Most of the time today we can use one of those recipes off-the-shelf. You go pick up a book about atomic physics, say, and it'll just tell you: use such-and-such a state space, and away you go. But in the background is thousands of instances of trial-and-error by often-frustrated physicists.
The first postulate of quantum mechanics told us how we describe states. What about how states change, that is, the dynamics of a quantum system? That's where the second postulate comes in. In the earlier essay we saw that quantum gates are described by unitary matrices acting on the state space of a quantum system. The second postulate tells us that something very similar is true for any isolated quantum system:
“Quantum mechanics” is the description of the behavior of matter and light in all its details and, in particular, of the happenings on an atomic scale. Things on a very small scale behave like nothing that you have any direct experience about. They do not behave like waves, they do not behave like particles, they do not behave like clouds, or billiard balls, or weights on springs, or like anything that you have ever seen.
Newton thought that light was made up of particles, but then it was discovered that it behaves like a wave. Later, however (in the beginning of the twentieth century), it was found that light did indeed sometimes behave like a particle. Historically, the electron, for example, was thought to behave like a particle, and then it was found that in many respects it behaved like a wave. So it really behaves like neither. Now we have given up. We say: “It is like neither.”
There is one lucky break, however—electrons behave just like light. The quantum behavior of atomic objects (electrons, protons, neutrons, photons, and so on) is the same for all, they are all “particle waves,” or whatever you want to call them. So what we learn about the properties of electrons (which we shall use for our examples) will apply also to all “particles,” including photons of light.
The gradual accumulation of information about atomic and small-scale behavior during the first quarter of the 20th century, which gave some indications about how small things do behave, produced an increasing confusion which was finally resolved in 1926 and 1927 by Schrödinger, Heisenberg, and Born. They finally obtained a consistent description of the behavior of matter on a small scale. We take up the main features of that description in this chapter.
Because atomic behavior is so unlike ordinary experience, it is very difficult to get used to, and it appears peculiar and mysterious to everyone—both to the novice and to the experienced physicist. Even the experts do not understand it the way they would like to, and it is perfectly reasonable that they should not, because all of direct, human experience and of human intuition applies to large objects. We know how large objects will act, but things on a small scale just do not act that way. So we have to learn about them in a sort of abstract or imaginative fashion and not by connection with our direct experience.
In this chapter we shall tackle immediately the basic element of the mysterious behavior in its most strange form. We choose to examine a phenomenon which is impossible, absolutely impossible, to explain in any classical way, and which has in it the heart of quantum mechanics. In reality, it contains the only mystery. We cannot make the mystery go away by “explaining” how it works. We will just tell you how it works. In telling you how it works we will have told you about the basic peculiarities of all quantum mechanics. To try to understand the quantum behavior of electrons, we shall compare and contrast their behavior, in a particular experimental setup, with the more familiar behavior of particles like bullets, and with the behavior of waves like water waves. We consider first the behavior of bullets in the experimental setup shown diagrammatically in Fig. 1–1. We have a machine gun that shoots a stream of bullets. It is not a very good gun, in that it sprays the bullets (randomly) over a fairly large angular spread, as indicated in the figure. In front of the gun we have a wall (made of armor plate) that has in it two holes just about big enough to let a bullet through. Beyond the wall is a backstop (say a thick wall of wood) which will “absorb” the bullets when they hit it. In front of the backstop we have an object which we shall call a “detector” of bullets. It might be a box containing sand. Any bullet that enters the detector will be stopped and accumulated. When we wish, we can empty the box and count the number of bullets that have been caught. The detector can be moved back and forth (in what we will call the x-direction). With this apparatus, we can find out experimentally the answer to the question: “What is the probability that a bullet which passes through the holes in the wall will arrive at the backstop at the distance x from the center?” First, you should realize that we should talk about probability, because we cannot say definitely where any particular bullet will go. A bullet which happens to hit one of the holes may bounce off the edges of the hole, and may end up anywhere at all. By “probability” we mean the chance that the bullet will arrive at the detector, which we can measure by counting the number which arrive at the detector in a certain time and then taking the ratio of this number to the total number that hit the backstop during that time. Or, if we assume that the gun always shoots at the same rate during the measurements, the probability we want is just proportional to the number that reach the detector in some standard time interval.
For our present purposes we would like to imagine a somewhat idealized experiment in which the bullets are not real bullets, but are indestructible bullets—they cannot break in half. In our experiment we find that bullets always arrive in lumps, and when we find something in the detector, it is always one whole bullet. If the rate at which the machine gun fires is made very low, we find that at any given moment either nothing arrives, or one and only one—exactly one—bullet arrives at the backstop. Also, the size of the lump certainly does not depend on the rate of firing of the gun. We shall say: “Bullets always arrive in identical lumps.” What we measure with our detector is the probability of arrival of a lump. And we measure the probability as a function of x. The result of such measurements with this apparatus (we have not yet done the experiment, so we are really imagining the result) are plotted in the graph drawn in part (c) of Fig. 1–1. In the graph we plot the probability to the right and x vertically, so that the x-scale fits the diagram of the apparatus. We call the probability P12 because the bullets may have come either through hole 1 or through hole 2. You will not be surprised that P12 is large near the middle of the graph but gets small if x is very large. You may wonder, however, why P12 has its maximum value at x=0. We can understand this fact if we do our experiment again after covering up hole 2, and once more while covering up hole 1. When hole 2 is covered, bullets can pass only through hole 1, and we get the curve marked P1 in part (b) of the figure. As you would expect, the maximum of P1 occurs at the value of x which is on a straight line with the gun and hole 1. When hole 1 is closed, we get the symmetric curve P2 drawn in the figure. Now we wish to consider an experiment with water waves. The apparatus is shown diagrammatically in Fig. 1–2. We have a shallow trough of water. A small object labeled the “wave source” is jiggled up and down by a motor and makes circular waves. To the right of the source we have again a wall with two holes, and beyond that is a second wall, which, to keep things simple, is an “absorber,” so that there is no reflection of the waves that arrive there. This can be done by building a gradual sand “beach.” In front of the beach we place a detector which can be moved back and forth in the x-direction, as before. The detector is now a device which measures the “intensity” of the wave motion. You can imagine a gadget which measures the height of the wave motion, but whose scale is calibrated in proportion to the square of the actual height, so that the reading is proportional to the intensity of the wave. Our detector reads, then, in proportion to the energy being carried by the wave—or rather, the rate at which energy is carried to the detector.
With our wave apparatus, the first thing to notice is that the intensity can have any size. If the source just moves a very small amount, then there is just a little bit of wave motion at the detector. When there is more motion at the source, there is more intensity at the detector. The intensity of the wave can have any value at all. We would not say that there was any “lumpiness” in the wave intensity. We have already worked out how such patterns can come about when we studied the interference of electric waves in Volume I. In this case we would observe that the original wave is diffracted at the holes, and new circular waves spread out from each hole. If we cover one hole at a time and measure the intensity distribution at the absorber we find the rather simple intensity curves shown in part (b) of the figure. I1 is the intensity of the wave from hole 1 (which we find by measuring when hole 2 is blocked off) and I2 is the intensity of the wave from hole 2 (seen when hole 1 is blocked).
The intensity I12 observed when both holes are open is certainly not the sum of I1 and I2. We say that there is “interference” of the two waves. At some places (where the curve I12 has its maxima) the waves are “in phase” and the wave peaks add together to give a large amplitude and, therefore, a large intensity. We say that the two waves are “interfering constructively” at such places. There will be such constructive interference wherever the distance from the detector to one hole is a whole number of wavelengths larger (or shorter) than the distance from the detector to the other hole. Now we imagine a similar experiment with electrons. It is shown diagrammatically in Fig. 1–3. We make an electron gun which consists of a tungsten wire heated by an electric current and surrounded by a metal box with a hole in it. If the wire is at a negative voltage with respect to the box, electrons emitted by the wire will be accelerated toward the walls and some will pass through the hole. All the electrons which come out of the gun will have (nearly) the same energy. In front of the gun is again a wall (just a thin metal plate) with two holes in it. Beyond the wall is another plate which will serve as a “backstop.” In front of the backstop we place a movable detector. The detector might be a geiger counter or, perhaps better, an electron multiplier, which is connected to a loudspeaker. How can such an interference come about? Perhaps we should say: “Well, that means, presumably, that it is not true that the lumps go either through hole 1 or hole 2, because if they did, the probabilities should add. Perhaps they go in a more complicated way. They split in half and …” But no! They cannot, they always arrive in lumps … “Well, perhaps some of them go through 1, and then they go around through 2, and then around a few more times, or by some other complicated path … then by closing hole 2, we changed the chance that an electron that started out through hole 1 would finally get to the backstop …” But notice! There are some points at which very few electrons arrive when both holes are open, but which receive many electrons if we close one hole, so closing one hole increased the number from the other. Notice, however, that at the center of the pattern, P12 is more than twice as large as P1+P2. It is as though closing one hole decreased the number of electrons which come through the other hole. It seems hard to explain both effects by proposing that the electrons travel in complicated paths.
It is all quite mysterious. And the more you look at it the more mysterious it seems. Many ideas have been concocted to try to explain the curve for P12 in terms of individual electrons going around in complicated ways through the holes. None of them has succeeded. None of them can get the right curve for P12 in terms of P1 and P2.
Yet, surprisingly enough, the mathematics for relating P1 and P2 to P12 is extremely simple. For P12 is just like the curve I12 of Fig. 1–2, and that was simple. What is going on at the backstop can be described by two complex numbers that we can call ϕ1 and ϕ2 (they are functions of x, of course). The absolute square of ϕ1 gives the effect with only hole 1 open. That is, P1=|ϕ1|2. The effect with only hole 2 open is given by ϕ2 in the same way. That is, P2=|ϕ2|2. And the combined effect of the two holes is just P12=|ϕ1+ϕ2|2. The mathematics is the same as that we had for the water waves! (It is hard to see how one could get such a simple result from a complicated game of electrons going back and forth through the plate on some strange trajectory. We shall now try the following experiment. To our electron apparatus we add a very strong light source, placed behind the wall and between the two holes, as shown in Fig. 1–4. We know that electric charges scatter light. So when an electron passes, however it does pass, on its way to the detector, it will scatter some light to our eye, and we can see where the electron goes. If, for instance, an electron were to take the path via hole 2 that is sketched in Fig. 1–4, we should see a flash of light coming from the vicinity of the place marked A in the figure. If an electron passes through hole 1, we would expect to see a flash from the vicinity of the upper hole. If it should happen that we get light from both places at the same time, because the electron divides in half … Let us just do the experiment!
Here is what we see: every time that we hear a “click” from our electron detector (at the backstop), we also see a flash of light either near hole 1 or near hole 2, but never both at once! And we observe the same result no matter where we put the detector. From this observation we conclude that when we look at the electrons we find that the electrons go either through one hole or the other. Experimentally, Proposition A is necessarily true.
What, then, is wrong with our argument against Proposition A? Why isn’t P12 just equal to P1+P2? Back to experiment! Let us keep track of the electrons and find out what they are doing. For each position (x-location) of the detector we will count the electrons that arrive and also keep track of which hole they went through, by watching for the flashes. We can keep track of things this way: whenever we hear a “click” we will put a count in Column 1 if we see the flash near hole 1, and if we see the flash near hole 2, we will record a count in Column 2. Every electron which arrives is recorded in one of two classes: those which come through 1 and those which come through 2. From the number recorded in Column 1 we get the probability P′1 that an electron will arrive at the detector via hole 1; and from the number recorded in Column 2 we get P′2, the probability that an electron will arrive at the detector via hole 2. Is there not some way we can see the electrons without disturbing them? We learned in an earlier chapter that the momentum carried by a “photon” is inversely proportional to its wavelength (p=h/λ). Certainly the jolt given to the electron when the photon is scattered toward our eye depends on the momentum that photon carries. Aha! If we want to disturb the electrons only slightly we should not have lowered the intensity of the light, we should have lowered its frequency (the same as increasing its wavelength). Let us use light of a redder color. We could even use infrared light, or radiowaves (like radar), and “see” where the electron went with the help of some equipment that can “see” light of these longer wavelengths. If we use “gentler” light perhaps we can avoid disturbing the electrons so much.
Let us try the experiment with longer waves. We shall keep repeating our experiment, each time with light of a longer wavelength. At first, nothing seems to change. The results are the same. Then a terrible thing happens. You remember that when we discussed the microscope we pointed out that, due to the wave nature of the light, there is a limitation on how close two spots can be and still be seen as two separate spots. This distance is of the order of the wavelength of light. So now, when we make the wavelength longer than the distance between our holes, we see a big fuzzy flash when the light is scattered by the electrons. We can no longer tell which hole the electron went through! We just know it went somewhere! And it is just with light of this color that we find that the jolts given to the electron are small enough so that P′12 begins to look like P12—that we begin to get some interference effect. And it is only for wavelengths much longer than the separation of the two holes (when we have no chance at all of telling where the electron went) that the disturbance due to the light gets sufficiently small that we again get the curve P12 shown in Fig. 1–3.
In our experiment we find that it is impossible to arrange the light in such a way that one can tell which hole the electron went through, and at the same time not disturb the pattern. It was suggested by Heisenberg that the then new laws of nature could only be consistent if there were some basic limitation on our experimental capabilities not previously recognized. He proposed, as a general principle, his uncertainty principle, which we can state in terms of our experiment as follows: “It is impossible to design an apparatus to determine which hole the electron passes through, that will not at the same time disturb the electrons enough to destroy the interference pattern.” If an apparatus is capable of determining which hole the electron goes through, it cannot be so delicate that it does not disturb the pattern in an essential way. No one has ever found (or even thought of) a way around the uncertainty principle. So we must assume that it describes a basic characteristic of nature.
The complete theory of quantum mechanics which we now use to describe atoms and, in fact, all matter, depends on the correctness of the uncertainty principle. Since quantum mechanics is such a successful theory, our belief in the uncertainty principle is reinforced. But if a way to “beat” the uncertainty principle were ever discovered, quantum mechanics would give inconsistent results and would have to be discarded as a valid theory of nature.
“Well,” you say, “what about Proposition A? Is it true, or is it not true, that the electron either goes through hole 1 or it goes through hole 2?” The only answer that can be given is that we have found from experiment that there is a certain special way that we have to think in order that we do not get into inconsistencies. What we must say (to avoid making wrong predictions) is the following. If one looks at the holes or, more accurately, if one has a piece of apparatus which is capable of determining whether the electrons go through hole 1 or hole 2, then one can say that it goes either through hole 1 or hole 2. But, when one does not try to tell which way the electron goes, when there is nothing in the experiment to disturb the electrons, then one may not say that an electron goes either through hole 1 or hole 2. If one does say that, and starts to make any deductions from the statement, he will make errors in the analysis. This is the logical tightrope on which we must walk if we wish to describe nature successfully. This is the way Heisenberg stated the uncertainty principle originally: If you make the measurement on any object, and you can determine the x-component of its momentum with an uncertainty Δp, you cannot, at the same time, know its x-position more accurately than Δx≥ℏ/2Δp, where ℏ is a definite fixed number given by nature. It is called the “reduced Planck constant,” and is approximately 1.05×10−34 joule-seconds. The uncertainties in the position and momentum of a particle at any instant must have their product greater than or equal to half the reduced Planck constant. This is a special case of the uncertainty principle that was stated above more generally. The more general statement was that one cannot design equipment in any way to determine which of two alternatives is taken, without, at the same time, destroying the pattern of interference. Now in order to do this it is necessary to know what the momentum of the screen is, before the electron goes through. So when we measure the momentum after the electron goes by, we can figure out how much the plate’s momentum has changed. But remember, according to the uncertainty principle we cannot at the same time know the position of the plate with an arbitrary accuracy. But if we do not know exactly where the plate is, we cannot say precisely where the two holes are. They will be in a different place for every electron that goes through. This means that the center of our interference pattern will have a different location for each electron. The wiggles of the interference pattern will be smeared out. We shall show quantitatively in the next chapter that if we determine the momentum of the plate sufficiently accurately to determine from the recoil measurement which hole was used, then the uncertainty in the x-position of the plate will, according to the uncertainty principle, be enough to shift the pattern observed at the detector up and down in the x-direction about the distance from a maximum to its nearest minimum. Such a random shift is just enough to smear out the pattern so that no interference is observed.
The uncertainty principle “protects” quantum mechanics. Heisenberg recognized that if it were possible to measure the momentum and the position simultaneously with a greater accuracy, the quantum mechanics would collapse. So he proposed that it must be impossible. Then people sat down and tried to figure out ways of doing it, and nobody could figure out a way to measure the position and the momentum of anything—a screen, an electron, a billiard ball, anything—with any greater accuracy. Quantum mechanics maintains its perilous but still correct existence.
Let us show for one particular case that the kind of relation given by Heisenberg must be true in order to keep from getting into trouble. We imagine a modification of the experiment of Fig. 1–3, in which the wall with the holes consists of a plate mounted on rollers so that it can move freely up and down (in the x-direction), as shown in Fig. 1–6. By watching the motion of the plate carefully we can try to tell which hole an electron goes through. Imagine what happens when the detector is placed at x=0. We would expect that an electron which passes through hole 1 must be deflected downward by the plate to reach the detector. Since the vertical component of the electron momentum is changed, the plate must recoil with an equal momentum in the opposite direction. The plate will get an upward kick. If the electron goes through the lower hole, the plate should feel a downward kick. It is clear that for every position of the detector, the momentum received by the plate will have a different value for a traversal via hole 1 than for a traversal via hole 2. So! Without disturbing the electrons at all, but just by watching the plate, we can tell which path the electron used.
We should say right away that you should not try to set up this experiment (as you could have done with the two we have already described). This experiment has never been done in just this way. The trouble is that the apparatus would have to be made on an impossibly small scale to show the effects we are interested in. We are doing a “thought experiment,” which we have chosen because it is easy to think about. We know the results that would be obtained because there are many experiments that have been done, in which the scale and the proportions have been chosen to show the effects we shall describe.
The first thing we notice with our electron experiment is that we hear sharp “clicks” from the detector (that is, from the loudspeaker). And all “clicks” are the same. There are no “half-clicks.”
At those places where the two waves arrive at the detector with a phase difference of π (where they are “out of phase”) the resulting wave motion at the detector will be the difference of the two amplitudes. The waves “interfere destructively,” and we get a low value for the wave intensity. We expect such low values wherever the distance between hole 1 and the detector is different from the distance between hole 2 and the detector by an odd number of half-wavelengths. The low values of I12 in Fig. 1–2 correspond to the places where the two waves interfere destructively.
Now let us measure the wave intensity for various values of x (keeping the wave source operating always in the same way). We get the interesting-looking curve marked I12 in part (c) of the figure.P2 is the probability distribution for bullets that pass through hole 2. Comparing parts (b) and (c) of Fig. 1–1, we find the important result that
Postulate 2: The evolution of an isolated quantum system is described by a unitary matrix acting on the state space of the system. That is, the state 
∣
ψ
⟩
∣ψ⟩ of the system at a time 
t
1
t 
1
​	
  is related to the state 
∣
ψ
′
⟩
∣ψ 
′
 ⟩ at a later time 
t
2
t 
2
​	
  by a unitary matrix, 
U
U: 
∣
ψ
′
⟩
=
U
∣
ψ
⟩
∣ψ 
′
 ⟩=U∣ψ⟩. That matrix
U
U may depend on the times 
t
1
t 
1
​	
  and 
t
2
t 
2
​	
 , but does not depend on the states 
∣
ψ
⟩
∣ψ⟩ and 
∣
ψ
′
⟩
∣ψ 
′
 ⟩.
Our quantum gates demonstrate this postulate in action. So, for instance, the Pauli 
X
X gate, also known as the quantum NOT gate, is an example. Here it is shown in the quantum circuit and matrix representations, as well as the explicit action on states:


So too is the Hadamard gate, 
H
H:


And so on, through the controlled-NOT gate, the Toffoli gate, and all the other quantum gates we met earlier.

Why is it unitary matrices which appear in the second postulate? If you try writing a few matrices down on paper, you quickly find that most matrices aren't unitary, or even close to it. Why can't we have a general matrix in the second postulate? One partial explanation, discussed in depth in the earlier essay, is that unitary matrices are the only matrices which preserve length. If we want the quantum state to remain normalized, then unitary matrices are the only matrices which do the trick, since any other matrix will result in the norm changing. That normalization is in turn connected to the requirement that the probabilities of measurement outcomes sum to one. In this sense, the postulates of quantum mechanics form a tightly interconnected web, with requirements like unitarity from one postulate reflecting requirements elsewhere, like normalization of the state vector, or probabilities summing to one.

How to figure out which unitary transformation is needed to describe any particular physical situation? As you might guess from our discussion of the first postulate, the second postulate is silent on this question. It needs to be figured out case by case. Theories like QED and the standard model supply additional rules specifying the exact (unitary) dynamics of the systems they describe. It's as before: quantum mechanics is a framework, not a complete physical theory in its own right. But being told that the correct way to describe dynamics is using unitary transformations on state space is already an incredibly prescriptive statement. And, as before, the quantum circuit model is a useful source of examples, and working with it is a good way to build intuition.
the early 1960s, quantum physics was regarded as one of the most successful theories of all time. It explained a wide range of phenomena to an unprecedented level of accuracy, from the structure of atoms and the formation of chemical bonds, to how lasers and superconductors worked. For some, it was more than just a theory, providing an all-encompassing framework for understanding the micro-world of elementary particles. However, it turned out that the very foundations of that entire framework were built on shaky ground – and the person who noticed wasn’t a physicist but an up-and-coming philosopher.

The debate that resulted not only opened the door to new ways of thinking about those foundations, but also had tucked away within it, overlooked by all the participants at the time, an entirely different philosophical perspective on quantum physics – one that can be traced back to the phenomenological philosopher Edmund Husserl. The impact of that shift in perspective is only now being fully appreciated, offering an entirely novel understanding of quantum mechanics, one that prompts a complete re-evaluation of the relationship between philosophy and science as a whole.

The philosopher who kick-started that debate was Hilary Putnam, who went on to make groundbreaking advances in philosophy of language and philosophy of mind, as well as in computer science, logic and mathematics. In 1961, he responded to a paper offering a resolution of the so-called Einstein-Podolsky-Rosen (EPR) paradox, which appeared to show that the description of reality offered by quantum mechanics could not be complete. In the course of his argument, Putnam pointed out that there was an even more profound problem that lay at the very heart of the theory, as it was standardly understood, and which had to do with one of the most basic of all scientific procedures: measurement.

That problem can be set out as follows. A crucial element in the formalism of quantum mechanics is a mathematical device known as the ‘wave function’. This is typically taken to represent the state of a given system – such as an atom or an electron – as a superposition of all its possible states. So, consider an electron and the property known as ‘spin’. (This is not really the same as the spin put on a ball in a game of baseball or cricket, but the name has stuck.) Spin comes in two forms, labelled ‘up’ and ‘down’, and so when we use the wave function to represent the spin state of our electron as it travels towards our detector, it is as a non-classical superposition of spin ‘up’ and spin ‘down’. However, when we come to measure that spin, the outcome is always one or the other, either ‘up’ or ‘down’, never a superposition of both. How can we account for the transition from that superposition to a definite outcome when we perform a measurement?

This question forms the basis of what came to be known as the ‘measurement problem’. One influential answer emerged from the mind of one of the greatest mathematicians of all time, János (or ‘John’) von Neumann, who was responsible for many important advances, not only in pure mathematics and physics but also in computer design and game theory. He pointed out that when our spin detector interacts with the electron, the state of that combined system of the detector + electron will also be described by quantum theory as a superposition of possible states. And so will the state of the even larger combined system of the observer’s eye and brain + the detector + electron. However far we extend this chain, anything physical that interacts with the system will be described by the theory as a superposition of all the possible states that combined system could occupy, and so the crucial question above will remain unanswered. Hence, von Neumann concluded, it had to be something non-physical that somehow generates the transition from a superposition to the definite state as recorded on the device and noted by the observer – namely, the observer’s consciousness. (It is this argument that is the source of much of the so-called New Age commentary on quantum mechanics about how reality must somehow be observer-dependent, and so on.)

What bothered Putnam was that if we accept von Neumann’s conclusion, then the theory could not be extended to apply to the entire Universe, because that would require an observer existing beyond the physical universe whose consciousness would collapse the superposition of all the Universe’s possible states into one definite one. Either physicists would have to give up the idea that quantum theory was universally applicable, or the standard account of measurement would have to be abandoned.

Putnam’s short paper, published in the journal Philosophy of Science, happened to be read by Henry Margenau, a former physicist turned philosopher of science, who then alerted the Nobel Prize-winning physicist Eugene Wigner. Together they published a response in which they defended von Neumann’s argument and dismissed Putnam’s concern. The debate then went back and forth over several years, the two sides essentially talking past each other until Abner Shimony decisively entered the fray. The holder of two PhDs, one each in philosophy and physics, and a former student of Wigner himself, Shimony subsequently went on to play a leading role in devising the experimental tests of Bell’s theorem (which builds on the EPR result by ruling out certain attempts to supplement quantum mechanics). He weighed in behind Putnam. The central concern was this: just how does consciousness effect this transition from a superposition to a definite state? With no satisfactory answer forthcoming, it appeared that Putnam and Shimony had won the day, clearing the philosophical ground for alternative approaches such as Hugh Everett’s Many-Worlds interpretation, according to which there is no such transition at all and each element of the superposition is realised as a definite outcome, albeit in a different branch of reality or alternative world.
ather than drawing on von Neumann’s chain argument as presented in his own text, which was quite technical and had only recently been translated into English, both sides in the debate actually cited core passages from what Wigner referred to as a ‘little book’ by two other physicists, Fritz London and Edmond Bauer. Originally published in French in 1939, La théorie de l’observation en mécanique quantique (The Theory of Observation in Quantum Mechanics) formed part of a series of semi-popular expositions of the latest advances in science and technology, covering everything from anthropology to zoology. At just 51 pages long, the pamphlet aimed to set out clearly and accessibly not only the basic framework of the quantum mechanical treatment of measurement, but the role of consciousness in that process. Regarded by both sides as a mere summary of von Neumann’s argument, it was anything but.

Both Bauer and London worked in Paris at the time, the former at the prestigious Collège de France and the latter at the Institut Henri Poincaré. Bauer was an excellent teacher and the first in France to teach the new quantum theory. London, however, was in a different league entirely. He earned his quantum mechanical spurs by showing how the theory could explain chemical bonding, leading his collaborator Walter Heitler to exclaim: ‘Now we can eat chemistry with a spoon!’ London went on to successfully apply the theory to superconductivity with his brother Heinz, and then used it to explain the superfluid behaviour of liquid helium, subsequently publishing a two-volume book on these phenomena that became a classic in the field.
What is phenomenology? It can be summarised as a fundamental enquiry into the correlations between mental acts or experiences, the objects that these acts or experiences are about, and the contents or (where appropriate) meanings of these acts or experiences. Its primary tool is known as the epoché (from the Greek for ‘suspension’), which requires the phenomenological investigator to ‘bracket off’ the world around us, and suppress the ‘natural attitude’ that blithely takes that world to be objective. The idea is to break the hold that such an attitude has on us so that we may uncover the fundamental epistemological and metaphysical presuppositions underpinning it.

It is important to note that this bracketing off does not mean ‘denying the existence of’. Adopting this manoeuvre does not amount to an endorsement of scepticism, nor should it be understood as leading to solipsism. Instead, by using the epoché, we can hold up to scrutiny both the supposedly objective world and that natural attitude, thereby reorienting our understanding of both. What we then discover is that the relationship between our consciousness and the world should be understood as ‘correlative’, in the sense that both exist in a ‘mutually dependent context of being’, as Maximilian Beck put it in 1928. This is not to say that consciousness and the world should be conceived of as existing independently of one another prior to being related, nor that the former somehow creates the latter. Rather, it is the correlations that constitute both consciousness and the world.

There is a great deal more to phenomenology than this, and indeed not everyone agrees with the correlationist interpretation. But it is this view that underpins London and Bauer’s ‘little book’ on measurement in quantum mechanics, which played such a crucial role in the debate over the role of consciousness in that process. Recall that Margenau and Wigner defended the standard view that consciousness somehow produces a definite observation from a quantum superposition, taking London and Bauer to be simply summarising von Neumann’s argument. Putnam and Shimony, on the other hand, questioned that whole approach, pressing the point that it was unclear how consciousness could actually yield such a result. However, both sides in that debate missed the core point of the ‘little book’. London and Bauer actually went beyond von Neumann in adopting a phenomenological perspective on the issue, according to which consciousness plays a constitutive role via the correlation between the observer and the world. They themselves make it clear how they are departing from the standard approach in the introduction:

Without intending to set up a theory of knowledge, although they were guided by a rather questionable philosophy, physicists were so to speak trapped in spite of themselves into discovering that the formalism of quantum mechanics already implies a well-defined theory of the relation between the object and the observer, a relation quite different from that implicit in naive realism, which had seemed, until then, one of the indispensable foundation stones of every natural science.
What London and Bauer are saying here is that quantum mechanics must be understood as not just a theory like any other – that is, as about the world in some sense – but as a theory of knowledge in itself, insofar as it ‘implies a well-defined theory of the relation between the object and the observer’. This represents a crucial difference from classical physics as it is usually understood. From the perspective of quantum mechanics, the relationship between the observer and the object being observed must now be seen as quite different from that which underpins the previous stance of ‘naive realism’, which is typically adopted with regard to classical mechanics and which holds that objects exist entirely independently of all observation and possess measurable properties, whether these are actually measured or not. That view must now be abandoned. The core of London and Bauer’s text then represents an attempt to articulate the nature of that relationship between the observer and the object or system being measured.

London and Bauer radically depart from von Neumann’s argument at a crucial juncture. In setting out the chain of correlations, from detector + system to observer’s body + detector + system, they do not stop at the consciousness of the observer but also include this in the overall quantum superposition. It is this move that expresses in physical terms the phenomenological idea of the ‘mutually dependent context of being’, so that not just the body of the observer but their consciousness is also correlated, quantum mechanically, with the system under investigation.

How do we go from that correlation, manifested through the quantum superposition, to having a definite belief corresponding to our observation of a certain measurement outcome? Here, London and Bauer insist that

it is not a mysterious interaction between the apparatus and the object that produces a new [wave function] for the system during the measurement. It is only the consciousness of an ‘I’ who can separate himself from the former function … and, by virtue of his observation, set up [or, in the original French, ‘constituer’, constitute] a new objectivity in attributing to the object henceforward a new function.
In other words, the transition from a superposition to a definite state is not triggered in some mysterious fashion by the consciousness of the observer and, as a result, Putnam and Shimony’s concern regarding how consciousness can cause a definite state to be produced is simply sidestepped. Instead, what we have is a separation of consciousness from the superposition, leading to a ‘new objectivity’, that is, a definite belief on the part of the observer and a definite state attributed to the system.

How can the observer step outside her own perspective and into that of another?

This separation is effected, as London and Bauer explain, via

a characteristic and quite familiar faculty which we can call the ‘faculty of introspection’. [The observer] can keep track from moment to moment of his own state. By virtue of this ‘immanent knowledge’ he attributes to himself the right to create his own objectivity – that is, to cut the chain of statistical correlations.
And, in a typed note inserted by London in his own copy of the manuscript, he wrote:

Accordingly, we will label this creative action as ‘making objective’. By it the observer establishes his own framework of objectivity and acquires a new piece of information about the object in question.
It is this characteristic and familiar act of reflection that cuts the chain of statistical correlations expressed by quantum theory as a set of nested superpositions, and keeps the twin phenomenological poles of those correlations – namely consciousness and the world – mutually separate. And so, on the one hand, the system is objectified, or ‘made objective’, in the sense of having a definite state attributed to it, and, on the other, the observer acquires a definite belief state through this objectifying act of reflection.

London and Bauer were not unaware of the radical nature of what they were saying. In the final section of their work, they acknowledge that, as a result, it might appear that the idea of scientific objectivity itself was under threat. Indeed, this is a general problem with all such views that deny that states of systems are observer-independent – how can the observer step outside her own perspective and into that of another, and thereby establish what London and Bauer call a ‘community of scientific perception’ about what constitutes the object of the investigation? Their response is to insist that ‘one always has the right to neglect the effect on the apparatus of the “scrutiny” of the observer.’

To understand what they mean here, it is important to realise that the word ‘scrutiny’ in this quote is translated from ‘regard’ in the original French text, where the placing of this term between quote marks in the original text itself indicates its significance. Within phenomenology, this ‘regard-to’ is a fundamental reflective act, which, when directed to something, can be understood in terms of consciousness grasping or seizing upon it. When it comes to mental processes, their existence is, then, guaranteed by that ‘regard’. However, although physical objects are likewise brought within the purview of consciousness by the ‘regard’, their existence is not, of course, guaranteed by it (note that phenomenology does not amount to a form of solipsism).

So, when it comes to the measurement apparatus, operated by the physicist in the ‘natural attitude’, we can neglect the effect on it of this ‘regard-to’. And we can further justify our ‘right’ to do so by appeal to what is now known as quantum decoherence. Although there were indications of the core principle behind this as early as 1929, the framework was clearly set out in the early 1970s. The basic idea is that, when a system interacts with the measurement apparatus, the coherence associated with the superposition appears to be lost among the many more physical degrees of freedom offered by the apparatus compared with that of the system. As a result, even though this process does not, in itself, lead to a definite state, as the superposition is still present, the behaviour of the measurement apparatus may be regarded as classical to all intents and purposes. The ‘scrutiny’ or ‘regard’ of the observer can be ignored (unlike the case when we do consider the transition to a definite state) and a collective scientific perception achieved.

In a lecture given in 1925, shortly before the first papers on the new quantum mechanics appeared, Husserl made it clear that phenomenology needed to be brought down from the abstract heights of philosophical theorising and expressed in concrete terms, stating:

The task that now arises is how to make [the] correlation between constituting subjectivity and constituted objectivity intelligible, not just to prattle about it in empty generality but to clarify it in terms of all the categorial forms of worldliness, in accordance with the universal structures of the world itself.
A little more than 10 years later, in The Crisis of European Sciences and Transcendental Philosophy (1936), his final, magisterial, incomplete work, Husserl decried the way in which the mathematisation of ‘material nature’ had led to its conceptualisation as distinct from consciousness. In order for this split to be healed, he argued, there needed to be a fundamental shift back to ‘the universe of the subjective’ via the adoption of the phenomenological stance. Only then can the results of science in general, and physics in particular, be properly grasped and understood.

Merleau-Ponty argued that the observer should not be placed beyond the reach of the wave function

Unfortunately, Husserl died the year before London and Bauer’s ‘little book’ was published, but if he had read it he might have appreciated how they had, in effect, responded to both of his concerns. By couching the relationship between observer and system within a phenomenological framework, they clarified the correlation between constituting subjectivity and constituted objectivity in terms of that specific ‘categorial [form] of worldliness’ represented by quantum mechanics. Furthermore, London and Bauer showed that, by virtue of embodying that correlative relationship between ourselves and the world, quantum mechanics, conceived of phenomenologically, bridges the psychophysical divide and restores within physics, and indeed science as a whole, ‘the universe of the subjective’.

This restoration of the phenomenological nature of London and Bauer’s text – something that was entirely overlooked in the debate between Putnam and Shimony, on one side, and Margenau and Wigner, on the other – is therefore important firstly for illustrating how that particular philosophical movement was entwined with the development of quantum physics, and secondly for situating this ‘little book’ at an early stage in the evolution of a philosophical approach to that theory that has been largely ignored within the philosophy of physics, at least until recently.

This is not to say that other writers in the phenomenological tradition failed to bring quantum mechanics within their philosophical purview. Gurwitsch and Patrick Heelan also emphasised the phenomenological role of human consciousness in the measurement process, again citing London and Bauer. Maurice Merleau-Ponty, one of the most prominent phenomenological thinkers, similarly engaged with quantum theory while in Paris, and likewise argued that the observer should not be placed beyond the reach of the wave function, but must be included in the description of reality offered by physics. He went on to have a significant influence on subsequent writers, including Michel Bitbol who, together with his collaborators, has developed a form of eco-phenomenology that allies the phenomenological stance with an approach to quantum mechanics known as QBism. Initially developed by the physicist Christopher Fuchs, this similarly adopts a first-person approach that takes the concepts of agent and experience as fundamental, and understands the wave function as representing not the state of the system, but that of that agent when it comes to their possible future experiences.

Recent developments such as these have converged in a series of conferences, in turn resulting in two landmark collections, both edited and with useful introductions by Harald Wiltsche and Philipp Berghofer: Phenomenological Approaches to Physics (2020), which also covers phenomenological approaches to the theory of relativity, and Phenomenology and QBism (2024).

Within the phenomenological framework, then, one of the central problems of quantum mechanics is resolved or, perhaps better, dissolved, through a subtle but crucial shift to understanding it as a theory of knowledge by virtue of embodying our correlative participation in the world. Whether or not you fully agree with such a philosophical stance, it not only adds a hugely stimulating and potentially fruitful dimension to our understanding of one of the most fundamental constituents of modern physics, but also throws new light on the often-overlooked significance of philosophical reflection in these developments.
However, London was not just a brilliant physicist. He was also keenly interested in philosophy from a young age. As a student at the University of Munich, he came to the attention of Alexander Pfänder, professor of philosophy and righthand man of Edmund Husserl, the founder of phenomenology. Indeed, London’s thesis on the nature of scientific theories was published in the leading phenomenological journal of the time, the Jahrbuch für Philosophie und Phänomenologische Forschung (the ‘Yearbook for Philosophy and Phenomenological Research’), which was edited by Pfänder himself. And this was no mere youthful fixation; London maintained his interest in phenomenology throughout his career. While in Paris, he had long discussions about physics and philosophy with his friend Aron Gurwitsch, who, like London, had an academic background in both subjects and went on to help establish phenomenology in the United States.
That debate, as historically important as it was for the further development of the foundations of quantum mechanics, also contained a significant philosophical element that was completely overlooked for many years, and that not only offers an entirely novel response to Putnam and Shimony’s concern, but also opens the door to a fundamentally different understanding of quantum physics. What they didn’t notice was the phenomenological angle.
More broadly: although quantum mechanics reached its final form in the 1920s, physicists spent much of the remainder of the twentieth century figuring out what unitary dynamics, state spaces, and quantum states are needed to describe this or that system. You can't just solve this problem once: optical physicists had to do it for light, atomic physicists for atoms, particle physicists have been doing it for the entire pantheon of particles described in the standard model of particle physics. Still, although there's much more to learn about the application of these two postulates, already they give us a remarkably constraining framework for thinking about what the world is and how it can change.
A point we've glossed over is the use of the term isolated in the first postulate. In particular, the first postulate tells us that every physical system has a state space, but inserts the qualifier isolated when saying which physical systems have a state vector. By isolated we mean a system that's not interacting with any other system. Of course, most physical systems aren't isolated. An atom will interact with its surroundings (for instance, it may be hit by a photon, or perhaps be affected by the charge of a nearby electron). A human being isn't an isolated physical system either – we're constantly being bombarded by light, cosmic rays, and all sorts of other things.
Isaac Newton believed that light is composed of particles, and he had good reason
to think so. All wave motion exibits interference and diﬀraction eﬀects, which are
the signature of any phenomenon involving waves. Newton looked for these eﬀects
by passing light through small holes, but no diﬀraction eﬀects were observed. He
concluded that light is a stream of particles.
One of Newton’s contemporaries, Christian Huygens, was an advocate of the wave
theory of light. Huygens pointed out that the refraction of light could be explained
if light moved at diﬀerent velocities in diﬀerent media, and that Newton’s inability
to find diﬀractive eﬀects could be due simply to the insensitivity of his experiments.
Interference eﬀects are most apparent when wavelengths are comparable to, or larger
than, the size of the holes. If the wavelength of light were very small compared to the
size of the holes used by Newton, interference eﬀects would be very hard to observe.
Huygens turned out to be right. More sensitive optical experiments by Young
(1801) and Fresnel demonstrated the interference and diﬀraction of light, and mea-
surements by Foucault (1850) showed that the speed of light in water was diﬀerent
from the speed of light in air, as required to explain refraction. Then Maxwell, in
1860, by unifying and extending the laws of electricity and magnetism, demonstrated
that electric and magnetic fields would be able to propagate through space as waves,
traveling with a velocity v = 1/√µ0ϵ0, which turned out to equal, within experimen-
tal error, the known velocity of light. Experimental confirmation of the existence of
electromagnetic waves followed shortly after, and by the 1880s the view that light is
a wave motion of the electromagnetic field was universally accepted.
It is a little ironic that following this great triumph of the wave theory of light,
evidence began to accumulate that light is, after all, a stream of particles (or, at least,
light has particle properties which somehow coexist with its wave properties). The
first hint of this behavior came from a study of black-body radiation undertaken by
Max Planck, which marks the historical beginning of quantum theory.
Any object, at any finite temperature, emits electromagnetic radiation at all pos-
sible wavelengths. The emission mechanism is simple: atoms are composed of nega-
tively charged electrons and positively charged nuclei, and upon collision with other
atoms these charges oscillate in some way. According to Maxwell’s theory, oscillating
charges emit (and can also absorb) electromagnetic radiation. So it is no mystery
that if we have a metallic box whose sides are kept at some constant temperature
T , the interior of the box will be filled with electromagnetic radiation, which is con-
stantly being emitted and reabsorbed by the atoms which compose the sides of the
box. There was, however, some mystery in the energy distribution of this radiation
as a function of frequency. Since there can only be an integer number of photons n at any given frequency, each
of energy hf, the energy of the field at that frequency can only be nhf. Planck’s
restriction on energies is thereby explained in a very natural, appealing way.
Except for one little thing. ”Frequency” is a concept which pertains to waves;
yet Einstein’s suggestion is that light is composed of particles. The notion that
the energy of each ”particle” is proportional to the frequency of the electromagnetic
”wave”, which in turn is composed of such ”particles”, seems like an inconsistent mix
of quite diﬀerent concepts. However, inconsistent or not, evidence in support of the
existence of photons continued to accumulate, as in the case of the Compton eﬀect.
2.3 The Compton Eﬀect
Consider an electromagnetic wave incident on an electron at rest. According to clas-
sical electromagnetism, the charged electron will begin to oscillate at the frequency of
the incident wave, and will therefore radiate further electromagnetic waves at exactly
the same frequency as the incident wave. Experiments involving X-rays incident on
free electrons show that this is not the case; the X-rays radiated by the electrons are
a frequencies lower than that of the incident X-rays. Compton explained this eﬀect
in terms of the scattering by electrons of individual photons.
According to special relativity, the relation between energy, momentum, and mass
is given by
E= p2c2 + m2c4 (2.14)
For particles at rest (p= 0), this is just Einstein’s celebrated formula E= mc2
.
For a particle moving at the speed of light, such as a photon, the rest mass m = 0;
otherwise the momentum would be infinite, since momentum p is related to velocity
v via the relativistic expression
p=
mv
(2.15)
1−
v2
c2
Then if, for a photon, m = 0 and E= hf, and given the relation for waves that
v = λf, we derive a relation between photon momentum and wavelength
E
p=
=
c
hf
c
=
h
λ (2.16)
where λ is the wavelength of the electromagnetic wave; in this case X-rays.
Now suppose that a photon of the incident X-ray, moving along the z-axis, strikes
an electron at rest. The photon is scattered at an angle θ relative to the z-axis,
while the electron is scattered at an angle φ, as shown in Fig. [2.4]. If⃗
p1 denotes the
momentum of the incident photon,⃗
p2 denotes the momentum of the scattered photon,
and⃗
pe is the momentum of the scattered electron, then conservation of momentum
tells us that
In general such non-isolated systems don't have their own quantum state! In fact, we saw an example in the earlier essay. 
Of course, it could be that the relation (2.28) is simply a practi-
cal limit on measurement; a particle might have a definite position and momentum
despite our inability to measure those quantities simultaneously. But the diﬃculty
could also be much more profound: if a physical state is simply the mathematical
representation of the outcome of an accurate measurement process (a view which
was advocated in Fig. [1.5] of Lecture 1) and if accurate numbers (x, p) are never
an outcome of any measurement, then perhaps we are mistaken in thinking that a
physical state corresponds to definite values of (x, p). In other words, the origin of
the uncertainty could be due to trying to fit a square peg (the true physical state,
whatever that may be) into a round hole (the set (x, p)). At the very least, if x and
p cannot be measured simultaneously, then there is certainly no experimental proof
that the classical state is the true physical state. This view is obviously a very radical
option; for the moment we only raise it as a possibility, and turn to the mystery of
the stability of the atom.
2.5 The Bohr Atom
Atoms have radii on the order of 10−10 m, and have masses on the order of 10−26 kg.
In 1911, Ernest Rutherford studied the internal structure of atoms by bombarding
gold foil with α-particles from radioactive Cesium. By studying the scattering of
the α-particles by the gold atoms (a topic we will turn to at the end of the course),
Rutherford found that almost all the mass of the atom is concentrated in a positively
charged nucleus, of radius on the order of 10−15 m, i.e. 100,000 times smaller than
the radius of the atom itself. The nucleus is surrounded by much lighter, negatively
charged electrons, which collectively account for less than 1/2000th of the total mass
of the atom. Atomic structure was pictured as analogous to the solar system, with the
nucleus playing the role of the sun, orbited by much lighter electrons (the ”planets”),
bound in their orbits by Coulomb attraction to the nucleus.
However, orbital motion is a form of accellerated motion, and electrons are charged
particles. According to electromagnetic theory, an accellerating charged particle ra-
diates electromagnetic waves. As electrons move in their orbits, they should be con-
stantly radiating energy in the form of electromagnetic waves, and as the electrons
lose energy, they should spiral into the nucleus; a process which would take only a
fraction (about 10−10) of a second. By this reasoning, atoms should be about the size
of nuclei, but in fact they are roughly 100,000 times larger. So what accounts for the
stability of the electron orbits; why don’t electrons spiral into the nucleus?
Another mystery of atomic structure was the existence of spectral lines. If a gas
is placed in a discharge tube, with a suﬃciently large voltage diﬀerence maintained
at opposite ends of the tube, the gas glows. But, unlike thermal radiation (which
occurs due to random collisions among atoms) the light emitted from the discharge
tube is not spread diﬀusely over a broad range of frequencies, but is composed instead
of discrete, very definite wavelengths. Matter in the solid state consists of atoms in a regular (”crystalline”) array of
some kind, and the atomic structure of solids is determined by X-ray diﬀraction. X-
rays, being a form of wave motion, reflect oﬀ the atoms in the array, and interfere to
form a pattern which can be calculated from the principles of physical optics, given a
knowlege of the structure of the array, and the wavelength of the X-rays. The inverse
problem, finding the structure of the array given the X-ray interference pattern, is
the subject of X-ray crystallography.
In 1927 Davisson and Germer, in an eﬀort to check De Broglie’s hypothesis that
electrons are associated with wave motion, directed a beam of electrons at the surface
of crystalline nickel. The electrons were reflected at various angles, and it was found
that the intensity of the reflected electrons, as a function of reflection angle, was
identical to the interference pattern that would be formed if the beam were instead
composed of X-rays. Assuming that the electron beam was indeed some sort of
wave, the wavelength could be determined from the intensity pattern of the reflected
electrons. The wavelength λ was found to equal, within experimental error, the
de Broglie prediction λ= h/p, where p is the electron momentum, determined by
accellerating the incident beam of electrons across a known potential V. Apart from
brilliantly confirming the existence of ”de Broglie waves”, this is an experiment with
extraordinary and profound implications. To discuss these implications, it is useful
to consider an experimental setup less complicated than that used by Davisson and
Germer, in which the electrons are directed at an inpenetrable barrier containing two,
very narrow, slits. First, however, we need an expression for the wavefunction of de
Broglie waves. When a conclusion (eq. (3.35) in this case) turns out to be false, and the reasoning
which led to the conclusion is correct, then there must be something wrong with
the premises. The premises in this case are that the electrons in the beam travel
independently of one another, and that each electron in the beam passed through
either slit A or slit B. More generally, we have assumed that the electron, as a
”pointlike particle”, follows a trajectory which takes it through one of the two slits
on its way to the screen.
It is easy to check whether the interference eﬀect is due to some collective inter-
action between electrons in the beam. We can use a beam of such low intensity that
the electrons go through the barrier one by one. It takes a little longer to accumulate
the data needed to compute PA(y), PB (y), and P (y), but the results are ultimately
the same. So this is not the explanation.
Then perhaps we should stop thinking of the electron as a point-like object, and
start thinking of it as literally a de Broglie wave? A wave, after all, can pass through
both slits; that is the origin of interference. The problem with this idea is that de
Broglie waves expand. At slits A and B, the wave is localized just in the narrow
region of the open slits, but as the wave propagates towards the screen it expands,
typically spanning (in a real electron diﬀraction experiment) a region on the order
of 10 cm across. Then one would detect ”parts” of an electron. Likewise, a wave of such
dimensions should leave a diﬀuse glow on a photographic plate, or a thick cylindrical
track through a cloud chamber. None of this is ever observed. No matter how big
the de Broglie wave of a single electron becomes (as big as a baseball, as big as a
house...), only a single ”click” of a geiger counter is heard, only a sharp ”dot” on a
photographic plate is recorded. An electron is not, literally, a wave. As far as can be
determined from measurements of electron position, electrons are discrete, point-like
objects.
If electrons are pointlike objects, then one could imagine (with the help of a pow-
erful microscope) actually observing the barrier as the electrons reach it, to determine
if they go through slit A, or slit B, or somehow through both. If one would perform
such an experiment,2 the result is that each electron is indeed observed to pass either
through slit A or slit B (never both), but then the interference pattern is wiped out!
Instead, one finds the uniform distribution of eq. (3.35). Thus if an electron is forced
(essentially by observation) to go through one or the other of the two slits, the inter-
ference eﬀect is lost. Interference is regained only if the electron is not constrained to
a trajectory which has passed, with certainty, through one or the other slit.
We are left with the uncomfortable conclusion that electrons are pointlike objects
which do not follow definite trajectories through space. This sounds like a paradox:
how then can electrons get from one point to another? It is actually not a paradox,
but it does require a considerable readjustment of our concept of the physical state
of electrons (not to mention atoms, molecules, and everything else that makes up the
physical Universe). This will be the subject of the next lecture. If in fact we could determine the position and momentum to an accuracy greater
than (6.25), then the particle would be left in a physical state with ∆x∆p <¯ h/2. But
according to eq. (6.24) there are no such physical states. Therefore, measurements of
that kind are impossible.
The Uncertainty Principle is an unavoidable consequence of quantum theory, and
if one could design a measurement process which would be more accurate than the
bound (6.25), then quantum theory would be wrong. We have already discussed one
attempt to measure x and p simultaneously, in the example of the Heisenberg mi-
croscope. In that case it was found that the photon composition of light, combined
with the Rayleigh criterion of resolution, results in ∆x∆p ≈¯ h, which is in agree-
ment with the Uncertainty Principle. There have been other ingenious proposals for
measurements which would violate the Uncertainty Principle, especially due to Al-
bert Einstein in his discussions with Niels Bohr.1 A careful study of these proposals
always reveals a flaw. In much the same way that the existence of perpetual motion
machines is ruled out by the Second Law of Thermodynamics, the existence of an
apparatus which would give a precise determination of position and momentum is in
conflict with the nature of physical states in quantum mechanics.
However, the example of Heisenberg’s microscope often leads to a misunderstand-
ing that the Uncertainty Principle is simply saying that ”the observation disturbs
what is being observed.” It is true that an observation usually changes the physical
state of the observed system. But it is not true that this is full content of the Un-
certainty principle. If position-momentum uncertainty were only a matter of light
disturbing the observed particle, then we would be free to imagine that a particle
really has a definite position and definite momentum at every moment in time, but
that the physical properties of light prevent their accurate simultaneous determina-
tion. This interpretation is wrong, because if a particle had a definite position and
momentum at every moment of time, then the particle would follow a definite tra-
jectory. We have already seen that the assumption that particles follow trajectories
is inconsistent with electron interference. The Heisenberg principle is best under-
stood, not as a slogan ”the observation disturbs what is observed,” but rather as a
consequence of the nature of physical states in quantum mechanics, which cannot be
simultaneously eigenstates of position and momentum. Despite the stationarity, this energy eigenstate is obviously just the limit of a non-
stationary situation, in which an incoming wavepacket is scattered backwards by an
infinite potential. During the (long) interval in which the incoming wavepacket (with
∆p very small) reflects from the end of the tube, the wavefunction near the end of the
tube looks very much like the energy eigenstate (8.10). In fact, in the ∆p → 0 limit,
we can easily identify the part of the eigenstate that corresponds to the incoming
wavepacket (φinc) and the part which corresponds to the scattered wavepacket (φref ).
This is a general feature of unbound states: an unbound stationary state can be
viewed as the limit of a dynamical situation, in which an incoming wavepacket of
a very precise momentum is scattered by a potential. Part of the unbound state is
identified as the incoming wave, other parts represent the scattered waves.
As a second example, consider a particle wavepacket incident on the potential well
of Fig. [8.1]. The incoming wavepacket is shown in Fig. [8.10a]. Upon encountering
the potential, the wavepacket splits into a reflected wavepacket, moving backwards
towards x = −∞, and a transmitted wavepacket which has passed through the well
and is moving on towards x = +∞, as shown in Fig. [8.10c].1 At some intermediate
time, the incoming and reflected wavepackets overlap, and as we take ∆p → 0 for the
incoming wave, the incoming and reflected waves overlap over a very large region. We live in a world which is almost, but not quite, symmetric. The Earth is round,
nearly, and moves in an orbit around the Sun which is circular, almost. Our galaxy
looks roughly like a spiral. Human beings and most animals have a left-right symme-
try; the left-hand side looks the same as the right-hand side, more or less. Starfish
have a pentagonal symmetry, table salt has a cubic symmetry, and in general Nature
abounds in geometrical shapes of various kinds.
The symmetry which is apparent in large structures (animals, planets, galaxies...)
is even more evident at the very small scales that are the domain of quantum physics.
Symmetries appear in the arrangement of atoms in solids, in the arrangement of
electrons in atoms, and in the structure of the nucleus. Symmetry principles are
especially important for understanding the variety and behavior of the elementary
particles, out of which all larger structures are built. Stars die. Their long lives are interesting and complex, occasionally culminating
in fantastic explosions (supernovas) that can briefly outshine entire galaxies. The
details are the subject matter of some other course. For us, it is suﬃcient to know
that every ”living” star balances its immense gravitational force, which tends to crush
all the atoms of the star inward to a single point, with an equally enormous outward
pressure, due to the heat produced by nuclear fusion. Eventually, any star will exhaust
its nuclear fuel, and then the gravitational force, unopposed, crushes the atoms of the
star to a fantastic density. In the course of collapse, for stars greater than about one
solar mass, the atomic electrons are absorbed by protons in the nuclei, via the process
e− + p → n + ν (16.1)
and the massless neutrinos ν, due to their weak interaction with all other matter,
escape into space. At this stage, all of the particles composing the star are neutrons,
the density of the star approximates that of atomic nuclei, and the dead star is known
as a ”neutron star.” For stars with masses less than about four solar masses, that is
the end of the story: the cold dead star remains as a neutron star until the end of
time. But this brings up the question: what force can there possibly be, within the
cold neutron star, that is capable of opposing the mighty gravitational force, which
would otherwise crush all the matter of the star to a single point?
It seems incredible that all this astrophysical drama should have anything at all
to do with the apparently more mundane question of why some materials conduct
electricity, and some don’t. Nevertheless, the physics of dead stars, and that of quite
ordinary solids, are related in certain unexpected ways. Both neutron stars, and the
electrons in cold metals, are examples of what are known as degenerate F ermi
gases. We begin by taking up the question of how is it possible for certain solids to
conduct electricity. A crystalline solid is a regular array of atoms, and, at first sight, conduction of
electricity is a mystery: if electrons are bound to atoms, how is possible for them to
move through the solid under the influence of a small electric field? The answer is
that in a crystal, not all of the electrons are actually bound to the atoms; in fact,
some of the electrons in the metal behave more like a gas of free particles, albeit with
some peculiar characteristics which are due to the exclusion principle.
To understand how electrons in a crystal can act as a gas, it is useful to solve for
the electron energy eigenstates in a highly idealized model of a solid, known as the
Kronig-Penny model, which makes the following simplifications:
S1. The solid is one-dimensional, rather than three-dimensional. The N atoms are
spaced a distance a from one another. In order that there are no special eﬀects
at the boundaries, we consider a solid has no boundary at all, by arranging the
atoms in a circle as shown in Fig. [16.1].
S2. Instead of a Coulomb potential, the potential of the n-th atom is represented
by a delta-function attractive potential well
Vn(x) =−gδ(x− xn) (16.2)
where xn is the position of the n-th atom.
S3. Interactions between electrons in the 1-dimensional solid are ignored.
Obviously, these are pretty drastic simplifications. The important feature of this
model, which it shares with realistic solids, is that the electrons are moving in a
periodic potential. For purposes of understanding the existence of conductivity, it
is the periodicity of the potential, not its precise shape (or even its dimensionality)
which is the crucial feature.
Arranging the atoms in a circle, as in Fig. [16.1], means that the position variable
is periodic, like an angle. Just as θ + 2π is the same angle as θ, so the position x + L
is the same position as x, where
L= N a (16.3)
is the length of the solid. Let the position of the n-th particle be xn = na, n =
0, 1, ..., N− 1, the potential then has the form. Thus there can a maximum of two
electrons (spin up and spin down) at any allowed energy in an energy band.
At the lowest possible temperature (T= 0 K), the electrons’ configuration is the
lowest possible energy consistent with the Exclusion Principle. A perfect Insulator
is a crystal in which the electrons completely fill one or more energy bands, and there
is a gap in energy from the most energetic electron to the next unoccupied energy
level. In a Conductor, the highest energy band containing electrons is only partially
filled.
In an applied electric field the electrons in a crystal will tend to accellerate, and
increase their energy. But...they can only increase their energy if there are (nearby)
higher energy states available, for electrons to occupy. If there are no nearby higher
energy states, as in an insulator, no current will flow (unless the applied field is so
enormous that electrons can ”jump” across the energy gap). In a conductor, there
are an enormous number of nearby energy states for electrons to move into. Electrons
are therefore free to accellerate, and a current flows through the material.
The actual physics of conduction, in a real solid, is of course far more complex
than this little calculation would indicate. Still, the Kronig-Penny model does a
remarkable job of isolating the essential eﬀect, namely, the formation of separated
energy bands, which is due to the periodicity of the potential.
16.2 The Free Electron Gas
In the Kronig-Penney model, the electron wavefunctions have a free-particle form in
the interval between the atoms; there is just a discontinuity in slope at precisely the
position of the atoms. In passing to the three-dimensional case, we’ll simplify the
situation just a bit more, by ignoring even the discontinuity in slope. The electron
wavefunctions are then entirely of the free particle form, with only some boundary
conditions that need to be imposed at the surface of the solid. Tossing away the
atomic potential means losing the energy gaps; there is only one ”band,” whose
energies are determined entirely by the boundary conditions. For some purposes
(such as thermodynamics of solids, or computing the bulk modulus), this is not such
a terrible approximation.
We consider the case of N electrons in a cubical solid of length L on a side. Since
the electrons are constrained to stay within the solid, but we are otherwise ignoring
atomic potentials and inter-electron forces, the problem maps directly into a gas of
non-interacting electrons in a cubical box. According to the principles of quantum mechanics, physical states are normalized
vectors in Hilbert space, and the sum of two vectors is another vector. If that vector
is normalized, it is a physical state too. So the wavefunction (22.3) is, from this point
of view, entirely kosher: it is a physically allowed state of the two-particle system. But
then if we ask: what is the state of particle 1, when the two-particle system is in state
(22.3), we discover that we can’t answer the question! Particle 1 is not in state ψA,
nor is it in state ψC ; neither is it in a linear combination such as cψA(x1) + dψC (x1).
The fact is, the state of particle 1 has become entangled with that of particle 2; it is
impossible to specify the state of either particle separately, even if the two particles
are non-interacting, and very far apart.
The fact that the quantum states of composite systems can be inseparable (or
entangled) in this way was first noticed by Einstein, Podolsky, and Rosen (”EPR”)
in 1933. The consequences of this entanglement are truly mind-bending, even by the
generous mind-bending standards of quantum mechanics.
22.1 The EPR Paradox
Einstein, Podolsky and Rosen were of the opinion that quantum mechanics is in some
way incomplete, because it leads to (what they considered to be) a paradox in certain
circumstances. To illustrate their point, we consider the following experiment (Fig.
22.1). Every time a button is pushed, two spin 1/2 particles are ejected, in opposite
directions, each particle eventually passing through a detector which measures the
spin of the particle along a given axis. The detectors can be rotated, so that the spin
along any axis, in particular along the x- and z-axes, can be measured. After many
trials, the following result is reported:
A. Both detectors are set to measure spin along the z-axis. Whenever the spin of the
particle on the left is found to be spin up, and this occurs 50% of the time, the
spin of the particle on the right is found to be spin down. Likewise, whenever
the particle on the left is found to be spin down, which is the other 50% of the
time, the particle on the right is found to be spin up.
This means that by measuring the z-component of the spin of the particle on the
left, we can determine the z-component of the spin of the particle on the right, without
actually interacting with the particle on the right. Suppose, in a particular run, that
the particle which moves to the left (which we’ll now call ”L”) is found to be spin-
up. With observable O2 and state Ψ chosen in this way, then by following the standard
rules of quantum theory we can compute the probabilities for finding outcomes GG,
RR, RG, and GR, for switch settings 12, 21, 22, and 11. These probabilities are
shown in Fig. 22.3 They are in complete agreement with the asserted results (A), (B)
and (C) above.
In this way, Bell’s Theorem is proven. There are physical states in quantum
mechanics which lead to predictions that can never be reproduced by a local hidden
variables theory. These physical states are always of the ”entangled” form, in which
it is impossible to deduce the state of either particle separately.
Of course, the example constructed here, although suﬃcient to prove theorem,
is rather special. Bell’s theorem, in more generality, states that if the measured
values of various observables violate certain inequalities, then those results cannot be
explained by any local hidden variables theory. This brings us to an experimental
question: Maybe quantum mechanics is wrong! Can we actually observe, in the
laboratory, results which violate the Bell inequalities (i.e. cannot be explained by
local hidden variable theories)? The relevant experiments were performed by Aspect and his collaborators in the
1970s. The two particles were two photons produced by the decay of Positronium
(an electron-positron bound state). All of the quantum-mechanical predictions were
confirmed. The mysterious non-local behavior of quantum theory, in which a mea-
surement of particle 1 somehow causes the distant particle 2 to jump into a state of
definite spin, cannot be explained by a local hidden variables theory. Defined in this way, the states {ϕn(x2)} do not have to be orthogonal, they do not
even have to be diﬀerent states. If all the {ϕn(x2)} are the same state, then Ψ(x1, x2)
is separable. If at least some of the {ϕn(x2)} are diﬀerent, then Ψ(x1, x2) is entangled.
Now we make the measurement of observable A on particle 1, and one of the
eigenvalues, λ= λk say, is obtained. This means that particle 1 has jumped into
the state φk(x1). But, if the intial state is entangled, it also means that particle 2
has jumped into state ϕk(x2), which it was not in before. In other words, the entire
two-particle state Ψ(x1, x2) has ”jumped” into state φk(x1)ϕk(x2), even if particles
1 and 2 are far apart, and a measurement is performed only on particle 1. This
certainly seems like a non-local influence, and Bell’s Theorem prevents the ”cheap”
explanation that particle 2 was really in state ϕk(x2) (perhaps supplemented by some
”hidden” variables) all along.
Its natural to suppose that if a measurement at detector 1 causes something to
happen to the distant particle 2, then this eﬀect could be used to send messages, in-
stantaneously, between the observer at detector 1 and the observer at detector 2. But
if instantaneous (or in general, faster than light) communication were possible, then
according to special relativity it should also be possible to send messages backwards
in time. This would have some interesting practical applications. A typical scenario
is as follows:
The four accomplices had planned it for years, after coming into pos-
session of two pairs of super-secret - and highly illegal - quantum radios.
Now for the payoﬀ. John, the ringleader, stands at the betting window of
a major racetrack, while his wife, Mary, waits impatiently on the planet
Mars. The third participant, Rajiv, is piloting a rocketship which is head-
ing towards Earth at a very substantial fraction of the speed of light. His
partner, F atima, is on another rocketship, moving parallel to Rajiv and
at equal speed towards Mars. The objective of all these preparations is
for John to learn the name of the winning horse at the Kentucky Derby,
before the race is actually run.
The worldlines of all four participants are indicated on a spacetime diagram shown
in Fig. 22.4. At point A in Fig. 22.4, the race has just ended.
”Niels Bohr”, a long shot paying 200 to 1, has come in first. As Rajiv’s
rocket streaks overhead, John signals the name of the winning horse to
him. Rajiv then uses his quantum radio to send this information instan-
taneously to F atima. When Rajiv signals the name of the winning horse to F atima, she re-
ceives this information at spacetime point B’, just as her rocket is passing
Mars. F atima relays the important information to Mary, and then Mary
contacts John, again by quantum radio, and transfers the message instan-
taneously (in the John-Mary rest frame) to Earth. The message reaches
John at spacetime point B, several minutes before the race is to begin.
John places a very large bet on ”Niels Bohr” to win, and ...
Lets leave the story there. Could entangled states be used to send instantaneous
messages in this way, even in principle? To answer this question, we again consider the
apparatus shown in Fig. 22.2. John is at detector 1. Mary is at detector 2, but has an
arsenal of laboratory equipment available to her, and can measure any observable she
likes. Is there some observation which she could perform on the right-hand particles,
which would tell her
I. whether John has turned on his detector; or
II. if John’s detector is on, whether the switch is on setting 1 or setting 2? Entangled states are the norm, not the exception, in quantum mechanics. Generally
speaking, when any two systems come into interaction, the resulting state of the
combined system will be entangled. In this technical sense of quantum inseparability,
the poet was right: no man is an island. No electron is an island either.
As we have seen, the existence of entangled states means that a measurement
of one system can cause the second system, perhaps very distant from the first, to
jump into one or another quantum state. It is time to return to the EPR criticism:
”Isn’t this situation paradoxical? Doesn’t it imply that something must be wrong
with quantum theory?”
Now first of all, as pointed out by Niels Bohr in his response to EPR, this situation
is not really a paradox. The non-locality pointed out by EPR is certainly very, very
strange. But quantum non-locality is not actually a paradox in the sense of being a
logical contradiction.
”Well then, doesn’t this non-local behavior violate theory of Relativity?” Accord-
ing to relativity theory, after all, no signal can propagate faster than the speed of
light, so how can the state of particle 2 change instantaneously, in the laboratory
reference frame, upon measurement of particle 1? Here we must be careful - the
relativistic prohibition is against information propagating faster than the speed of
light. If such propagation were possible, it would also be possible to send messages
backwards in time, which would raise many other (authentic!) paradoxes. we have just seen that the observer at detector 2 can never, by simply observing one
of the particles, conclude anything about the settings of detector 1. Non-locality is
not a violation of relativity in the sense of information transfer.
At this point, it is worth noting is that non-locality is not really unique to entan-
gled states; some kind of non-local eﬀect occurs in almost any measurement. Con-
sider, for example a single particle, whose wavepacket ψ(x) is distributed uniformly
in a large, dark room, filled with particle detectors. At a given time t= 0 the detec-
tors are suddenly all switched on and, just as suddenly, the position of the particle
is measured. This means that the wavepacket of the particle, which was spread all
over the room at t < 0, suddenly collapses to a narrow gaussian in the region of one
of the detectors at t= 0. This is known as the ”collapse of the wavefunction”, and it
also seems to be a form of non-local behavior. If we think of the particle wavepacket
as some sort of fluid, by what mechanism does it instantaneously collapse?
Now, in volume 1 of these notes I have urged you to avoid thinking of the wavefunc-
tion as representing some sort of material fluid. There are just too many diﬃculties
inherent in that kind of picture. Rather, one should think of Hilbert Space, rather
than ordinary space, as being the arena of dynamics, where the state (vector) of a
system evolves continuously, according to the Schrodinger equation, by rotation. This
change of arena alleviates a lot of the problems associated with non-locality. On the
other hand, I also said that a measurement causes the state to ”jump,” probabilisti-
cally, into one of a number of eigenstates of the quantity being measured.
Its time to face up to the real problem, the hard problem: By what process does
a measurement cause a state to suddenly jump to one of a set of states? Can this
behavior itself be explained quantum mechanically? This very puzzling issue has not
been settled to this day. It is known as the Problem of Measurement. The simplest answer was given by von Neumann, who urges us to follow the chain
of events into the brain of the observer. The detector is in a superposition of ”red
light/green light” states, and it emits photons in a superposition of the corresponding
frequencies. The photons reach the retina of the observer, and certain neurons are
left in a superposition of excited/un-excited states. The message from the retina
travels to the cerebral cortex; very large collections of neurons are now in quantum
superpositions, and the brain remains in such a superposition until, at some point, a
sensation occurs. At the instant of conscious perception, the observer, the detector,
and even the particle, jump into one or the other of the ”up/down, red/green” states.
What von Neumann is suggesting is that human consciousness causes the wave-
function to ”collapse,” with a certain probablity, into one or another of the possible
neural states; the collapse of the wavefunction occurs due to the awareness of the
observer. It follows, since the Schrodinger equation can never cause a wavefunction
to collapse (i.e. cause a pure state to go to a mixture), that the mental function
described as ”awareness” or ”conciousness” cannot be described by the Schrodinger
equation; it is not a physical process in the usual sense.
The notion that there is something special about conciousness, and that it cannot
be explained by the dynamical laws that govern all other physical processes, is anath-
ema to most scientists. It is reminiscent of vitalism; a theory which held that one
must look beyond the usual laws of physics and chemistry to explain the processes
which occur in living organisms. This theory was, of course, long ago discredited by
spectacular advances in molecular biology.
Still, von Neumann’s idea should not be rejected out of hand. Philosophers have
argued for centuries about the so-called mind/body problem, and there exist sophisti-
cated arguments to the eﬀect that, e.g., a computer following a complicated algorithm
to simulate some aspect of human behavior can never actually ”understand” what it
is doing.1 In the absence of any well-established ”Theory of Consciousness,” it is not
entirely obvious that awareness can be explained entirely in terms of the dynamics
of molecules obeying the Schrodinger equation. von Neumann argues that mental
processes simply cannot be explained in this way, due to the absence of superimposed
mental states. His argument, although obviously radical, cannot be immediately
dismissed. Rather, the wavefunction is a compact
representation of the observer’s information about the observables of a given object,
and merely describes the possible outcome of a series of measurements. Put another
way, the wavefunction does not refer to ”physical reality” per se, in the absence of an
observer; it serves only to predict and correllate measurements. If there is no observer,
then no meaning can be attached to a quantum-mechanical wavefunction. From this
point of view, the ”collapse” of the wavefunction does not describe a new physical
process; it is just a change in the information available to the observer, obtained by
measurement.
The main criticism that can be leveled at Bohr’s interpretation is that it becomes
meaningless to speak of the physical state of an object in the absence of an observer.
How, then, can we describe quantum-mechanical processes that may have occured
in nature prior to the evolution of human beings, or events which, for one reason
or another, may have escaped human scrutiny? In classical physics, every object in
the Universe has a physical state, a definite position and momentum, regardless of
whether or not human beings are around to measure that state. Not so for quantum
mechanics, at least in the Bohr interpretation. It is impossible, in this view, to
imagine that any object is in any definite quantum state, without at least an implicit
reference to the Observer.
The fact that human observers are somehow an essential feature of the quantum-
mechanical description of nature is, for some, a very troubling aspect of Bohr’s view.
In general, the Copenhagen interpretation has something of the flavor of logical pos-
itivism, a philosophy which holds that the purpose of science is to correllate mea-
surements, rather than describe some ”objective” reality. ”Objective reality,” in the
positivist view, is a meaningless concept, and science should be formulated entirely
in terms of quantities which can be measured directly. This view, in the hands of
Mach and others, had considerable influence on physics in the early 20th century, and
certainly the Copenhagen interpretation show traces of this influence.
It is a little distressing to think that science is not about Nature, but only about
correllating measurements. Still, the consistency of the Copenhagen interpretation,
and its ability to evade puzzles connected with the apparent non-local ”collapse” of
entangled wavefunctions, should not be underestimated. To the extent that there is
an ”oﬃcial” interpretation of quantum theory, it would be the Copenhagen view. In classical physics the Euler-Lagrange equations are derived from the condition that
the action S[x(t)] should be stationary. These second order equations are equiva-
lent to the first order Hamilton equations of motion, which are obtained by taking
appropriate derivatives of the Hamiltonian function H[q, p]. Now the Hamiltonian
is a quantity that we have encountered frequently in these pages. But what about
the action? It would be surprising if something so fundamental to classical physics
had no part at all to play in quantum theory. In fact, the action has the central
role in an alternative approach to quantum mechanics known as the ”path-integral
formulation.”
Lets start with the concept of a propagator. Given the wavefunction ψ(x, t) at
time t, the wavefunction at any later time t + T can be expressed as
ψ(x, t + T ) = dy GT (x, y)ψ(y, t) (24.1)
where GT (x, y) is known as the propagator, and of course it depends on the Hamilto-
nian of theory. In fact, given a time-independent Hamiltonian with eigenstates
Hφn(x) = Enφn(x) (24.2)
its easy to see that
GT (x, y) =
φn(x)φ∗
n(y)e−iEnT (24.3)
n
Richard Feynman, in 1948, discovered a very beautiful expression for the propagator
in terms of an integral over all paths that the particle can follow, from point y at
time t, to point x at time t + T , with an integrand
eiS[x(t)]/¯ h (24.4)
As we will see, his expression can be taken as a new way of quantizing a mechanical
system, equivalent to the ”canonical” approach based on exchanging observables for
operators. Having derived the path-integral from the Schrodinger equation, one can of course
go in the other direction, i.e. derive the Schrodinger equation starting from the
concept of an integration over paths. We have seen that path-integrals with gaussian
integrands can be evaluated exactly; integrands with non-gaussian terms can often
be evaluated approximately by a perturbation technique. We have also seen that
path-integrals lead to the usual form of the momentum operator. Logically, the path-
integral approach is an alternative to canonical quantization based on commutators;
either method can be used to quantize a classical theory. Why then, have we spent the whole year following the Schrodinger equation ap-
proach? Why not begin with path-integral, and solve bound-state and scattering
problems by that method? In fact, such an approach to teaching non-relativistic
quantum mechanics can be and has been followed, by Feynman himself. The results
are enshrined in a textbook by Feynman and Hibbs. However, no other elementary
textbook, and not many instructors, have followed Feynman’s example. The reason
is simply that, in non-relativistic quantum mechanics, the path-integral is a rather
cumbersome procedure for solving problems, as compared to the Schrodinger equa-
tion.
In relativistic quantum field theory, however, the situation is diﬀerent: for very
many problems it is the path-integral technique which is easier to use, and better
adapted than operator methods to the thorny technicalities that are encountered. As
a bonus, the path-integral formulation is naturally suited to various non-perturbative
approximation methods, such as the Monte Carlo procedure, in cases where pertur-
bation theory cannot be applied. Finally, there are interesting and deep connections
between quantum field theory, based on the Feynam path integral, and statistical
mechanics, based on the analysis of a partition function. But this is a long story, to
be taught in another, more advanced, course.
Introduction and Review 


that are not visible to the eye, including radio waves, microwaves, 
x-rays, and gamma rays. These are the “colors” of light that do not 
happen to fall within the narrow violet-to-red range of the rainbow 
that we can see. 


self-check B 

At the turn of the 20th century, a strange new phenomenon was discov- 
ered in vacuum tubes: mysterious rays of unknown origin and nature. 
These rays are the same as the ones that shoot from the back of your 
TV’s picture tube and hit the front to make the picture. Physicists in 
1895 didn’t have the faintest idea what the rays were, so they simply 
named them “cathode rays,” after the name for the electrical contact 
from which they sprang. A fierce debate raged, complete with national- 
istic overtones, over whether the rays were a form of light or of matter. 
What would they have had to do in order to settle the issue? > 
Answer, p. 563 


Many physical phenomena are not themselves light or matter, 
but are properties of light or matter or interactions between light 
and matter. For instance, motion is a property of all light and some 
matter, but it is not itself light or matter. The pressure that keeps 
a bicycle tire blown up is an interaction between the air and the 
tire. Pressure is not a form of matter in and of itself. It is as 
much a property of the tire as of the air. Analogously, sisterhood 
and employment are relationships among people but are not people 
themselves. 


Some things that appear weightless actually do have weight, and 
so qualify as matter. Air has weight, and is thus a form of matter 
even though a cubic inch of air weighs less than a grain of sand. A 
helium balloon has weight, but is kept from falling by the force of the 
surrounding more dense air, which pushes up on it. Astronauts in 
orbit around the Earth have weight, and are falling along a curved 
arc, but they are moving so fast that the curved arc of their fall 
is broad enough to carry them all the way around the Earth in a 
circle. They perceive themselves as being weightless because their 
space capsule is falling along with them, and the floor therefore does 
not push up on their feet. 


Optional Topic: Modern Changes in the Definition of Light and 
Matter 

Einstein predicted as a consequence of his theory of relativity that light 
would after all be affected by gravity, although the effect would be ex- 
tremely weak under normal conditions. His prediction was borne out 
by observations of the bending of light rays from stars as they passed 
close to the sun on their way to the Earth. Einstein’s theory also implied 
the existence of black holes, stars so massive and compact that their 
intense gravity would not even allow light to escape. (These days there 
is strong evidence that black holes exist.) 


Einstein’s interpretation was that light doesn’t really have mass, but 
that energy is affected by gravity just like mass is. The energy in a light 


Section 0.2 





c/ This telescope picture shows 
two images of the same distant 
object, an exotic, very luminous 
object called a quasar. This is 
interpreted as evidence that a 
massive, dark object, possibly 
a black hole, happens to be 
between us and it. Light rays that 
would otherwise have missed the 
earth on either side have been 
bent by the dark object’s gravity 
so that they reach us. The actual 
direction to the quasar is presum- 
ably in the center of the image, 
but the light along that central line 
doesn’t get to us because it is 
absorbed by the dark object. The 
quasar is known by its catalog 
number, MG1131+0456, or more 
informally as Einstein’s Ring. 


What is physics? 19 


molecule 


neutrons 
lave eo) melo) ars) 





? 
20 Chapter 0 
d / Reductionism. 


beam is equivalent to a certain amount of mass, given by the famous 
equation E = mc?, where c is the speed of light. Because the speed 
of light is such a big number, a large amount of energy is equivalent to 
only a very small amount of mass, so the gravitational force on a light 
ray can be ignored for most practical purposes. 


There is however a more satisfactory and fundamental distinction 
between light and matter, which should be understandable to you if you 
have had a chemistry course. In chemistry, one learns that electrons 
obey the Pauli exclusion principle, which forbids more than one electron 
from occupying the same orbital if they have the same spin. The Pauli 
exclusion principle is obeyed by the subatomic particles of which matter 
is composed, but disobeyed by the particles, called photons, of which a 
beam of light is made. 


Einstein’s theory of relativity is discussed more fully in book 6 of this 
series. 


The boundary between physics and the other sciences is not 
always clear. For instance, chemists study atoms and molecules, 
which are what matter is built from, and there are some scientists 
who would be equally willing to call themselves physical chemists 
or chemical physicists. It might seem that the distinction between 
physics and biology would be clearer, since physics seems to deal 
with inanimate objects. In fact, almost all physicists would agree 
that the basic laws of physics that apply to molecules in a test tube 
work equally well for the combination of molecules that constitutes 
a bacterium. (Some might believe that something more happens in 
the minds of humans, or even those of cats and dogs.) What differ- 
entiates physics from biology is that many of the scientific theories 
that describe living things, while ultimately resulting from the fun- 
damental laws of physics, cannot be rigorously derived from physical 
principles. 


Isolated systems and reductionism 


To avoid having to study everything at once, scientists isolate the 
things they are trying to study. For instance, a physicist who wants 
to study the motion of a rotating gyroscope would probably prefer 
that it be isolated from vibrations and air currents. Even in biology, 
where field work is indispensable for understanding how living things 
relate to their entire environment, it is interesting to note the vital 
historical role played by Darwin’s study of the Galapagos Islands, 
which were conveniently isolated from the rest of the world. Any 
part of the universe that is considered apart from the rest can be 
called a “system.” 


Physics has had some of its greatest successes by carrying this 
process of isolation to extremes, subdividing the universe into smaller 
and smaller parts. Matter can be divided into atoms, and the be- 
havior of individual atoms can be studied. Atoms can be split apart 


Introduction and Review 


into their constituent neutrons, protons and electrons. Protons and 
neutrons appear to be made out of even smaller particles called 
quarks, and there have even been some claims of experimental ev- 
idence that quarks have smaller parts inside them. This method 
of splitting things into smaller and smaller parts and studying how 
those parts influence each other is called reductionism. The hope is 
that the seemingly complex rules governing the larger units can be 
better understood in terms of simpler rules governing the smaller 
units. To appreciate what reductionism has done for science, it is 
only necessary to examine a 19th-century chemistry textbook. At 
that time, the existence of atoms was still doubted by some, elec- 
trons were not even suspected to exist, and almost nothing was 
understood of what basic rules governed the way atoms interacted 
with each other in chemical reactions. Students had to memorize 
long lists of chemicals and their reactions, and there was no way to 
understand any of it systematically. Today, the student only needs 
to remember a small set of rules about how atoms interact, for in- 
stance that atoms of one element cannot be converted into another 
via chemical reactions, or that atoms from the right side of the pe- 
riodic table tend to form strong bonds with atoms from the left 
side. 


Discussion questions 


A I’ve suggested replacing the ordinary dictionary definition of light 
with a more technical, more precise one that involves weightlessness. It’s 
still possible, though, that the stuff a lightbulb makes, ordinarily called 
“light,” does have some small amount of weight. Suggest an experiment 
to attempt to measure whether it does. 


B_ _Heat is weightless (i.e., an object becomes no heavier when heated), 
and can travel across an empty room from the fireplace to your skin, 
where it influences you by heating you. Should heat therefore be con- 
sidered a form of light by our definition? Why or why not? 


C Similarly, should sound be considered a form of light? 


0.3 How to learn physics 


For as knowledges are now delivered, there is a kind of con- 
tract of error between the deliverer and the receiver; for he 
that delivereth knowledge desireth to deliver it in such a form 
as may be best believed, and not as may be best examined; 
and he that receiveth knowledge desireth rather present sat- 
isfaction than expectant inquiry. 


Francis Bacon 


Many students approach a science course with the idea that they 
can succeed by memorizing the formulas, so that when a problem 


Section 0.3 How to learn physics 


21 


22 


Chapter 0 


is assigned on the homework or an exam, they will be able to plug 
numbers in to the formula and get a numerical result on their cal- 
culator. Wrong! That’s not what learning science is about! There 
is a big difference between memorizing formulas and understanding 
concepts. To start with, different formulas may apply in different 
situations. One equation might represent a definition, which is al- 
ways true. Another might be a very specific equation for the speed 
of an object sliding down an inclined plane, which would not be true 
if the object was a rock drifting down to the bottom of the ocean. 
If you don’t work to understand physics on a conceptual level, you 
won’t know which formulas can be used when. 


Most students taking college science courses for the first time 
also have very little experience with interpreting the meaning of an 
equation. Consider the equation w = A/h relating the width of a 
rectangle to its height and area. A student who has not developed 
skill at interpretation might view this as yet another equation to 
memorize and plug in to when needed. A slightly more savvy stu- 
dent might realize that it is simply the familiar formula A = wh 
in a different form. When asked whether a rectangle would have 
a greater or smaller width than another with the same area but 
a smaller height, the unsophisticated student might be at a loss, 
not having any numbers to plug in on a calculator. The more ex- 
perienced student would know how to reason about an equation 
involving division — if h is smaller, and A stays the same, then w 
must be bigger. Often, students fail to recognize a sequence of equa- 
tions as a derivation leading to a final result, so they think all the 
intermediate steps are equally important formulas that they should 
memorize. 


When learning any subject at all, it is important to become as 
actively involved as possible, rather than trying to read through 
all the information quickly without thinking about it. It is a good 
idea to read and think about the questions posed at the end of each 
section of these notes as you encounter them, so that you know you 
have understood what you were reading. 


Many students’ difficulties in physics boil down mainly to diffi- 
culties with math. Suppose you feel confident that you have enough 
mathematical preparation to succeed in this course, but you are 
having trouble with a few specific things. In some areas, the brief 
review given in this chapter may be sufficient, but in other areas 
it probably will not. Once you identify the areas of math in which 
you are having problems, get help in those areas. Don’t limp along 
through the whole course with a vague feeling of dread about some- 
thing like scientific notation. The problem will not go away if you 
ignore it. The same applies to essential mathematical skills that you 
are learning in this course for the first time, such as vector addition. 


Sometimes students tell me they keep trying to understand a 


Introduction and Review 


certain topic in the book, and it just doesn’t make sense. The worst 
thing you can possibly do in that situation is to keep on staring 
at the same page. Every textbook explains certain things badly — 
even mine! — so the best thing to do in this situation is to look 
at a different book. Instead of college textbooks aimed at the same 
mathematical level as the course you’re taking, you may in some 
cases find that high school books or books at a lower math level 
give clearer explanations. 


Finally, when reviewing for an exam, don’t simply read back 
over the text and your lecture notes. Instead, try to use an active 
method of reviewing, for instance by discussing some of the discus- 
sion questions with another student, or doing homework problems 
you hadn’t done the first time. 


0.4 Self-evaluation 


The introductory part of a book like this is hard to write, because 
every student arrives at this starting point with a different prepara- 
tion. One student may have grown up outside the U.S. and so may 
be completely comfortable with the metric system, but may have 
had an algebra course in which the instructor passed too quickly 
over scientific notation. Another student may have already taken 
calculus, but may have never learned the metric system. The fol- 
lowing self-evaluation is a checklist to help you figure out what you 
need to study to be prepared for the rest of the course. 





If you disagree with this state- | you should study this section: 
ment... 





I am familiar with the basic metric | section 0.5 Basic of the Metric Sys- 
units of meters, kilograms, and sec- | tem 

onds, and the most common metric 
prefixes: milli- (m), kilo- (k), and 





centi- (c). 
I know about the newton, a unit of | section 0.6 The newton, the Metric 
force Unit of Force 





I am familiar with these less com- | section 0.7 Less Common Metric 
mon metric prefixes: mega- (M), | Prefixes 
micro- (4), and nano- (n). 





I am comfortable with scientific no- | section 0.8 Scientific Notation 
tation. 





I can confidently do metric conver- | section 0.9 Conversions 
sions. 





I understand the purpose and use of | section 0.10 Significant Figures 
significant figures. 














It wouldn’t hurt you to skim the sections you think you already 
know about, and to do the self-checks in those sections. 


Section 0.4 Self-evaluation 


0.5 Basics of the metric system 


The metric system 


Every country in the world besides the U.S. uses a system of 
units known in English as the “metric system.?” This system is 
entirely decimal, thanks to the same eminently logical people who 
brought about the French Revolution. In deference to France, the 
system’s official name is the Systeme International, or SI, meaning 
International System. The system uses a single, consistent set of 
Greek and Latin prefixes that modify the basic units. Each prefix 
stands for a power of ten, and has an abbreviation that can be 
combined with the symbol for the unit. For instance, the meter is 
a unit of distance. The prefix kilo- stands for 10°, so a kilometer, 1 
km, is a thousand meters. 


The basic units of the SI are the meter for distance, the second 
for time, and the kilogram (not the gram) for mass. 


The following are the most common metric prefixes. You should 
memorize them. 


prefix meaning example 
kilo k 103 60 kg =a person’s mass 
centi- c 107? 28cm = height of a piece of paper 
milli- m 107? lms = time for one vibration of a guitar 


string playing the note D 


The prefix centi-, meaning 10~?, is only used in the centimeter; 
a hundredth of a gram would not be written as 1 cg but as 10 mg. 
The centi- prefix can be easily remembered because a cent is 10~? 


66) 


dollars. The official SI abbreviation for seconds is “s” (not “sec”) 


66 


and grams are “g” (not “gm”). 


The second 


When I stated briefly above that the second was a unit of time, it 
may not have occurred to you that this was not much of a definition. 
We can make a dictionary-style definition of a term like “time,” or 
give a general description like Isaac Newton’s: “Absolute, true, and 
mathematical time, of itself, and from its own nature, flows equably 
without relation to anything external...” Newton’s characterization 
sounds impressive, but physicists today would consider it useless as 
a definition of time. Today, the physical sciences are based on oper- 
ational definitions, which means definitions that spell out the actual 
steps (operations) required to measure something numerically. 


In an era when our toasters, pens, and coffee pots tell us the 
time, it is far from obvious to most people what is the fundamental 
operational definition of time. Until recently, the hour, minute, and 
second were defined operationally in terms of the time required for 





Liberia and Myanmar have not legally adopted metric units, but use them 
in everyday life. 


Chapter 0 Introduction and Review 


the earth to rotate about its axis. Unfortunately, the Earth’s ro- 
tation is slowing down slightly, and by 1967 this was becoming an 
issue in scientific experiments requiring precise time measurements. 
The second was therefore redefined as the time required for a cer- 
tain number of vibrations of the light waves emitted by a cesium 
atoms in a lamp constructed like a familiar neon sign but with the 
neon replaced by cesium. The new definition not only promises to 
stay constant indefinitely, but for scientists is a more convenient 
way of calibrating a clock than having to carry out astronomical 
measurements. 


self-check C 
What is a possible operational definition of how strong apersonis? p> 
Answer, p. 563 


The meter 


The French originally defined the meter as 10~7 times the dis- 
tance from the equator to the north pole, as measured through Paris 
(of course). Even if the definition was operational, the operation of 
traveling to the north pole and laying a surveying chain behind you 
was not one that most working scientists wanted to carry out. Fairly 
soon, a standard was created in the form of a metal bar with two 
scratches on it. This was replaced by an atomic standard in 1960, 
and finally in 1983 by the current definition, which is that the speed 
of light has a defined value in units of m/s. 


The kilogram 


The third base unit of the SI is the kilogram, a unit of mass. 
Mass is intended to be a measure of the amount of a substance, 
but that is not an operational definition. Bathroom scales work by 
measuring our planet’s gravitational attraction for the object being 
weighed, but using that type of scale to define mass operationally 
would be undesirable because gravity varies in strength from place 
to place on the earth. The kilogram was for a long time defined 
by a physical artifact (figure f), but in 2019 it was redefined by 
giving a defined value to Planck’s constant (p. 970), which plays a 
fundamental role in the description of the atomic world. 


Combinations of metric units 


Just about anything you want to measure can be measured with 
some combination of meters, kilograms, and seconds. Speed can be 
measured in m/s, volume in m°, and density in kg/m®. Part of what 
makes the SI great is this basic simplicity. No more funny units like 
a cord of wood, a bolt of cloth, or a jigger of whiskey. No more 
liquid and dry measure. Just a simple, consistent set of units. The 
SI measures put together from meters, kilograms, and seconds make 
up the mks system. For example, the mks unit of speed is m/s, not 
km/hr. 





e/The original definition of 


the meter. 





CE ae OR 


f/A duplicate of the Paris 
kilogram, maintained at the Dan- 
ish National Metrology Institute. 
As of 2019, the kilogram is no 
longer defined in terms of a 
physical standard. 


Section 0.5 Basics of the metric system 25 


26 


Chapter 0 





Checking units 


A useful technique for finding mistakes in one’s algebra is to 


analyze the units associated with the variables. 


Checking units example 1 
> Jae starts from the formula V = 3Ah for the volume of a cone, 
where A is the area of its base, and h is its height. He wants to 
find an equation that will tell him how tall a conical tent has to be 
in order to have a certain volume, given its radius. His algebra 
goes like this: 





| V=—Ah 
3 
2 A=nr- 
3 V= anr?h 
2 
Tr 
paay 


Is his algebra correct? If not, find the mistake. 


> Line 4 is supposed to be an equation for the height, so the units 
of the expression on the right-hand side had better equal meters. 
The pi and the 3 are unitless, so we can ignore them. In terms of 
units, line 4 becomes 


m2 1 
nee. 
me om 


This is false, so there must be a mistake in the algebra. The units 
of lines 1, 2, and 3 check out, so the mistake must be in the step 
from line 3 to line 4. In fact the result should have been 


3V 
h= mre. 


Now the units check: m = m?/m?. 


Discussion question 


A Isaac Newton wrote, “...the natural days are truly unequal, though 
they are commonly considered as equal, and used for a measure of 
time. . . It may be that there is no such thing as an equable motion, whereby 
time may be accurately measured. All motions may be accelerated or re- 
tarded...” Newton was right. Even the modern definition of the second 
in terms of light emitted by cesium atoms is subject to variation. For in- 
stance, magnetic fields could cause the cesium atoms to emit light with 
a slightly different rate of vibration. What makes us think, though, that a 
pendulum clock is more accurate than a sundial, or that a cesium atom 
is a more accurate timekeeper than a pendulum clock? That is, how can 
one test experimentally how the accuracies of different time standards 
compare? 


Introduction and Review 


0.6 The Newton, the metric unit of force 


A force is a push or a pull, or more generally anything that can 
change an object’s speed or direction of motion. A force is required 
to start a car moving, to slow down a baseball player sliding in to 
home base, or to make an airplane turn. (Forces may fail to change 
an object’s motion if they are canceled by other forces, e.g., the 
force of gravity pulling you down right now is being canceled by the 
force of the chair pushing up on you.) The metric unit of force is 
the Newton, defined as the force which, if applied for one second, 
will cause a 1-kilogram object starting from rest to reach a speed of 
1 m/s. Later chapters will discuss the force concept in more detail. 
In fact, this entire book is about the relationship between force and 
motion. 


In section 0.5, I gave a gravitational definition of mass, but by 
defining a numerical scale of force, we can also turn around and de- 
fine a scale of mass without reference to gravity. For instance, if a 
force of two Newtons is required to accelerate a certain object from 
rest to 1 m/s in 1 s, then that object must have a mass of 2 kg. 
From this point of view, mass characterizes an object’s resistance 
to a change in its motion, which we call inertia or inertial mass. 
Although there is no fundamental reason why an object’s resistance 
to a change in its motion must be related to how strongly gravity 
affects it, careful and precise experiments have shown that the in- 
ertial definition and the gravitational definition of mass are highly 
consistent for a variety of objects. It therefore doesn’t really matter 
for any practical purpose which definition one adopts. 


Discussion question 


A Spending a long time in weightlessness is unhealthy. One of the 
most important negative effects experienced by astronauts is a loss of 
muscle and bone mass. Since an ordinary scale won’t work for an astro- 
naut in orbit, what is a possible way of monitoring this change in mass? 
(Measuring the astronaut’s waist or biceps with a measuring tape is not 
good enough, because it doesn't tell anything about bone mass, or about 
the replacement of muscle with fat.) 


0.7 Less common metric prefixes 


The following are three metric prefixes which, while less common 
than the ones discussed previously, are well worth memorizing. 


prefix meaning example 
mega- M_ 10° 6.4Mm = radius of the earth 
micro- yp 107° 10 pm = size of a white blood cell 
nano- n_ 107% 0.154 nm = distance between carbon 


nuclei in an ethane molecule 


Note that the abbreviation for micro is the Greek letter mu, ju 
— a common mistake is to confuse it with m (milli) or M (mega). 


pee little 


10 -9 nano — nuns 





10 6 micro <—[ mix 


10-3 oni No 


10 3 kilo 


10 6 mega «| MUQS. 


g/This is a mnemonic. to 
help you remember the most im- 
portant metric prefixes. The word 
“little” is to remind you that the 
list starts with the prefixes used 
for small quantities and builds 
upward. The exponent changes 
by 3, except that of course that 
we do not need a special prefix 
for 10°, which equals one. 


Section 0.6 The Newton, the metric unit of force 27 


28 


Chapter 0 


There are other prefixes even less common, used for extremely 
large and small quantities. For instance, 1 femtometer = 107! m is 
a convenient unit of distance in nuclear physics, and 1 gigabyte = 
10° bytes is used for computers’ hard disks. The international com- 
mittee that makes decisions about the SI has recently even added 
some new prefixes that sound like jokes, e.g., 1 yoctogram = 10724 g 
is about half the mass of a proton. In the immediate future, how- 
ever, you’re unlikely to see prefixes like “yocto-” and “zepto-” used 
except perhaps in trivia contests at science-fiction conventions or 
other geekfests. 


self-check D 

Suppose you could slow down time so that according to your perception, 
a beam of light would move across a room at the speed of a slow walk. 
If you perceived a nanosecond as if it was a second, how would you 
perceive a microsecond? > Answer, p. 564 


d 


0.8 Scientific notation 


Most of the interesting phenomena in our universe are not on the 
human scale. It would take about 1,000,000,000,000,000,000,000 
bacteria to equal the mass of a human body. When the physicist 
Thomas Young discovered that light was a wave, it was back in the 
bad old days before scientific notation, and he was obliged to write 
that the time required for one vibration of the wave was 1/500 of 
a millionth of a millionth of a second. Scientific notation is a less 
awkward way to write very large and very small numbers such as 
these. Here’s a quick review. 


Scientific notation means writing a number in terms of a product 
of something from 1 to 10 and something else that is a power of ten. 
For instance, 

32 = 32% 10! 
320 = 3.2 x 10? 
3200 = 3.2 x 10° 


Each number is ten times bigger than the previous one. 


Since 10! is ten times smaller than 10? , it makes sense to use 
the notation 10° to stand for one, the number that is in turn ten 
times smaller than 10! . Continuing on, we can write 10! to stand 
for 0.1, the number ten times smaller than 10° . Negative exponents 
are used for small numbers: 


3.2 = 3.2 x 10° 
0.32 = 3.2 x 107! 
0.032 = 3.2 x 10~ 


Introduction and Review 


A common source of confusion is the notation used on the dis- 
plays of many calculators. Examples: 


3.2 x 10° (written notation) 
3.2E+6 (notation on some calculators) 
32° (notation on some other calculators) 


The last example is particularly unfortunate, because 3.2° really 

stands for the number 3.2 x 3.2 « 3.2 x 3.2 x 3.2 x 3.2 = 1074, a 

totally different number from 3.2 x 10° = 3200000. The calculator 

notation should never be used in writing. It’s just a way for the 

manufacturer to save money by making a simpler display. 
self-check E 


A student learns that 104 bacteria, standing in line to register for classes 
at Paramecium Community College, would form a queue of this size: 


ee 
The student concludes that 10? bacteria would form a line of this length: 


Why is the student incorrect? > Answer, p. 564 


0.9 Conversions 


Conversions are one of the three essential mathematical skills, sum- 
marized on pp.545-546, that you need for success in this course. 


I suggest you avoid memorizing lots of conversion factors be- 
tween SI units and U.S. units, but two that do come in handy are: 


1 inch = 2.54 cm 


An object with a weight on Earth of 2.2 pounds-force has a 
mass of 1 kg. 


The first one is the present definition of the inch, so it’s exact. The 
second one is not exact, but is good enough for most purposes. (U.S. 
units of force and mass are confusing, so it’s a good thing they’re 
not used in science. In U.S. units, the unit of force is the pound- 
force, and the best unit to use for mass is the slug, which is about 
14.6 kg.) 


More important than memorizing conversion factors is under- 
standing the right method for doing conversions. Even within the 
SI, you may need to convert, say, from grams to kilograms. Differ- 
ent people have different ways of thinking about conversions, but 
the method I’ll describe here is systematic and easy to understand. 
The idea is that if 1 kg and 1000 g represent the same mass, then 


Section 0.9 Conversions 


29 


30 


Chapter 0 


we can consider a fraction like 
10° g 
1 kg 





to be a way of expressing the number one. This may bother you. For 
instance, if you type 1000/1 into your calculator, you will get 1000, 
not one. Again, different people have different ways of thinking 
about it, but the justification is that it helps us to do conversions, 
and it works! Now if we want to convert 0.7 kg to units of grams, 
we can multiply kg by the number one: 





10° ¢ 
0.7 kg x 
. 1 kg 
If yow’re willing to treat symbols such as “kg” as if they were vari- 
ables as used in algebra (which they’re really not), you can then 
cancel the kg on top with the kg on the bottom, resulting in 
LO? 


0.7 ke x BY aul 





To convert grams to kilograms, you would simply flip the fraction 
upside down. 


One advantage of this method is that it can easily be applied to 
a series of conversions. For instance, to convert one year to units of 
seconds, 


,, 365 days | 24 hours | 60 min 60s 
1 year 1 day lheur  1lmin — 
= 3.15 « 10" 6. 





Should that exponent be positive, or negative? 


A common mistake is to write the conversion fraction incorrectly. 
For instance the fraction 


10° ke 
eS 





(incorrect) 


does not equal one, because 10? kg is the mass of a car, and 1 g is 
the mass of a raisin. One correct way of setting up the conversion 
factor would be 
1073 kg 
lg 
You can usually detect such a mistake if you take the time to check 
your answer and see if it is reasonable. 


(correct). 


If common sense doesn’t rule out either a positive or a negative 
exponent, here’s another way to make sure you get it right. There 
are big prefixes and small prefixes: 


Introduction and Review 


big prefixes: k M 
small prefixes: m wp n 


(It’s not hard to keep straight which are which, since “mega” and 
“micro” are evocative, and it’s easy to remember that a kilometer 
is bigger than a meter and a millimeter is smaller.) In the example 
above, we want the top of the fraction to be the same as the bottom. 
Since & is a big prefix, we need to compensate by putting a small 
number like 107? in front of it, not a big number like 10%. 


> Solved problem: a simple conversion page 36, problem 6 


> Solved problem: the geometric mean page 37, problem 8 
Discussion question 


A Each of the following conversions contains an error. In each case, 
explain what the error is. 


(a) 1000 kg x agg = 19 








b) 50m x $% =0.5cm 


(b) 
(c) “Nano” is 10-9, so there are 10-9 nm in a meter. 
(d) “Micro” is 10-®, so 1 kg is 10® ug. 


0.10 Significant figures 


The international governing body for football (“soccer” in the US) 
says the ball should have a circumference of 68 to 70 cm. Taking the 
middle of this range and dividing by a gives a diameter of approx- 
imately 21.96338214668155633610595934540698196 cm. The digits 
after the first few are completely meaningless. Since the circumfer- 
ence could have varied by about a centimeter in either direction, the 
diameter is fuzzy by something like a third of a centimeter. We say 
that the additional, random digits are not significant figures. If you 
write down a number with a lot of gratuitous insignificant figures, 
it shows a lack of scientific literacy and imples to other people a 
greater precision than you really have. 


As a rule of thumb, the result of a calculation has as many 
significant figures, or “sig figs,” as the least accurate piece of data 
that went in. In the example with the soccer ball, it didn’t do us any 
good to know z to dozens of digits, because the bottleneck in the 
precision of the result was the figure for the circumference, which 
was two sig figs. The result is 22 cm. The rule of thumb works best 
for multiplication and division. 


For calculations involving multiplication and division, a given 
fractional or “percent” error in one of the inputs causes the same 
fractional error in the output. The number of digits in a number 


Section 0.10 


Significant figures 


31 


32 


Chapter 0 


provides a rough measure of its possible fractional error. These are 
called significant figures or “sig figs.” Examples: 























3.14 3 sig figs 

3.1 2 sig figs 

0.03 1 sig fig, because the zeroes are just placeholders 

3.0 x 10! | 2 sig figs 

30 could be 1 or 2 sig figs, since we can’t tell if the 
0 is a placeholder or a real sig fig 








In such calculations, your result should not have more than the 
number of sig figs in the least accurate piece of data you started 
with. 


Sig figs in the area of a triangle example 2 
> A triangle has an area of 6.45 m? and a base with a width of 
4.0138 m. Find its height. 


> The area is related to the base and height by A = bh/2. 
_2A 


b 
= 3.21391200358762 m_ (calculator output) 


=3.21m 


h 


The given data were 3 sig figs and 5 sig figs. We're limited by the 
less accurate piece of data, so the final result is 3 sig figs. The 
additional digits on the calculator don’t mean anything, and if we 
communicated them to another person, we would create the false 
impression of having determined h with more precision than we 
really obtained. 


self-check F 
The following quote is taken from an editorial by Norimitsu Onishi in the 
New York Times, August 18, 2002. 


Consider Nigeria. Everyone agrees it is Africa’s most populous 
nation. But what is its population? The United Nations says 
114 million; the State Department, 120 million. The World Bank 
says 126.9 million, while the Central Intelligence Agency puts it 
at 126,635,626. 


What should bother you about this? > Answer, p. 564 


Dealing correctly with significant figures can save you time! Of- 
ten, students copy down numbers from their calculators with eight 
significant figures of precision, then type them back in for a later 
calculation. That’s a waste of time, unless your original data had 
that kind of incredible precision. 

self-check G 


How many significant figures are there in each of the following mea- 
surements? 


Introduction and Review 


(1) 9.937 m 
(2) 4.05 
(3) 0.0000000000000037 kg > Answer, p. 564 


The rules about significant figures are only rules of thumb, and 
are not a substitute for careful thinking. For instance, $20.00 + 
$0.05 is $20.05. It need not and should not be rounded off to $20. 
In general, the sig fig rules work best for multiplication and division, 
and we sometimes also apply them when doing a complicated calcu- 
lation that involves many types of operations. For simple addition 
and subtraction, it makes more sense to maintain a fixed number of 
digits after the decimal point. 


When in doubt, don’t use the sig fig rules at all. Instead, in- 
tentionally change one piece of your initial data by the maximum 
amount by which you think it could have been off, and recalculate 
the final result. The digits on the end that are completely reshuffled 
are the ones that are meaningless, and should be omitted. 


A nonlinear function example 3 
> How many sig figs are there in sin 88.7°? 


> We're using a sine function, which isn’t addition, subtraction, 
multiplication, or division. It would be reasonable to guess that 
since the input angle had 3 sig figs, so would the output. But if 
this was an important calculation and we really needed to know, 
we would do the following: 


sin 88.7° = 0.999742609322698 
sin 88.8° = 0.999780683474846 


Surprisingly, the result appears to have as many as 5 sig figs, not 
just 3: 
sin 88.7° = 0.99974, 


where the final 4 is uncertain but may have some significance. 
The unexpectedly high precision of the result is because the sine 
function is nearing its maximum at 90 degrees, where the graph 
flattens out and becomes insensitive to the input angle. 


0.11 A note about diagrams 


A quick note about diagrams. Often when you solve a problem, 
the best way to get started and organize your thoughts is by draw- 
ing a diagram. For an artist, it’s desirable to be able to draw a 
recognizable, realistic, perspective picture of a tomato, like the one 
at the top of figure h. But in science and engineering, we usually 
don’t draw solid figures in perspective, because that would make it 
difficult to label distances and angles. Usually we want views or 
cross-sections that project the object into its planes of symmetry, 
as in the line drawings in the figure. 





horizontal 
cross-section 


h/A_ diagram of 


Section 0.11 A note about diagrams 


vertical 


cross-section 


a 


tomato. 


33 


Summary 


Selected vocabulary 


matter 


defi- 


operational 
nition 


Systeme Interna- 
tional 


Anything that is affected by gravity. 
Anything that can travel from one place to an- 
other through empty space and can influence 
matter, but is not affected by gravity. 

A definition that states what operations 
should be carried out to measure the thing be- 
ing defined. 

A fancy name for the metric system. 


The use of metric units based on the meter, 


kilogram, and second. Example: meters per 
second is the mks unit of speed, not cm/s or 
km/hr. 

A numerical measure of how difficult it is to 
change an object’s motion. 

Digits that contribute to the accuracy of a 


significant figures 


measurement. 
Notation 
1 ee meter, the metric distance unit 
KO fe BBs ot kilogram, the metric unit of mass 
Sebati aoe 2 second, the metric unit of time 
Me ie ba ee the metric prefix mega-, 10° 
Kes os we et eas the metric prefix kilo-, 10° 
Ti? vies eis ae the metric prefix milli-, 107° 
fis sn ho eA: the metric prefix micro-, 10~° 
TS iS speek ee the metric prefix nano-, 1079 
Summary 


Physics is the use of the scientific method to study the behavior 
of light and matter. The scientific method requires a cycle of the- 
ory and experiment, theories with both predictive and explanatory 
value, and reproducible experiments. 


The metric system is a simple, consistent framework for measure- 
ment built out of the meter, the kilogram, and the second plus a set 
of prefixes denoting powers of ten. The most systematic method for 
doing conversions is shown in the following example: 

-3 

= = 0.378 

ms 





10 
370 ms x 


Mass is a measure of the amount of a substance. Mass can be 
defined gravitationally, by comparing an object to a standard mass 
on a double-pan balance, or in terms of inertia, by comparing the 
effect of a force on an object to the effect of the same force on a 
standard mass. The two definitions are found experimentally to 
be proportional to each other to a high degree of precision, so we 


Introduction and Review 


usually refer simply to “mass,” without bothering to specify which 
type. 


A force is that which can change the motion of an object. The 
metric unit of force is the Newton, defined as the force required to 
accelerate a standard 1-kg mass from rest to a speed of 1 m/s in 1 
S. 


Scientific notation means, for example, writing 3.2 x 10° rather 
than 320000. 


Writing numbers with the correct number of significant figures 
correctly communicates how accurate they are. As a rule of thumb, 
the final result of a calculation is no more accurate than, and should 
have no more significant figures than, the least accurate piece of 
data. 


Summary 


36 


Chapter 0 


Problems 
Key 


Vv A computerized answer check is available online. 
Jf A problem that requires calculus. 
x A difficult problem. 


1 Correct use of a calculator: (a) Calculate SS SEI on a cal- 
culator. [Self-check: The most common mistake results in 97555.40.] 
Vv 


(b) Which would be more like the price of a TV, and which would 
be more like the price of a house, $3.5 x 10° or $3.5°? 


2 Compute the following things. If they don’t make sense be- 
cause of units, say so. 

(a) 3cm + 5 cm 

(b) 1.11 m + 22 cm 

(c) 120 miles + 2.0 hours 

(d) 120 miles / 2.0 hours 


3 Your backyard has brick walls on both ends. You measure a 
distance of 23.4 m from the inside of one wall to the inside of the 
other. Each wall is 29.4 cm thick. How far is it from the outside 
of one wall to the outside of the other? Pay attention to significant 
figures. 


4 The speed of light is 3.0 x 10° m/s. Convert this to furlongs 
per fortnight. A furlong is 220 yards, and a fortnight is 14 days. An 
inch is 2.54 cm. Vv 


5 Express each of the following quantities in micrograms: 
(a) 10 mg, (b) 10* g, (c) 10 kg, (d) 100 x 10° g, (e) 1000 ng. v 


6 Convert 134 mg to units of kg, writing your answer in scientific 
notation. > Solution, p. 547 


7 In the last century, the average age of the onset of puberty for 
girls has decreased by several years. Urban folklore has it that this 
is because of hormones fed to beef cattle, but it is more likely to be 
because modern girls have more body fat on the average and pos- 
sibly because of estrogen-mimicking chemicals in the environment 
from the breakdown of pesticides. A hamburger from a hormone- 
implanted steer has about 0.2 ng of estrogen (about double the 
amount of natural beef). A serving of peas contains about 300 
ng of estrogen. An adult woman produces about 0.5 mg of estrogen 
per day (note the different unit!). (a) How many hamburgers would 
a girl have to eat in one day to consume as much estrogen as an 
adult woman’s daily production? (b) How many servings of peas? 


Introduction and Review 


8 The usual definition of the mean (average) of two numbers a 
and b is (a+6)/2. This is called the arithmetic mean. The geometric 
mean, however, is defined as (ab)!/? (i.e., the square root of ab). For 
the sake of definiteness, let’s say both numbers have units of mass. 
(a) Compute the arithmetic mean of two numbers that have units 
of grams. Then convert the numbers to units of kilograms and 
recompute their mean. Is the answer consistent? (b) Do the same 
for the geometric mean. (c) If a and b both have units of grams, 
what should we call the units of ab? Does your answer make sense 
when you take the square root? (d) Suppose someone proposes to 
you a third kind of mean, called the superduper mean, defined as 
(ab)'/3. Is this reasonable? > Solution, p. 547 


9 In an article on the SARS epidemic, the May 7, 2003 New 
York Times discusses conflicting estimates of the disease’s incuba- 
tion period (the average time that elapses from infection to the first 
symptoms). “The study estimated it to be 6.4 days. But other sta- 
tistical calculations ... showed that the incubation period could be 
as long as 14.22 days.” What’s wrong here? 


10 The photo shows the corner of a bag of pretzels. What’s 
wrong here? 


11 The distance to the horizon is given by the expression V2rh, 

where r is the radius of the Earth, and h is the observer’s height 

above the Earth’s surface. (This can be proved using the Pythagorean 
theorem.) Show that the units of this expression make sense. Don’t 

try to prove the result, just check its units. (See example 1 on p. 

26 for an example of how to do this.) 


12 (a) Based on the definitions of the sine, cosine, and tangent, 
what units must they have? (b) A cute formula from trigonometry 
lets you find any angle of a triangle if you know the lengths of 
its sides. Using the notation shown in the figure, and letting s = 
(a+b+c)/2 be half the perimeter, we have 


—b\(s— 
tan A/2 = (s — b)(s =o) 
s(s — a) 
Show that the units of this equation make sense. In other words, 
check that the units of the right-hand side are the same as your 
answer to part a of the question. > Solution, p. 547 


13 A 2002 paper by Steegmann et al. uses data from modern 
human groups like the Inuit to argue that Neanderthals in Ice Age 
Europe had to eat up “to 4,480 kcal per day to support strenuous 
winter foraging and cold resistance costs.” What’s wrong here? 


NET WT. 3 1/2 oz. (99.2 g) 


Problem 10. 


Problem 12. 


Problems 





37 


38 


Exercise 0: Models and idealization 
Equipment: 

coffee filters 

ramps (one per group) 

balls of various sizes 

sticky tape 

vacuum pump and “guinea and feather” apparatus (one) 


The motion of falling objects has been recognized since ancient times as an important piece of 
physics, but the motion is inconveniently fast, so in our everyday experience it can be hard to 
tell exactly what objects are doing when they fall. In this exercise you will use several techniques 
to get around this problem and study the motion. Your goal is to construct a scientific model of 
falling. A model means an explanation that makes testable predictions. Often models contain 
simplifications or idealizations that make them easier to work with, even though they are not 
strictly realistic. 


1. One method of making falling easier to observe is to use objects like feathers that we know 
from everyday experience will not fall as fast. You will use coffee filters, in stacks of various 
sizes, to test the following two hypotheses and see which one is true, or whether neither is true: 


Hypothesis 1A: When an object is dropped, it rapidly speeds up to a certain natural falling 
speed, and then continues to fall at that speed. The falling speed is proportional to the object’s 
weight. (A proportionality is not just a statement that if one thing gets bigger, the other does 
too. It says that if one becomes three times bigger, the other also gets three times bigger, etc.) 


Hypothesis 1B: Different objects fall the same way, regardless of weight. 
Test these hypotheses and discuss your results with your instructor. 


2. A second way to slow down the action is to let a ball roll down a ramp. The steeper the 
ramp, the closer to free fall. Based on your experience in part 1, write a hypothesis about what 
will happen when you race a heavier ball against a lighter ball down the same ramp, starting 
them both from rest. 


Hypothesis: 
Show your hypothesis to your instructor, and then test it. 


You have probably found that falling was more complicated than you thought! Is there more 
than one factor that affects the motion of a falling object? Can you imagine certain idealized 
situations that are simpler? Try to agree verbally with your group on an informal model of 
falling that can make predictions about the experiments described in parts 3 and 4. 


3. You have three balls: a standard “comparison ball” of medium weight, a light ball, and a 
heavy ball. Suppose you stand on a chair and (a) drop the light ball side by side with the 
comparison ball, then (b) drop the heavy ball side by side with the comparison ball, then (c) 
join the light and heavy balls together with sticky tape and drop them side by side with the 
comparison ball. 


Use your model to make a prediction:__________________________ 


Test your prediction. 


Chapter 0 Introduction and Review 


4. Your instructor will pump nearly all the air out of a chamber containing a feather and a 
heavier object, then let them fall side by side in the chamber. 


Use your model to make a prediction: 


Exercise 0: Models and idealization 


39 


40 


Chapter 0 


Introduction and Review 





Chapter 1 
Scaling and Estimation 


1.1 Introduction 


Why can’t an insect be the size of a dog? Some skinny stretched- 
out cells in your spinal cord are a meter tall — why does nature 
display no single cells that are not just a meter tall, but a meter 
wide, and a meter thick as well? Believe it or not, these are questions 
that can be answered fairly easily without knowing much more about 
physics than you already do. The only mathematical technique you 
really need is the humble conversion, applied to area and volume. 


Area and volume 


Area can be defined by saying that we can copy the shape of 
interest onto graph paper with 1 cm x 1 cm squares and count the 
number of squares inside. Fractions of squares can be estimated by 
eye. We then say the area equals the number of squares, in units of 
square cm. Although this might seem less “pure” than computing 
areas using formulae like A = rr? for a circle or A = wh/2 for a 
triangle, those formulae are not useful as definitions of area because 
they cannot be applied to irregularly shaped areas. 


2 


Units of square cm are more commonly written as cm“ in science. 


Of course, the unit of measurement symbolized by “cm” is not an 


Life would be very different if you 
were the size of an insect. 





a/Amoebas this size are 
seldom encountered. 


41 


b / Visualizing 


conversions 


of 


area and volume using traditional 


U.S. units. 


42 


Chapter 1 


algebra symbol standing for a number that can be literally multiplied 
by itself. But it is advantageous to write the units of area that way 
and treat the units as if they were algebra symbols. For instance, 
if you have a rectangle with an area of 6m? and a width of 2 m, 
then calculating its length as (6 m?)/(2 m) = 3 m gives a result 
that makes sense both numerically and in terms of units. This 
algebra-style treatment of the units also ensures that our methods 
of converting units work out correctly. For instance, if we accept 


the fraction 
100 cm 


1m 





as a valid way of writing the number one, then one times one equals 
one, so we should also say that one can be represented by 


100 cm ‘i 100 cm 
1m 1m 





’ 


which is the same as 
10000 cm? 


1 m2 
That means the conversion factor from square meters to square cen- 
timeters is a factor of 10+, i.e., a square meter has 104 square cen- 
timeters in it. 


All of the above can be easily applied to volume as well, using 
one-cubic-centimeter blocks instead of squares on graph paper. 


To many people, it seems hard to believe that a square meter 
equals 10000 square centimeters, or that a cubic meter equals a 
million cubic centimeters — they think it would make more sense if 
there were 100 cm? in 1 m?, and 100 cm? in 1 m?, but that would be 
incorrect. The examples shown in figure b aim to make the correct 
answer more believable, using the traditional U.S. units of feet and 
yards. (One foot is 12 inches, and one yard is three feet.) 


=a a Ah YO 
13 


1ft2 lyd2=9ft 2 
1yd3=27ft 3 


self-check A 

Based on figure b, convince yourself that there are 9 ft? ina square yard, 
and 27 ft? in a cubic yard, then demonstrate the same thing symbolically 
(i.e., with the method using fractions that equal one). > Answer, p. 
564 


Scaling and Estimation 


> Solved problem: converting mm? to cm? page 59, problem 10 


> Solved problem: scaling a liter page 60, problem 19 


Discussion question 


A How many square centimeters are there in a square inch? (1 inch = 
2.54 cm) First find an approximate answer by making a drawing, then de- 
rive the conversion factor more accurately using the symbolic method. 


c / Galileo Galilei (1564-1642) was a Renaissance Italian who brought the 
scientific method to bear on physics, creating the modern version of the 
science. Coming from a noble but very poor family, Galileo had to drop 
out of medical school at the University of Pisa when he ran out of money. 
Eventually becoming a lecturer in mathematics at the same school, he 
began a career as a notorious troublemaker by writing a burlesque ridi- 
culing the university’s regulations — he was forced to resign, but found a 
new teaching position at Padua. He invented the pendulum clock, inves- 
tigated the motion of falling bodies, and discovered the moons of Jupiter. 
The thrust of his life’s work was to discredit Aristotle’s physics by con- 
fronting it with contradictory experiments, a program that paved the way 
for Newton’s discovery of the relationship between force and motion. In 
chapter 3 we’ll come to the story of Galileo’s ultimate fate at the hands of 
the Church. 





1.2 Scaling of area and volume 


Great fleas have lesser fleas 
Upon their backs to bite ’em. 
And lesser fleas have lesser still, 
And so ad infinitum. 


Jonathan Swift 


Now how do these conversions of area and volume relate to the 
questions I posed about sizes of living things? Well, imagine that 
you are shrunk like Alice in Wonderland to the size of an insect. 
One way of thinking about the change of scale is that what used 
to look like a centimeter now looks like perhaps a meter to you, 
because you’re so much smaller. If area and volume scaled according 
to most people’s intuitive, incorrect expectations, with 1 m? being 
the same as 100 cm?, then there would be no particular reason 
why nature should behave any differently on your new, reduced 
scale. But nature does behave differently now that you’re small. 
For instance, you will find that you can walk on water, and jump 
to many times your own height. The physicist Galileo Galilei had 
the basic insight that the scaling of area and volume determines 
how natural phenomena behave differently on different scales. He 
first reasoned about mechanical structures, but later extended his 
insights to living things, taking the then-radical point of view that at 
the fundamental level, a living organism should follow the same laws 


Section 1.2 Scaling of area and volume 43 


et 


d/The small boat holds up 
just fine. 


e/A larger boat built with 
the same proportions as_ the 
small one will collapse under its 
own weight. 


f/A boat this large needs to 
have timbers that are thicker 
compared to its size. 


44 Chapter 1 


of nature as a machine. We will follow his lead by first discussing 
machines and then living things. 


Galileo on the behavior of nature on large and small scales 


One of the world’s most famous pieces of scientific writing is 
Galileo’s Dialogues Concerning the Two New Sciences. Galileo was 
an entertaining writer who wanted to explain things clearly to laypeo- 
ple, and he livened up his work by casting it in the form of a dialogue 
among three people. Salviati is really Galileo’s alter ego. Simplicio 
is the stupid character, and one of the reasons Galileo got in trouble 
with the Church was that there were rumors that Simplicio repre- 
sented the Pope. Sagredo is the earnest and intelligent student, with 
whom the reader is supposed to identify. (The following excerpts 
are from the 1914 translation by Crew and de Salvio.) 


SAGREDO: Yes, that is what | mean; and | refer especially to 
his last assertion which | have always regarded as false... ; 
namely, that in speaking of these and other similar machines 
one cannot argue from the small to the large, because many 
devices which succeed on a small scale do not work on a 
large scale. Now, since mechanics has its foundations in ge- 
ometry, where mere size [ is unimportant], | do not see that 
the properties of circles, triangles, cylinders, cones and other 
solid figures will change with their size. If, therefore, a large 
machine be constructed in such a way that its parts bear to 
one another the same ratio as in a smaller one, and if the 
smaller is sufficiently strong for the purpose for which it is 
designed, | do not see why the larger should not be able to 
withstand any severe and destructive tests to which it may be 
subjected. 


Salviati contradicts Sagredo: 


SALVIATI: ...Please observe, gentlemen, how facts which 
at first seem improbable will, even on scant explanation, drop 
the cloak which has hidden them and stand forth in naked and 
simple beauty. Who does not know that a horse falling from a 
height of three or four cubits will break his bones, while a dog 
falling from the same height or a cat from a height of eight 
or ten cubits will suffer no injury? Equally harmless would be 
the fall of a grasshopper from a tower or the fall of an ant from 
the distance of the moon. 


The point Galileo is making here is that small things are sturdier 
in proportion to their size. There are a lot of objections that could be 
raised, however. After all, what does it really mean for something to 
be “strong”, to be “strong in proportion to its size,” or to be strong 
“out of proportion to its size?” Galileo hasn’t given operational 
definitions of things like “strength,” i.e., definitions that spell out 
how to measure them numerically. 


Scaling and Estimation 


Also, a cat is shaped differently from a horse — an enlarged 
photograph of a cat would not be mistaken for a horse, even if the 
photo-doctoring experts at the National Inquirer made it look like a 
person was riding on its back. A grasshopper is not even a mammal, 
and it has an exoskeleton instead of an internal skeleton. The whole 
argument would be a lot more convincing if we could do some iso- 
lation of variables, a scientific term that means to change only one 
thing at a time, isolating it from the other variables that might have 
an effect. If size is the variable whose effect we’re interested in see- 
ing, then we don’t really want to compare things that are different 
in size but also different in other ways. 


SALVIATI: — ...we asked the reason why [shipbuilders] em- 
ployed stocks, scaffolding, and bracing of larger dimensions 
for launching a big vessel than they do for a small one; and 
[an old man] answered that they did this in order to avoid the 
danger of the ship parting under its own heavy weight, a dan- 
ger to which small boats are not subject? 


After this entertaining but not scientifically rigorous beginning, 
Galileo starts to do something worthwhile by modern standards. 
He simplifies everything by considering the strength of a wooden 
plank. The variables involved can then be narrowed down to the 
type of wood, the width, the thickness, and the length. He also 
gives an operational definition of what it means for the plank to 
have a certain strength “in proportion to its size,” by introducing 
the concept of a plank that is the longest one that would not snap 
under its own weight if supported at one end. If you increased 
its length by the slightest amount, without increasing its width or 
thickness, it would break. He says that if one plank is the same 
shape as another but a different size, appearing like a reduced or 
enlarged photograph of the other, then the planks would be strong 
“in proportion to their sizes” if both were just barely able to support 
their own weight. 










Dae (aS 
eircom ost 








g/Galileo discusses planks 
made of wood, but the concept 
may be easier to imagine with 
clay. All three clay rods in the 
figure were originally the same 
shape. The medium-sized one 
was twice the height, twice the 
length, and twice the width of 
the small one, and similarly the 
large one was twice as big as 
the medium one in all its linear 
dimensions. The big one has 
four times the linear dimensions 
of the small one, 16 times the 
cross-sectional area when cut 
perpendicular to the page, and 
64 times the volume. That means 
that the big one has 64 times the 
weight to support, but only 16 
times the strength compared to 
the smallest one. 


h/1. This plank is as long as it 
can be without collapsing under 
its own weight. If it was a hun- 
dredth of an inch longer, it would 
collapse. 2. This plank is made 
out of the same kind of wood. It is 
twice as thick, twice as long, and 
twice as wide. It will collapse un- 
der its own weight. 


Section 1.2 Scaling of area and volume 45 


Also, Galileo is doing something that would be frowned on in 
modern science: he is mixing experiments whose results he has ac- 
tually observed (building boats of different sizes), with experiments 
that he could not possibly have done (dropping an ant from the 
height of the moon). He now relates how he has done actual ex- 
periments with such planks, and found that, according to this op- 
erational definition, they are not strong in proportion to their sizes. 
The larger one breaks. He makes sure to tell the reader how impor- 
tant the result is, via Sagredo’s astonished response: 


SAGREDO: My brain already reels. My mind, like a cloud 
momentarily illuminated by a lightning flash, is for an instant 
filled with an unusual light, which now beckons to me and 
which now suddenly mingles and obscures strange, crude 
ideas. From what you have said it appears to me impossible 
to build two similar structures of the same material, but of 
different sizes and have them proportionately strong. 


In other words, this specific experiment, using things like wooden 
planks that have no intrinsic scientific interest, has very wide impli- 
cations because it points out a general principle, that nature acts 
differently on different scales. 


To finish the discussion, Galileo gives an explanation. He says 
that the strength of a plank (defined as, say, the weight of the heav- 
iest boulder you could put on the end without breaking it) is pro- 
portional to its cross-sectional area, that is, the surface area of the 
fresh wood that would be exposed if you sawed through it in the 
middle. Its weight, however, is proportional to its volume.! 


How do the volume and cross-sectional area of the longer plank 
compare with those of the shorter plank? We have already seen, 
while discussing conversions of the units of area and volume, that 
these quantities don’t act the way most people naively expect. You 
might think that the volume and area of the longer plank would both 
be doubled compared to the shorter plank, so they would increase 
in proportion to each other, and the longer plank would be equally 
able to support its weight. You would be wrong, but Galileo knows 
that this is a common misconception, so he has Salviati address the 
point specifically: 


SALVIATI: — ... Take, for example, a cube two inches on a 
side so that each face has an area of four square inches 
and the total area, i.e., the sum of the six faces, amounts 
to twenty-four square inches; now imagine this cube to be 
sawed through three times [with cuts in three perpendicular 
planes] so as to divide it into eight smaller cubes, each one 
inch on the side, each face one inch square, and the total 





‘Galileo makes a slightly more complicated argument, taking into account 
the effect of leverage (torque). The result I’m referring to comes out the same 
regardless of this effect. 


Scaling and Estimation 


surface of each cube six square inches instead of twenty- 
four in the case of the larger cube. It is evident therefore, 
that the surface of the little cube is only one-fourth that of 
the larger, namely, the ratio of six to twenty-four; but the vol- 
ume of the solid cube itself is only one-eighth; the volume, 
and hence also the weight, diminishes therefore much more 
rapidly than the surface... You see, therefore, Simplicio, that 
| was not mistaken when ...1 said that the surface of a small 
solid is comparatively greater than that of a large one. 


The same reasoning applies to the planks. Even though they 
are not cubes, the large one could be sawed into eight small ones, 
each with half the length, half the thickness, and half the width. 
The small plank, therefore, has more surface area in proportion to 
its weight, and is therefore able to support its own weight while the 
large one breaks. 


Scaling of area and volume for irregularly shaped objects 


You probably are not going to believe Galileo’s claim that this 
has deep implications for all of nature unless you can be convinced 
that the same is true for any shape. Every drawing you’ve seen so 
far has been of squares, rectangles, and rectangular solids. Clearly 
the reasoning about sawing things up into smaller pieces would not 
prove anything about, say, an egg, which cannot be cut up into eight 
smaller egg-shaped objects with half the length. 


Is it always true that something half the size has one quarter 
the surface area and one eighth the volume, even if it has an irreg- 
ular shape? Take the example of a child’s violin. Violins are made 
for small children in smaller size to accomodate their small bodies. 
Figure i shows a full-size violin, along with two violins made with 
half and 3/4 of the normal length.? Let’s study the surface area of 
the front panels of the three violins. 


Consider the square in the interior of the panel of the full-size 
violin. In the 3/4-size violin, its height and width are both smaller 
by a factor of 3/4, so the area of the corresponding, smaller square 
becomes 3/4 x 3/4 = 9/16 of the original area, not 3/4 of the original 
area. Similarly, the corresponding square on the smallest violin has 
half the height and half the width of the original one, so its area is 
1/4 the original area, not half. 


The same reasoning works for parts of the panel near the edge, 
such as the part that only partially fills in the other square. The 
entire square scales down the same as a square in the interior, and 
in each violin the same fraction (about 70%) of the square is full, so 
the contribution of this part to the total area scales down just the 
same. 





?The customary terms “half-size” and “3/4-size” actually don’t describe the 
sizes in any accurate way. They’re really just standard, arbitrary marketing 
labels. 





i/The area of a_ shape is 
proportional to the square of its 
linear dimensions, even if the 
shape is irregular. 


Section 1.2 Scaling of area and volume 47 





j/The muffin comes out of 
the oven too hot to eat. Breaking 
it up into four pieces increases 
its surface area while keeping 
the total volume the same. It 


cools faster because of the 
greater surface-to-volume ratio. 
In general, smaller things have 
greater surface-to-volume ratios, 
but in this example there is no 
easy way to compute the effect 
exactly, because the small pieces 
aren't the same shape as the 
original muffin. 


48 Chapter 1 


Since any small square region or any small region covering part 
of a square scales down like a square object, the entire surface area 
of an irregularly shaped object changes in the same manner as the 
surface area of a square: scaling it down by 3/4 reduces the area by 
a factor of 9/16, and so on. 


In general, we can see that any time there are two objects with 
the same shape, but different linear dimensions (i.e., one looks like a 
reduced photo of the other), the ratio of their areas equals the ratio 
of the squares of their linear dimensions: 


Ay (li\? 

a) 
Note that it doesn’t matter where we choose to measure the linear 
size, L, of an object. In the case of the violins, for instance, it could 
have been measured vertically, horizontally, diagonally, or even from 
the bottom of the left fhole to the middle of the right f-hole. We 
just have to measure it in a consistent way on each violin. Since all 


the parts are assumed to shrink or expand in the same manner, the 
ratio L,/L» is independent of the choice of measurement. 


It is also important to realize that it is completely unnecessary 
to have a formula for the area of a violin. It is only possible to 
derive simple formulas for the areas of certain shapes like circles, 
rectangles, triangles and so on, but that is no impediment to the 
type of reasoning we are using. 


Sometimes it is inconvenient to write all the equations in terms 
of ratios, especially when more than two objects are being compared. 
A more compact way of rewriting the previous equation is 


Ax L?. 


The symbol “x” means “is proportional to.” Scientists and engi- 
neers often speak about such relationships verbally using the phrases 


“scales like” or “goes like,” for instance “area goes like length squared. 


All of the above reasoning works just as well in the case of vol- 
ume. Volume goes like length cubed: 


Va L. 


self-check B 

When a car or truck travels over a road, there is wear and tear on the 
road surface, which incurs a cost. Studies show that the cost C per kilo- 
meter of travel is related to the weight per axle w by C x w’. Translate 
this into a statement about ratios. > Answer, p. 564 


If different objects are made of the same material with the same 
density, p = m/V, then their masses, m = pV, are proportional to 
L°. (The symbol for density is p, the lower-case Greek letter “rho.” ) 


Scaling and Estimation 


” 


An important point is that all of the above reasoning about 
scaling only applies to objects that are the same shape. For instance, 
a piece of paper is larger than a pencil, but has a much greater 
surface-to-volume ratio. 


Scaling of the area of a triangle example 1 
> In figure k, the larger triangle has sides twice as long. How 
many times greater is its area? 


Correct solution #1: Area scales in proportion to the square of the 
linear dimensions, so the larger triangle has four times more area 
(22 = 4). 


Correct solution #2: You could cut the larger triangle into four of 
the smaller size, as shown in fig. (b), so its area is four times 
greater. (This solution is correct, but it would not work for a shape 
like a circle, which can’t be cut up into smaller circles.) 


Correct solution #3: The area of a triangle is given by 


A = bh/2, where b is the base and h is the height. The areas of 
the triangles are 


Ay = bihy /2 
Ap = bohp /2 
= (2b;)(2h1)/2 
= 2b; hy 
A2/Ay = (2b; hy) /(b; hy /2) 
=4 


(Although this solution is correct, it is a lot more work than solution 
#1, and it can only be used in this case because a triangle is a 
simple geometric shape, and we happen to know a formula for its 
area.) 


Correct solution #4: The area of a triangle is A = bh/2. The 
comparison of the areas will come out the same as long as the 
ratios of the linear sizes of the triangles is as specified, so let’s 
just say 6; = 1.00 m and bo = 2.00 m. The heights are then also 
hy = 1.00 m and ho = 2.00 m, giving areas A; = 0.50 m? and 
Ao = 2.00 m2, so A2/Aj = 4.00. 


(The solution is correct, but it wouldn’t work with a shape for 
whose area we don’t have a formula. Also, the numerical cal- 
culation might make the answer of 4.00 appear inexact, whereas 
solution #1 makes it clear that it is exactly 4.) 


Incorrect solution: The area of a triangle is A = bh/2, and if you 
plug in b = 2.00 m and h = 2.00 m, you get A = 2.00 m2, so 
the bigger triangle has 2.00 times more area. (This solution is 
incorrect because no comparison has been made with the smaller 
triangle.) 


k/Example 1. The big trian- 
gle has four times more area than 
the little one. 


1/A tricky way of solving ex- 
ample 1, explained in solution #2. 


Section 1.2 Scaling of area and volume 49 


© 


m/Example 2. The big sphere 
has 125 times more volume than 


the little one. 


n/Example_ 3. 





The 48-point 


“S” has 1.78 times more area 
than the 36-point “S.” 


50 


Scaling of the volume of a sphere example 2 
> In figure m, the larger sphere has a radius that is five times 
greater. How many times greater is its volume? 


Correct solution #1: Volume scales like the third power of the 
linear size, so the larger sphere has a volume that is 125 times 
greater (5° = 125). 


Correct solution #2: The volume of a sphere is V = (4/3)zr?, so 


V; a an} 
4 
= an(5n)° 
500 _ 3 
ae ai 
4 
Vo/V4 = (AP) / (3x?) = 125 
Incorrect solution: The volume of a sphere is V = (4/3)zr°, so 
4 
4 
Vo = 372 
4 
= an} 
20 4 
Vo/V1 = (Fx) / (3-4) =5 


(The solution is incorrect because (5r;)° is not the same as 57.) 


Scaling of a more complex shape example 3 
> The first letter “S” in figure n is in a 36-point font, the second in 
48-point. How many times more ink is required to make the larger 
“S’? (Points are a unit of length used in typography.) 


Correct solution: The amount of ink depends on the area to be 
covered with ink, and area is proportional to the square of the 
linear dimensions, so the amount of ink required for the second 
“S” is greater by a factor of (48 /36)° = 1.78. 


Incorrect solution: The length of the curve of the second “S” is 
longer by a factor of 48/36 = 1.33, so 1.33 times more ink is 
required. 


(The solution is wrong because it assumes incorrectly that the 
width of the curve is the same in both cases. Actually both the 


Chapter 1 Scaling and Estimation 


width and the length of the curve are greater by a factor of 48/36, 
so the area is greater by a factor of (48/36)? = 1.78.) 


Reasoning about ratios and proportionalities is one of the three 
essential mathematical skills, summarized on pp.545-546, that you 
need for success in this course. 


> Solved problem: a telescope gathers light page 59, problem 11 


> Solved problem: distance from an earthquake page 59, problem 12 


Discussion questions 


A __ A toy fire engine is 1/30 the size of the real one, but is constructed 
from the same metal with the same proportions. How many times smaller 
is its weight? How many times less red paint would be needed to paint 
it? 

B Galileo spends a lot of time in his dialog discussing what really 
happens when things break. He discusses everything in terms of Aristo- 
tle’s now-discredited explanation that things are hard to break, because 
if something breaks, there has to be a gap between the two halves with 
nothing in between, at least initially. Nature, according to Aristotle, “ab- 
hors a vacuum,” i.e., nature doesn't “like” empty space to exist. Of course, 
air will rush into the gap immediately, but at the very moment of breaking, 
Aristotle imagined a vacuum in the gap. Is Aristotle’s explanation of why 
it is hard to break things an experimentally testable statement? If so, how 
could it be tested experimentally? 


1.3 x Scaling applied to biology 
Organisms of different sizes with the same shape 


The left-hand panel in figure o shows the approximate valid- 
ity of the proportionality m o L? for cockroaches (redrawn from 
McMahon and Bonner). The scatter of the points around the curve 
indicates that some cockroaches are proportioned slightly differently 
from others, but in general the data seem well described by m « L?. 
That means that the largest cockroaches the experimenter could 
raise (is there a 4-H prize?) had roughly the same shape as the 
smallest ones. 


Another relationship that should exist for animals of different 
sizes shaped in the same way is that between surface area and 
body mass. If all the animals have the same average density, then 
body mass should be proportional to the cube of the animal’s lin- 
ear size, m x L?, while surface area should vary proportionately to 
L?. Therefore, the animals’ surface areas should be proportional to 
m2/3_ As shown in the right-hand panel of figure o, this relationship 
appears to hold quite well for the dwarf siren, a type of salamander. 
Notice how the curve bends over, meaning that the surface area does 
not increase as quickly as body mass, e.g., a salamander with eight 


Section 1.3 x Scaling applied to biology 


1000 


body mass (mg) 


750 


500 


250 


Body mass, m, versus leg 
length, L, for the cockroach 
Periplaneta americana 
The data points rep- 
resent individual 
specimens, and the 

curve is a fit to the 

data of the form 


m=kL 3 where k is 
a constant. 





1 2 


length of leg segment (mm) 


o / Geometrical scaling of animals. 


52 


Chapter 1 


1000 





800 
a 
5 
S 800 Surface 
a area versus 
8 body mass for 
5 dwarf sirens, a 
2) 
400 species of sala- 
mander ( Pseudo- 
branchus striatus __). 
The data points 
200 represent individual 
specimens, and the curve is 
afitofthe formA=km = 2/3. 
0 
3 0 500 1000 


body mass (g) 


times more body mass will have only four times more surface area. 


This behavior of the ratio of surface area to mass (or, equiv- 
alently, the ratio of surface area to volume) has important conse- 
quences for mammals, which must maintain a constant body tem- 
perature. It would make sense for the rate of heat loss through the 
animal’s skin to be proportional to its surface area, so we should 
expect small animals, having large ratios of surface area to volume, 
to need to produce a great deal of heat in comparison to their size to 
avoid dying from low body temperature. This expectation is borne 
out by the data of the left-hand panel of figure p, showing the rate 
of oxygen consumption of guinea pigs as a function of their body 
mass. Neither an animal’s heat production nor its surface area is 
convenient to measure, but in order to produce heat, the animal 
must metabolize oxygen, so oxygen consumption is a good indicator 
of the rate of heat production. Since surface area is proportional to 
m2/3, the proportionality of the rate of oxygen consumption to m2/3 
is consistent with the idea that the animal needs to produce heat at a 
rate in proportion to its surface area. Although the smaller animals 


Scaling and Estimation 


Diameter versus length 
of the third lumbar 
vertebrae of adult 
African Bovidae 
(antelopes and oxen). 
The smallest animal 
represented is the 
cat-sized Gunther's 
dik-dik, and the 
largest is the 

850-kg giant 

eland. The 

solid curve is 

a fit of the 


diameter (cm) 


Rate of oxygen 
consumption versus 
body mass for guinea 
pigs at rest. The 
curve is a fit of the 


form (rate)=km 2/3 


oxygen consumption (mL/min) 





0.0 0.2 0.4 0.6 0.8 1.0 0 
body mass (kg) 


p / Scaling of animals’ bodies related to metabolic rate and skeletal strength. 


metabolize less oxygen and produce less heat in absolute terms, the 
amount of food and oxygen they must consume is greater in propor- 
tion to their own mass. The Etruscan pigmy shrew, weighing in at 
2 grams as an adult, is at about the lower size limit for mammals. 
It must eat continually, consuming many times its body weight each 
day to survive. 


Changes in shape to accommodate changes in size 


Large mammals, such as elephants, have a small ratio of surface 
area to volume, and have problems getting rid of their heat fast 
enough. An elephant cannot simply eat small enough amounts to 
keep from producing excessive heat, because cells need to have a 
certain minimum metabolic rate to run their internal machinery. 
Hence the elephant’s large ears, which add to its surface area and 
help it to cool itself. Previously, we have seen several examples 
of data within a given species that were consistent with a fixed 
shape, scaled up and down in the cases of individual specimens. The 
elephant’s ears are an example of a change in shape necessitated by 
a change in scale. 


and the dashed 
line is a linear 
fit. (After 
McMahon and 
Bonner, 1983.) 





length (cm) 


Section 1.3 x Scaling applied to biology 


q / Galileo’s 

showing how 
bones must be greater in diam- 
eter compared to their lengths. 


54 





original drawing, 
larger animals’ 


Large animals also must be able to support their own weight. 
Returning to the example of the strengths of planks of different 
sizes, we can see that if the strength of the plank depends on area 
while its weight depends on volume, then the ratio of strength to 
weight goes as follows: 


strength/weight « A/V « 1/L. 


Thus, the ability of objects to support their own weights decreases 
inversely in proportion to their linear dimensions. If an object is to 
be just barely able to support its own weight, then a larger version 
will have to be proportioned differently, with a different shape. 


Since the data on the cockroaches seemed to be consistent with 
roughly similar shapes within the species, it appears that the abil- 
ity to support its own weight was not the tightest design constraint 
that Nature was working under when she designed them. For large 
animals, structural strength is important. Galileo was the first to 
quantify this reasoning and to explain why, for instance, a large an- 
imal must have bones that are thicker in proportion to their length. 
Consider a roughly cylindrical bone such as a leg bone or a vertebra. 
The length of the bone, LD, is dictated by the overall linear size of the 
animal, since the animal’s skeleton must reach the animal’s whole 
length. We expect the animal’s mass to scale as L?, so the strength 
of the bone must also scale as L?. Strength is proportional to cross- 
sectional area, as with the wooden planks, so if the diameter of the 
bone is d, then 


a «x L? 


d x L3/?, 

If the shape stayed the same regardless of size, then all linear di- 
mensions, including d and L, would be proportional to one another. 
If our reasoning holds, then the fact that d is proportional to L®/?, 
not L, implies a change in proportions of the bone. As shown in the 
right-hand panel of figure p, the vertebrae of African Bovidae follow 
the rule d x L?/? fairly well. The vertebrae of the giant eland are 
as chunky as a coffee mug, while those of a Gunther’s dik-dik are as 
slender as the cap of a pen. 


Discussion questions 


A Single-celled animals must passively absorb nutrients and oxygen 
from their surroundings, unlike humans who have lungs to pump air in and 
out and a heart to distribute the oxygenated blood throughout their bodies. 
Even the cells composing the bodies of multicellular animals must absorb 
oxygen from a nearby capillary through their surfaces. Based on these 
facts, explain why cells are always microscopic in size. 


B The reasoning of the previous question would seem to be contra- 
dicted by the fact that human nerve cells in the spinal cord can be as 
much as a meter long, although their widths are still very small. Why is 
this possible? 


Chapter 1 Scaling and Estimation 


1.4 Order-of-magnitude estimates 


It is the mark of an instructed mind to rest satisfied with the 
degree of precision that the nature of the subject permits and 
not to seek an exactness where only an approximation of the 
truth is possible. 


Aristotle 


It is a common misconception that science must be exact. For 
instance, in the Star Trek TV series, it would often happen that 
Captain Kirk would ask Mr. Spock, “Spock, we’re in a pretty bad 
situation. What do you think are our chances of getting out of 
here?” The scientific Mr. Spock would answer with something like, 
“Captain, I estimate the odds as 237.345 to one.” In reality, he 
could not have estimated the odds with six significant figures of 
accuracy, but nevertheless one of the hallmarks of a person with a 
good education in science is the ability to make estimates that are 
likely to be at least somewhere in the right ballpark. In many such 
situations, it is often only necessary to get an answer that is off by no 
more than a factor of ten in either direction. Since things that differ 
by a factor of ten are said to differ by one order of magnitude, such 
an estimate is called an order-of-magnitude estimate. The tilde, 
~, is used to indicate that things are only of the same order of 
magnitude, but not exactly equal, as in 


odds of survival ~ 100 to one. 


The tilde can also be used in front of an individual number to em- 
phasize that the number is only of the right order of magnitude. 


Although making order-of-magnitude estimates seems simple and 
natural to experienced scientists, it’s a mode of reasoning that is 
completely unfamiliar to most college students. Some of the typical 
mental steps can be illustrated in the following example. 


Cost of transporting tomatoes (incorrect solution) example 4 
> Roughly what percentage of the price of a tomato comes from 
the cost of transporting it in a truck? 


> The following incorrect solution illustrates one of the main ways 
you can go wrong in order-of-magnitude estimates. 


Incorrect solution: Let’s say the trucker needs to make a $400 
profit on the trip. Taking into account her benefits, the cost of gas, 
and maintenance and payments on the truck, let’s say the total 
cost is more like $2000. I’d guess about 5000 tomatoes would fit 
in the back of the truck, so the extra cost per tomato is 40 cents. 
That means the cost of transporting one tomato is comparable to 
the cost of the tomato itself. Transportation really adds a lot to the 
cost of produce, | guess. 


The problem is that the human brain is not very good at esti- 
mating area or volume, so it turns out the estimate of 5000 tomatoes 


Section 1.4 Order-of-magnitude estimates 


55 





r/Can you guess how many 
jelly beans are in the jar? If you 
try to guess directly, you will 
almost certainly underestimate. 
The right way to do it is to esti- 
mate the linear dimensions, then 
get the volume indirectly. See 
problem 26, p. 62. 





s/Consider a_ spherical cow. 


fitting in the truck is way off. That’s why people have a hard time 
at those contests where you are supposed to estimate the number of 
jellybeans in a big jar. Another example is that most people think 
their families use about 10 gallons of water per day, but in reality 
the average is about 300 gallons per day. When estimating area 
or volume, you are much better off estimating linear dimensions, 
and computing volume from the linear dimensions. Here’s a better 
solution to the problem about the tomato truck: 


Cost of transporting tomatoes (correct solution) example 5 
As in the previous solution, say the cost of the trip is $2000. The 
dimensions of the bin are probably 4m x 2m x 1 m, for a vol- 
ume of 8 m?. Since the whole thing is just an order-of-magnitude 
estimate, let’s round that off to the nearest power of ten, 10 m°. 
The shape of a tomato is complicated, and | don’t know any for- 
mula for the volume of a tomato shape, but since this is just an 
estimate, let’s pretend that a tomato is a cube, 0.05m x 0.05m x 
0.05 m, for a volume of 1.25 x 10-4 m®. Since this is just a rough 
estimate, let's round that to 10-4m%. We can find the total num- 
ber of tomatoes by dividing the volume of the bin by the volume 
of one tomato: 10 m°/10-* m® = 10° tomatoes. The transporta- 
tion cost per tomato is $2000/10° tomatoes=$0.02/tomato. That 
means that transportation really doesn’t contribute very much to 
the cost of a tomato. 


Approximating the shape of a tomato as a cube is an example of 
another general strategy for making order-of-magnitude estimates. 
A similar situation would occur if you were trying to estimate how 
many m? of leather could be produced from a herd of ten thousand 
cattle. There is no point in trying to take into account the shape of 
the cows’ bodies. A reasonable plan of attack might be to consider 
a spherical cow. Probably a cow has roughly the same surface area 
as a sphere with a radius of about 1 m, which would be 47(1 m)?. 
Using the well-known facts that pi equals three, and four times three 
equals about ten, we can guess that a cow has a surface area of about 
10 m?, so the herd as a whole might yield 10° m? of leather. 


Estimating mass indirectly example 6 
Usually the best way to estimate mass is to estimate linear di- 
mensions, then use those to infer volume, and then get the mass 
based on the volume. For example, Amphicoelias, shown in the 
figure, may have been the largest land animal ever to live. Fossils 
tell us the linear dimensions of an animal, but we can only indi- 
rectly guess its mass. Given the length scale in the figure, let’s 
estimate the mass of an Amphicoelias. 


Its torso looks like it can be approximated by a rectangular box 
with dimensions 10 mx 5 mx 3m, giving about 2 x 10° m°. Living 
things are mostly made of water, so we assume the animal to 
have the density of water, 1 g/cm®, which converts to 10? kg/m®. 


56 Chapter 1 Scaling and Estimation 


This gives a mass of about 2 x 10° kg, or 200 metric tons. 





The following list summarizes the strategies for getting a good 
order-of-magnitude estimate. 


1. Don’t even attempt more than one significant figure of preci- 
sion. 


2. Don’t guess area, volume, or mass directly. Guess linear di- 
mensions and get area, volume, or mass from them. 


3. When dealing with areas or volumes of objects with complex 
shapes, idealize them as if they were some simpler shape, a 
cube or a sphere, for example. 


4. Check your final answer to see if it is reasonable. If you esti- 
mate that a herd of ten thousand cattle would yield 0.01 m? 
of leather, then you have probably made a mistake with con- 
version factors somewhere. 


Section 1.4 Order-of-magnitude estimates 


57 


58 


Chapter 1 


Summary 


Notation 

OG. MegSodA emacs dus is proportional to 

BOE Ses tN ne Bae ed on the order of, is on the order of 
Summary 


Nature behaves differently on large and small scales. Galileo 
showed that this results fundamentally from the way area and vol- 
ume scale. Area scales as the second power of length, A « L?, while 
volume scales as length to the third power, V « L?. 


An order of magnitude estimate is one in which we do not at- 
tempt or expect an exact answer. The main reason why the unini- 
tiated have trouble with order-of-magnitude estimates is that the 
human brain does not intuitively make accurate estimates of area 
and volume. Estimates of area and volume should be approached 
by first estimating linear dimensions, which one’s brain has a feel 
for. 


Scaling and Estimation 


Problems 
Key 


VA computerized answer check is available online. 
J A problem that requires calculus. 
x A difficult problem. 


1 How many cubic inches are there in a cubic foot? The answer 
is not 12. Vv 
2 Assume a dog’s brain is twice as great in diameter as a cat’s, 


but each animal’s brain cells are the same size and their brains are 
the same shape. In addition to being a far better companion and 
much nicer to come home to, how many times more brain cells does 
a dog have than a cat? The answer is not 2. 

2 


3 The population density of Los Angeles is about 4000 people/km*. 


That of San Francisco is about 6000 people/km?. How many times 
farther away is the average person’s nearest neighbor in LA than in 
San Francisco? The answer is not 1.5. Vv 


4 A hunting dog’s nose has about 10 square inches of active 
surface. How is this possible, since the dog’s nose is only about 1 in 
x lin x 1 in = 1 in®? After all, 10 is greater than 1, so how can it 
fit’? 


5 Estimate the number of blades of grass on a football field. 


6 In a computer memory chip, each bit of information (a 0 or 
a 1) is stored in a single tiny circuit etched onto the surface of a 
silicon chip. The circuits cover the surface of the chip like lots in a 
housing development. A typical chip stores 64 Mb (megabytes) of 
data, where a byte is 8 bits. Estimate (a) the area of each circuit, 
and (b) its linear size. 


7 Suppose someone built a gigantic apartment building, mea- 
suring 10 km x 10 km at the base. Estimate how tall the building 
would have to be to have space in it for the entire world’s population 
to live. 


8 A hamburger chain advertises that it has sold 10 billion Bongo 
Burgers. Estimate the total mass of feed required to raise the cows 
used to make the burgers. 


9 Estimate the volume of a human body, in cm’. 


10 How many cm? is 1 mm?? > Solution, p. 547 


11 Compare the light-gathering powers of a 3-cm-diameter tele- 
scope and a 30-cm telescope. > Solution, p. 547 


12 One step on the Richter scale corresponds to a factor of 100 
in terms of the energy absorbed by something on the surface of the 
Earth, e.g., a house. For instance, a 9.3-magnitude quake would 
release 100 times more energy than an 8.3. The energy spreads out 


Problems 


59 


from the epicenter as a wave, and for the sake of this problem we’ll 
assume we’re dealing with seismic waves that spread out in three 
dimensions, so that we can visualize them as hemispheres spreading 
out under the surface of the earth. If a certain 7.6-magnitude earth- 
quake and a certain 5.6-magnitude earthquake produce the same 
amount of vibration where I live, compare the distances from my 
house to the two epicenters. > Solution, p. 548 


13 In Europe, a piece of paper of the standard size, called A4, 
is a little narrower and taller than its American counterpart. The 
ratio of the height to the width is the square root of 2, and this has 
some useful properties. For instance, if you cut an A4 sheet from left 
to right, you get two smaller sheets that have the same proportions. 
You can even buy sheets of this smaller size, and they’re called A5. 
There is a whole series of sizes related in this way, all with the same 
proportions. (a) Compare an A5 sheet to an A4 in terms of area and 
linear size. (b) The series of paper sizes starts from an AO sheet, 
which has an area of one square meter. Suppose we had a series 
of boxes defined in a similar way: the BO box has a volume of one 
cubic meter, two B1 boxes fit exactly inside an BO box, and so on. 
What would be the dimensions of a BO box? Vv 


14 Estimate the mass of one of the hairs in Albert Einstein’s 
moustache, in units of kg. 


15 According to folklore, every time you take a breath, you are 
inhaling some of the atoms exhaled in Caesar’s last words. Is this 
true? If so, how many? 





Albert Einstein, and his mous- 


tache, problem 14. 16 The Earth’s surface is about 70% water. Mars’s diameter is 
about half the Earth’s, but it has no surface water. Compare the 
land areas of the two planets. v 


17 The traditional Martini glass is shaped like a cone with 
the point at the bottom. Suppose you make a Martini by pouring 
vermouth into the glass to a depth of 3 cm, and then adding gin 
to bring the depth to 6 cm. What are the proportions of gin and 
vermouth? > Solution, p. 548 


18 The central portion of a CD is taken up by the hole and some 
surrounding clear plastic, and this area is unavailable for storing 
data. The radius of the central circle is about 35% of the outer 
radius of the data-storing area. What percentage of the CD’s area 
is therefore lost? Vv 





Problem 19. 19 The one-liter cube in the photo has been marked off into 
smaller cubes, with linear dimensions one tenth those of the big 
one. What is the volume of each of the small cubes? 

> Solution, p. 548 


60 Chapter 1 Scaling and Estimation 


20 [This problem is now problem 0-12 on p. 37.] 


21 Estimate the number of man-hours required for building the 
Great Wall of China. > Solution, p. 548 


22 (a) Using the microscope photo in the figure, estimate the 
mass of a one cell of the E. coli bacterium, which is one of the 
most common ones in the human intestine. Note the scale at the 
lower right corner, which is 1 wm. Each of the tubular objects in 
the column is one cell. (b) The feces in the human intestine are 
mostly bacteria (some dead, some alive), of which EL. coli is a large 
and typical component. Estimate the number of bacteria in your 
intestines, and compare with the number of human cells in your 
body, which is believed to be roughly on the order of 10!%. (c) 
Interpreting your result from part b, what does this tell you about 
the size of a typical human cell compared to the size of a typical 
bacterial cell? 


23 A taxon (plural taxa) is a group of living things. For ex- 
ample, Homo sapiens and Homo neanderthalensis are both taxa — 
specifically, they are two different species within the genus Homo. 
Surveys by botanists show that the number of plant taxa native 
to a given contiguous land area A is usually approximately propor- 
tional to A‘/3. (a) There are 70 different species of lupine native 
to Southern California, which has an area of about 200,000 km?. 
The San Gabriel Mountains cover about 1,600 km?. Suppose that 
you wanted to learn to identify all the species of lupine in the San 
Gabriels. Approximately how many species would you have to fa- 
miliarize yourself with? > Answer, p. 569 V 
(b) What is the interpretation of the fact that the exponent, 1/3, is 
less than one? 


Problem 22. 


Problems 





61 


24 X-ray images aren’t only used with human subjects but also, 
for example, on insects and flowers. In 2003, a team of researchers 
at Argonne National Laboratory used x-ray imagery to find for the 
first time that insects, although they do not have lungs, do not 
necessarily breathe completely passively, as had been believed pre- 
viously; many insects rapidly compress and expand their trachea, 
head, and thorax in order to force air in and out of their bodies. 
One difference between x-raying a human and an insect is that if a 
medical x-ray machine was used on an insect, virtually 100% of the 
x-rays would pass through its body, and there would be no contrast 
in the image produced. Less penetrating x-rays of lower energies 
have to be used. For comparison, a typical human body mass is 
about 70 kg, whereas a typical ant is about 10 mg. Estimate the 
ratio of the thicknesses of tissue that must be penetrated by x-rays 
in one case compared to the other. v 


25 Radio was first commercialized around 1920, and ever since 
then, radio signals from our planet have been spreading out across 
our galaxy. It is possible that alien civilizations could detect these 
signals and learn that there is life on earth. In the 90 years that the 
signals have been spreading at the speed of light, they have created 
a sphere with a radius of 90 light-years. To show an idea of the 
size of this sphere, I’ve indicated it in the figure as a tiny white 
circle on an image of a spiral galaxy seen edge on. (We don’t have 
similar photos of our own Milky Way galaxy, because we can’t see 
Problem 25. it from the outside.) So far we haven’t received answering signals 
from aliens within this sphere, but as time goes on, the sphere will 
expand as suggested by the dashed outline, reaching more and more 
stars that might harbor extraterrestrial life. Approximately what 
year will it be when the sphere has expanded to fill a volume 100 
times greater than the volume it fills today in 2010? Vv 





26 Estimate the number of jellybeans in figure r on p. 56. 
> Solution, p. 548 


27 At the grocery store you will see oranges packed neatly in 
stacks. Suppose we want to pack spheres as densely as possible, 
so that the greatest possible fraction of the space is filled by the 
spheres themselves, not by empty space. Let’s call this fraction f. 
Mathematicians have proved that the best possible result is f ~ 
0.7405, which requires a systematic pattern of stacking. If you buy 
ball bearings or golf balls, however, the seller is probably not going 
to go to the trouble of stacking them neatly. Instead they will 
probably pour the balls into a box and vibrate the box vigorously 
for a while to make them settle. This results in a random packing. 
The closest random packing has f ¥ 0.64. Suppose that golf balls, 
with a standard diameter of 4.27 cm, are sold in bulk with the 
Problem 27. closest random packing. What is the diameter of the largest ball 
that could be sold in boxes of the same size, packed systematically, 
so that there would be the same number of balls per box? v 
62 Chapter 1. Scaling and Estimation 
28 Plutonium-239 is one of a small number of important long- 
lived forms of high-level radioactive nuclear waste. The world’s 
waste stockpiles have about 10° metric tons of plutonium. Drinking 
water is considered safe by U.S. government standards if it contains 
less than 2 x 107!3 g/cm? of plutonium. The amount of radioac- 
tivity to which you were exposed by drinking such water on a daily 
basis would be very small compared to the natural background radi- 
ation that you are exposed to every year. Suppose that the world’s 
inventory of plutonium-239 were ground up into an extremely fine 
dust and then dispersed over the world’s oceans, thereby becoming 
mixed uniformly into the world’s water supplies over time. Esti- 
mate the resulting concentration of plutonium, and compare with 
the government standard. 
Problems 
63 
Exercise 1: Scaling applied to leaves 
Equipment: 
leaves of three sizes, having roughly similar proportions of length, width, and thickness balance 
Each group will have one leaf, and should measure its surface area and volume, and determine 
its surface-to-volume ratio. For consistency, every group should use units of cm? and cm?, and 
should only find the area of one side of the leaf. The area can be found by tracing the area of 
the leaf on graph paper and counting squares. The volume can be found by weighing the leaf 
and assuming that its density is 1 g/cm? (the density of water). What implications do your results have for the plants’ abilities to survive in different environments? 
The idea is that each of the algebra symbols with an arrow writ- 
ten on top, called a vector, is actually an abbreviation for three 
different numbers, the x, y, and z components. The three compo- 
nents are referred to as the components of the vector, e.g., Fi, is the 
x component of the vector F’. The notation with an arrow on top 
is good for handwritten equations, but is unattractive in a printed 
book, so books use boldface, F, to represent vectors. After this 
point, I’ll use boldface for vectors throughout this book. 


Quantities can be classified as vectors or scalars. In a phrase like 
“a to the northeast,” it makes sense to fill in the blank with 
“force” or “velocity,” which are vectors, but not with “mass” or 
“time,” which are scalars. Any nonzero vector has both a direction 
and an amount. The amount is called its magnitude. The notation 
for the magnitude of a vector A is |AJ, like the absolute value sign 
used with scalars. 


Often, as in example (b), we wish to use the vector notation to 
represent adding up all the x components to get a total x component, 
etc. The plus sign is used between two vectors to indicate this type 
of component-by-component addition. Of course, vectors are really 
triplets of numbers, not numbers, so this is not the same as the use 
of the plus sign with individual numbers. But since we don’t want to 
have to invent new words and symbols for this operation on vectors, 
we use the same old plus sign, and the same old addition-related 
words like “add,” “sum,” and “total.” Combining vectors this way 
is called vector addition. 


Similarly, the minus sign in example (a) was used to indicate 
negating each of the vector’s three components individually. The 
equals sign is used to mean that all three components of the vector 
on the left side of an equation are the same as the corresponding 
components on the right. 


Example (c) shows how we abuse the division symbol in a similar 
manner. When we write the vector Av divided by the scalar At, 
we mean the new vector formed by dividing each one of the velocity 
components by At. 


It’s not hard to imagine a variety of operations that would com- 
bine vectors with vectors or vectors with scalars, but only four of 
them are required in order to express Newton’s laws: 


Chapter 7 Vectors 


operation definition 

vector + vector Add component by component to 
make a new set of three numbers. 

vector — vector Subtract component by component 
to make a new set of three numbers. 


vector - scalar Multiply each component of the vec- 
tor by the scalar. 
vector /scalar Divide each component of the vector 


by the scalar. 


As an example of an operation that is not useful for physics, there 
just aren’t any useful physics applications for dividing a vector by 
another vector component by component. In optional section 7.5, 
we discuss in more detail the fundamental reasons why some vector 
operations are useful and others useless. 


We can do algebra with vectors, or with a mixture of vectors 
and scalars in the same equation. Basically all the normal rules of 
algebra apply, but if you’re not sure if a certain step is valid, you 
should simply translate it into three component-based equations and 
see if it works. 


Order of addition example 1 
> If we are adding two force vectors, F + G, is it valid to assume 
as in ordinary algebra that F + Gis the same as G+ F? 


> To tell if this algebra rule also applies to vectors, we simply 
translate the vector notation into ordinary algebra notation. In 
terms of ordinary numbers, the components of the vector F + G 
would be Fy + Gy, Fy + Gy, and Fz + Gz, which are certainly the 
same three numbers as Gy + Fy, Gy + Fy, and Gz+ Fz. Yes, F+G 
is the same as G+F. 


It is useful to define a symbol r for the vector whose components 
are x, y, and z, and a symbol Ar made out of Az, Ay, and Az. 


Although this may all seem a little formidable, keep in mind that 
it amounts to nothing more than a way of abbreviating equations! 
Also, to keep things from getting too confusing the remainder of this 
chapter focuses mainly on the Ar vector, which is relatively easy to 
visualize. 


self-check A 

Translate the equations vy = Ax/At, v, = Ay/At, and vz = Az/At for 
motion with constant velocity into a single equation in vector notation. 
> Answer, p. 566 


Section 7.1 


Vector notation 


205 


nl 


x component 
(positive) 


y 
y component 
(negative) 
x 


b/The x and y components 
of a vector can be thought of as 
the shadows it casts onto the x 
and y axes. 


Q 


c / Self-check B. 





d/A_ playing card returns to 
its original state when rotated by 
180 degrees. 


Drawing vectors as arrows 


A vector in two dimensions can be easily visualized by drawing 
an arrow whose length represents its magnitude and whose direction 
represents its direction. The « component of a vector can then be 
visualized as the length of the shadow it would cast in a beam of 
light projected onto the x axis, and similarly for the y component. 
Shadows with arrowheads pointing back against the direction of the 
positive axis correspond to negative components. 


In this type of diagram, the negative of a vector is the vector 
with the same magnitude but in the opposite direction. Multiplying 
a vector by a scalar is represented by lengthening the arrow by that 
factor, and similarly for division. 


self-check B 

Given vector Q represented by an arrow in figure c, draw arrows repre- 
senting the vectors 1.5Q and —Q. > Answer, p. 
566 


This leads to a way of defining vectors and scalars that reflects 
how physicists think in general about these things: 


definition of vectors and scalars 
A general type of measurement (force, velocity, ...) is a vector if it 
can be drawn as an arrow so that rotating the paper produces the 
same result as rotating the actual quantity. A type of quantity that 
never changes at all under rotation is a scalar. 


For example, a force reverses itself under a 180-degree rotation, 
but a mass doesn’t. We could have defined a vector as something 
that had both a magnitude and a direction, but that would have left 
out zero vectors, which don’t have a direction. A zero vector is a 
legitimate vector, because it behaves the same way under rotations 
as a zero-length arrow, which is simply a dot. 


A remark for those who enjoy brain-teasers: not everything is 
a vector or a scalar. An American football is distorted compared 
to a sphere, and we can measure the orientation and amount of 
that distortion quantitatively. The distortion is not a vector, since 
a 180-degree rotation brings it back to its original state. Something 
similar happens with playing cards, figure d. For some subatomic 
particles, such as electrons, 360 degrees isn’t even enough; a 720- 
degree rotation is needed to put them back the way they were! 


Discussion questions 


A You drive to your friend’s house. How does the magnitude of your Ar 
vector compare with the distance you've added to the car’s odometer? 


206 Chapter 7 Vectors 


7.2 Calculations with magnitude and direction 


If you ask someone where Las Vegas is compared to Los Angeles, 
they are unlikely to say that the Ax is 290 km and the Ay is 230 
km, in a coordinate system where the positive x axis is east and the 
y axis points north. They will probably say instead that it’s 370 
km to the northeast. If they were being precise, they might give the 
direction as 38° counterclockwise from east. In two dimensions, we 
can always specify a vector’s direction like this, using a single angle. 
A magnitude plus an angle suffice to specify everything about the 
vector. The following two examples show how we use trigonometry 
and the Pythagorean theorem to go back and forth between the x—y 
and magnitude-angle descriptions of vectors. 


Finding magnitude and angle from components example 2 
> Given that the Ar vector from LA to Las Vegas has Ax = 290 km 
and Ay = 230 km, how would we find the magnitude and direction 
of Ar? 


> We find the magnitude of Ar from the Pythagorean theorem: 


|Ar| = ,/Ax2 + Ay? 


= 370 km 





We know all three sides of the triangle, so the angle @ can be 
found using any of the inverse trig functions. For example, we 
know the opposite and adjacent sides, so 


_1 AY 
1 —— 
8 = tan ea 
= 38°. 
Finding components from magnitude and angle example 3 


> Given that the straight-line distance from Los Angeles to Las 
Vegas is 370 km, and that the angle 0 in the figure is 38°, how 
can the x and y components of the Ar vector be found? 


> The sine and cosine of 0 relate the given information to the 
information we wish to find: 


AX 

cos 8 = —_— 
|Ar| 

; Ay 
sin@ = —— 
|Ar| 


Solving for the unknowns gives 


Ax = |Ar| cos 8 

= 290 km and 
Ay = |Ar|sin@ 

= 230 km. 





“YS ax 


Los 
Angeles 


Las V egas 








e / Examples 2 and 3. 


Section 7.2 Calculations with magnitude and direction 


207 


Los 
Angeles 





Ax 
(negative) 


f / Example 4. 


208 


San Diego 


The following example shows the correct handling of the plus 
and minus signs, which is usually the main cause of mistakes. 


Negative components example 4 
> San Diego is 120 km east and 150 km south of Los Angeles. An 
airplane pilot is setting course from San Diego to Los Angeles. At 
what angle should she set her course, measured counterclock- 
wise from east, as shown in the figure? 


> If we make the traditional choice of coordinate axes, with x 
pointing to the right and y pointing up on the map, then her Ax is 
negative, because her final x value is less than her initial x value. 
Her Ay is positive, so we have 


Ax = —120 km 
Ay = 150 km. 


If we work by analogy with example 2, we get 


_1 Ay 

7 (SY 

8 =tan Ax 
= tan~'(—1.25) 
= —51°. 


According to the usual way of defining angles in trigonometry, 
a negative result means an angle that lies clockwise from the x 
axis, which would have her heading for the Baja California. What 
went wrong? The answer is that when you ask your calculator to 
take the arctangent of a number, there are always two valid pos- 
sibilities differing by 180°. That is, there are two possible angles 
whose tangents equal -1.25: 


tan 129° = —1.25 
tan —51° = —1.25 


You calculator doesn’t Know which is the correct one, so it just 
picks one. In this case, the one it picked was the wrong one, and 
it was up to you to add 180°to it to find the right answer. 


Chapter 7 Vectors 





‘A shortcut example 5 
> A split second after nine o’clock, the hour hand on a clock dial 
has moved clockwise past the nine-o’clock position by some im- 
perceptibly small angle @. Let positive x be to the right and posi- 
tive y up. If the hand, with length 2, is represented by a Ar vector g / Example 5. 
going from the dial’s center to the tip of the hand, find this vector’s 

AX. 





> The following shortcut is the easiest way to work out examples 
like these, in which a vector’s direction is known relative to one 
of the axes. We can tell that Ar will have a large, negative x 
component and a small, positive y. Since Ax < 0, there are 
really only two logical possibilities: either Ax = —£cos >, or Ax = 
—fsind. Because ¢ is small, cos ¢ is large and sind is small. 
We conclude that Ax = —fcos . 


A typical application of this technique to force vectors is given in 
example 6 on p. 226. 


Discussion question 


A _ Inexample 4, we dealt with components that were negative. Does it 
make sense to classify vectors as positive and negative? 


Section 7.2 Calculations with magnitude and direction 209 


Las Vegas 








oY 
Los 
Angeles 
San Diego 
h/ Example 6. 


A+B 
A 
A 

al 7A 

B 
i/ Vectors can be added graph- 
ically by placing them tip to tail, 
and then drawing a vector from 


the tail of the first vector to the tip 
of the second vector. 


7.3. Techniques for adding vectors 


Vector addition is one of the three essential mathematical skills, 
summarized on pp.545-546, that you need for success in this course. 


Addition of vectors given their components 


The easiest type of vector addition is when you are in possession 
of the components, and want to find the components of their sum. 


Adding components example 6 
> Given the Ax and Ay values from the previous examples, find 
the Ax and Ay from San Diego to Las Vegas. 


> 


AXtotal = AX1 + Axo 
= —120 km+ 290 km 
= 170 km 


AYtotal = AY1 + Aye 
= 150 km + 230 km 
= 380 


Note how the signs of the x components take care of the west- 
ward and eastward motions, which partially cancel. 


Addition of vectors given their magnitudes and directions 


In this case, you must first translate the magnitudes and di- 
rections into components, and the add the components. In our San 
Diego-Los Angeles-Las Vegas example, we can simply string together 
the preceding examples; this is done on p. 546. 


Graphical addition of vectors 


Often the easiest way to add vectors is by making a scale drawing 
on a piece of paper. This is known as graphical addition, as opposed 
to the analytic techniques discussed previously. (It has nothing to 
do with x — y graphs or graph paper. “Graphical” here simply 
means drawing. It comes from the Greek verb “grapho,” to write, 
like related English words including “graphic.” ) 


210 Chapter 7 Vectors 


LA to Vegas, graphically example 7 
> Given the magnitudes and angles of the Ar vectors from San 
Diego to Los Angeles and from Los Angeles to Las Vegas, find 
the magnitude and angle of the Ar vector from San Diego to Las 
Vegas. 


> Using a protractor and a ruler, we make a careful scale draw- 
ing, as shown in figure j. The protractor can be conveniently 
aligned with the blue rules on the notebook paper. A scale of 
1 mm -> 2 km was chosen for this solution because it was as big 
as possible (for accuracy) without being so big that the drawing 
wouldn't fit on the page. With a ruler, we measure the distance 
from San Diego to Las Vegas to be 206 mm, which corresponds 
to 412 km. With a protractor, we measure the angle 0 to be 65°. A If you’re doing graphical addition of vectors, does it matter which 
vector you start with and which vector you start from the other vector’s 
tip? 

B If you add a vector with magnitude 1 to a vector of magnitude 2, 
what magnitudes are possible for the vector sum? 


Cc Which of these examples of vector addition are correct, and which 
are incorrect? 


7.4 x Unit vector notation 


When we want to specify a vector by its components, it can be cum- 
bersome to have to write the algebra symbol for each component: 


Az = 290 km, Ay = 230 km 
A more compact notation is to write 
Ar = (290 km)x + (230 km)y, 


where the vectors x, y, and Z, called the unit vectors, are defined 
as the vectors that have magnitude equal to 1 and directions lying 
along the x, y, and z axes. In speech, they are referred to as “x-hat” 
and so on. 


A slightly different, and harder to remember, version of this 
notation is unfortunately more prevalent. In this version, the unit 
vectors are called i, j, and k: 


Ar = (290 km)i + (230 km)j. 


Chapter 7 Vectors 


7.5 x Rotational invariance 


Let’s take a closer look at why certain vector operations are use- 
ful and others are not. Consider the operation of multiplying two 
vectors component by component to produce a third vector: 


Ry — P,Q 
Ry = PyQy 
R, = PQ: 


As a simple example, we choose vectors P and Q to have length 
1, and make them perpendicular to each other, as shown in figure 
k/1. If we compute the result of our new vector operation using the 
coordinate system in k/2, we find: 


Ry =0 
Res 
R,=0. 


The x component is zero because P, = 0, the y component is zero 
because @, = 0, and the z component is of course zero because 
both vectors are in the x — y plane. However, if we carry out the 
same operations in coordinate system k/3, rotated 45 degrees with 
respect to the previous one, we find 


Ry, = 1/2 
Ry =—1/2 
Ry= 0. 


The operation’s result depends on what coordinate system we use, 
and since the two versions of R. have different lengths (one being zero 
and the other nonzero), they don’t just represent the same answer 
expressed in two different coordinate systems. Such an operation 
will never be useful in physics, because experiments show physics 
works the same regardless of which way we orient the laboratory 
building! The useful vector operations, such as addition and scalar 
multiplication, are rotationally invariant, i.e., come out the same 
regardless of the orientation of the coordinate system. 


Calibrating an electronic compass example 8 
Some smart phones and GPS units contain electronic compasses 
that can sense the direction of the earth’s magnetic field vector, 
notated B. Because all vectors work according to the same rules, 
you don’t need to know anything special about magnetism in or- 
der to understand this example. Unlike a traditional compass that 
uses a magnetized needle on a bearing, an electronic compass 
has no moving parts. It contains two sensors oriented perpendic- 
ular to one another, and each sensor is only sensitive to the com- 
ponent of the earth’s field that lies along its own axis. Because a 


/ 


3 
k / Component-by-component 
multiplication of the vectors in 1 


would produce different vectors 
in coordinate systems 2 and 3. 


Section 7.5 x Rotational invariance 213 


214 


Chapter 7 Vectors 


choice of coordinates is arbitrary, we can take one of these sen- 
sors as defining the x axis and the other the y. Given the two 
components B, and By, the device’s computer chip can compute 
the angle of magnetic north relative to its sensors, tan~'(By/B,). 


All compasses are vulnerable to errors because of nearby mag- 
netic materials, and in particular it may happen that some part 
of the compass’s own housing becomes magnetized. In an elec- 
tronic compass, rotational invariance provides a convenient way 
of calibrating away such effects by having the user rotate the de- 
vice in a horizontal circle. 


Suppose that when the compass is oriented in a certain way, it 
measures B, = 1.00 and By = 0.00 (in certain units). We then 
expect that when it is rotated 90 degrees clockwise, the sensors 
will detect B, = 0.00 and By = 1.00. 


But imagine instead that we get By = 0.20 and By = 0.80. This 
would violate rotational invariance, since rotating the coordinate 
system is supposed to give a different description of the same 
vector. The magnitude appears to have changed from 1.00 to 
V0.202 + 0.802 = 0.82, and a vector can’t change its magnitude 
just because you rotate it. The compass’s computer chip figures 
out that some effect, possibly a slight magnetization of its hous- 
ing, must be adding an erroneous 0.2 units to all the By readings, 
because subtracting this amount from all the B, values gives vec- 
tors that have the same magnitude, satisfying rotational invari- 
ance. 


Summary 
Selected vocabulary 


vector. ...... a quantity that has both an amount (magni- 
tude) and a direction in space 

magnitude .... the “amount” associated with a vector 

scalar ....... a quantity that has no direction in space, only 
an amount 

Notation 

RS erinaiis ek Sa a vector with components A;, A,, and A, 

A ssravigube aide ie tt handwritten notation for a vector 

AE) i oe wate the magnitude of vector A 

1 siege A the vector whose components are x, y, and z 

PNT oo hastens the vector whose components are Ax, Ay, and 
Az 

Ky Vew ae Saks (optional topic) unit vectors; the vectors with 
magnitude 1 lying along the x, y, and z axes 

Ty fg ioe ted Ak a harder to remember notation for the unit 
vectors 


Other terminology and notation 
displacement vec- a name for the symbol Ar 


TOPS Se eee 
speed ....... the magnitude of the velocity vector, i.e., the 
velocity stripped of any information about its 
direction 
Summary 


A vector is a quantity that has both a magnitude (amount) and 
a direction in space, as opposed to a scalar, which has no direction. 
The vector notation amounts simply to an abbreviation for writing 
the vector’s three components. 


In two dimensions, a vector can be represented either by its two 
components or by its magnitude and direction. The two ways of 
describing a vector can be related by trigonometry. 


The two main operations on vectors are addition of a vector to 
a vector, and multiplication of a vector by a scalar. 


Vector addition means adding the components of two vectors 
to form the components of a new vector. In graphical terms, this 
corresponds to drawing the vectors as two arrows laid tip-to-tail and 
drawing the sum vector from the tail of the first vector to the tip 
of the second one. Vector subtraction is performed by negating the 
vector to be subtracted and then adding. 


Multiplying a vector by a scalar means multiplying each of its 
components by the scalar to create a new vector. Division by a 
scalar is defined similarly. 


Summary 


215 


Problems 
Key 


Vv A computerized answer check is available online. 
J A problem that requires calculus. 
x A difficult problem. 


1 The figure shows vectors A and B. Graphically calculate 
the following, as in figure i on p. 210, self-check C on p. 211, and 


A self-check B on p. 206. 
A+B, A—B, B— A, —2B, A — 2B 
<e—___ : 
No numbers are involved. 
B 
Problem 1. 


2 Phnom Penh is 470 km east and 250 km south of Bangkok. 
Hanoi is 60 km east and 1030 km north of Phnom Penh. 

(a) Choose a coordinate system, and translate these data into Ax 
and Ay values with the proper plus and minus signs. 

(b) Find the components of the Ar vector pointing from Bangkok 
to Hanoi. Vv 


3 If you walk 35 km at an angle 25° counterclockwise from east, 
and then 22 km at 230° counterclockwise from east, find the distance 
and direction from your starting point to your destination. v 


4 A machinist is drilling holes in a piece of aluminum according 
to the plan shown in the figure. She starts with the top hole, then 
moves to the one on the left, and then to the one on the right. Since 
this is a high-precision job, she finishes by moving in the direction 
and at the angle that should take her back to the top hole, and 
checks that she ends up in the same place. What are the distance 
Problem 4. and direction from the right-hand hole to the top one? v 





216 Chapter 7 Vectors 


5 Suppose someone proposes a new operation in which a vector 
A and a scalar B are added together to make a new vector C like 
this: 


C,=Az+B 
Cy=Ay+B 
C,=A,+B 





Prove that this operation won’t be useful in physics, because it’s 
not rotationally invariant. 


Problems 


217 


218 Chapter 7 Vectors 


a: 2 6. 2. 6 eb | 
| | 


i2 13 14 15 36 17 15 ip ae 
| @ 





yu E MORSE IN Motion. 
/ / 


MUYBRIDGE 


Chapter 8 
Vectors and Motion 


In 1872, capitalist and former California governor Leland Stanford 
asked photographer Eadweard Muybridge if he would work for him 
on a project to settle a $25,000 bet (a princely sum at that time). 
Stanford’s friends were convinced that a trotting horse always had 
at least one foot on the ground, but Stanford claimed that there was 
a moment during each cycle of the motion when all four feet were 
in the air. The human eye was simply not fast enough to settle the 
question. In 1878, Muybridge finally succeeded in producing what 
amounted to a motion picture of the horse, showing conclusively 
that all four feet did leave the ground at one point. (Muybridge was 
a colorful figure in San Francisco history, and his acquittal for the 
murder of his wife’s lover was considered the trial of the century in 
California. ) 


The losers of the bet had probably been influenced by Aris- 
totelian reasoning, for instance the expectation that a leaping horse 
would lose horizontal velocity while in the air with no force to push 
it forward, so that it would be more efficient for the horse to run 
without leaping. But even for students who have converted whole- 


a 


219 





a/The racing greyhound’s 
velocity vector is in the direction 
of its motion, i.e., tangent to its 
curved path. 








y 


b/ Example 1. 


heartedly to Newtonianism, the relationship between force and ac- 
celeration leads to some conceptual difficulties, the main one being 
a problem with the true but seemingly absurd statement that an 
object can have an acceleration vector whose direction is not the 
same as the direction of motion. The horse, for instance, has nearly 
constant horizontal velocity, so its a, is zero. But as anyone can tell 
you who has ridden a galloping horse, the horse accelerates up and 
down. The horse’s acceleration vector therefore changes back and 
forth between the up and down directions, but is never in the same 
direction as the horse’s motion. In this chapter, we will examine 
more carefully the properties of the velocity, acceleration, and force 
vectors. No new principles are introduced, but an attempt is made 
to tie things together and show examples of the power of the vector 
formulation of Newton’s laws. 


8.1 The velocity vector 
For motion with constant velocity, the velocity vector is 
v = Ar/At. 


The Ar vector points in the direction of the motion, and dividing 
it by the scalar At only changes its length, not its direction, so the 
velocity vector points in the same direction as the motion. When the 
velocity is not constant, i.e., when the x —t, y—t, and z—t graphs 
are not all linear, we use the slope-of-the-tangent-line approach to 
define the components v;, vy, and vz, from which we assemble the 
velocity vector. Even when the velocity vector is not constant, it 
still points along the direction of motion. 


[only for constant velocity] 


Vector addition is the correct way to generalize the one-dimensional 
concept of adding velocities in relative motion, as shown in the fol- 
lowing example: 





‘Velocity vectors in relative motion example 1 
> You wish to cross a river and arrive at a dock that is directly 
across from you, but the river’s current will tend to carry you 
downstream. To compensate, you must steer the boat at an an- 
gle. Find the angle 0, given the magnitude, |Vy,|, of the water’s 
velocity relative to the land, and the maximum speed, |Vgy, of 
which the boat is capable relative to the water. 


> The boat’s velocity relative to the land equals the vector sum of 
its velocity with respect to the water and the water’s velocity with 
respect to the land, 


VeL = Veaw + Vw. 
If the boat is to travel straight across the river, i.e., along the y 


axis, then we need to have Vg,,x = 0. This x component equals 
the sum of the x components of the other two vectors, 


VeL,x = VBw,x + VWLxs 


220 Chapter 8 Vectors and Motion 


or 


O= —|Vew| sin 0 + \Vw_l- 


Solving for 8, we find sin 8 = |Vwz|/|Vgw|, so 





a Seine Vw 
lVew| 
> Solved problem: Annie Oakley page 234, problem 8 
Discussion questions 
A Is it possible for an airplane to maintain a constant velocity vector 


but not a constant |v|? How about the opposite — a constant |v| but not a 
constant velocity vector? Explain. 


B New York and Rome are at about the same latitude, so the earth’s 
rotation carries them both around nearly the same circle. Do the two cities 
have the same velocity vector (relative to the center of the earth)? If not, 
is there any way for two cities to have the same velocity vector? 





The acceleration vector 


When the acceleration is constant, we can define the acceleration 
vector as 


a= Av/At, [only for constant acceleration] 
which can be written in terms of initial and final velocities as 
a= (vy — vy) /At. [only for constant acceleration] 


Otherwise, we can use the type of graphical definition described in 
section 8.1 for the velocity vector. 


Now there are two ways in which we could have a nonzero accel- 
eration. Either the magnitude or the direction of the velocity vector 
could change. This can be visualized with arrow diagrams as shown 
in figures c and d. Both the magnitude and direction can change 
simultaneously, as when a car accelerates while turning. Only when 
the magnitude of the velocity changes while its direction stays con- 
stant do we have a Av vector and an acceleration vector along the 
same line as the motion. 


self-check A 

(1) In figure c, is the object speeding up, or slowing down? (2) What 
would the diagram look like if v; was the same as v;? (3) Describe how 
the Av vector is different depending on whether an object is speeding 
up or slowing down. > Answer, p. 566 





c/A_ change 


in the magni- 


tude of the velocity vector implies 


an acceleration. 





d/A_ change 
of the velocity 


in the direction 
vector also pro- 


duces a nonzero Av vector, and 
thus a nonzero. acceleration 


vector, Av/At. 


Section 8.2 The acceleration vector 


221 


The acceleration vector points in the direction that an accelerom- 
eter would point, as in figure e. 


e/The car has just swerved to 
the right. The air freshener hang- 
ing from the rear-view mirror acts 
as an accelerometer, showing 
that the acceleration vector is to 
the right. 





self-check B 
In projectile motion, what direction does the acceleration vector have? 
> Answer, p. 566 


velocity acceleration force 








™ < + 
\ . . 
| = « 
< < 
x < « 
| = + 
< 
A +] = 
f / Example 2. 
‘Rappelling example 2 


In figure f, the rappeller’s velocity has long periods of gradual 
change interspersed with short periods of rapid change. These 
correspond to periods of small acceleration and force, and peri- 
ods of large acceleration and force. 


222 Chapter 8 Vectors and Motion 

















Av points down —— ee Av points up 


g / Example 3. 





‘The galloping horse example 3 
Figure g on page 223 shows outlines traced from the first, third, 
fifth, seventh, and ninth frames in Muybridge’s series of pho- 
tographs of the galloping horse. The estimated location of the 
horse’s center of mass is shown with a circle, which bobs above 
and below the horizontal dashed line. 


If we don’t care about calculating velocities and accelerations in 
any particular system of units, then we can pretend that the time 
between frames is one unit. The horse’s velocity vector as it 
moves from one point to the next can then be found simply by 
drawing an arrow to connect one position of the center of mass to 
the next. This produces a series of velocity vectors which alter- 
nate between pointing above and below horizontal. 


The Av vector is the vector which we would have to add onto one 
velocity vector in order to get the next velocity vector in the series. 
The Av vector alternates between pointing down (around the time 
when the horse is in the air, B) and up (around the time when the 
horse has two feet on the ground, D). 


Section 8.2 The acceleration vector 








223 








h/ Example 4. 


224 


Discussion questions 


A When acar accelerates, why does a bob hanging from the rearview 
mirror swing toward the back of the car? Is it because a force throws it 
backward? If so, what force? Similarly, describe what happens in the 
other cases described above. 


B Superman is guiding a crippled spaceship into port. The ship’s 
engines are not working. If Superman suddenly changes the direction of 
his force on the ship, does the ship’s velocity vector change suddenly? Its 
acceleration vector? Its direction of motion? 





The force vector and simple machines 


Force is relatively easy to intuit as a vector. The force vector points 
in the direction in which it is trying to accelerate the object it is 
acting on. 


Since force vectors are so much easier to visualize than accel- 
eration vectors, it is often helpful to first find the direction of the 
(total) force vector acting on an object, and then use that to find 
the direction of the acceleration vector. Newton’s second law tells 
us that the two must be in the same direction. 


A component of a force vector example 4 
Figure h, redrawn from a classic 1920 textbook, shows a boy 
pulling another child on a sled. His force has both a horizontal 
component and a vertical one, but only the horizontal one accel- 
erates the sled. (The vertical component just partially cancels the 
force of gravity, causing a decrease in the normal force between 
the runners and the snow.) There are two triangles in the figure. 
One triangle’s hypotenuse is the rope, and the other's is the mag- 
nitude of the force. These triangles are similar, so their internal 
angles are all the same, but they are not the same triangle. One 
is a distance triangle, with sides measured in meters, the other 
a force triangle, with sides in newtons. In both cases, the hori- 
zontal leg is 93% as long as the hypotenuse. It does not make 
sense, however, to compare the sizes of the triangles — the force 
triangle is not smaller in any meaningful sense. 


Chapter 8 Vectors and Motion 


Pushing a block up a ramp example 5 
> Figure i shows a block being pushed up a frictionless ramp at 
constant speed by an externally applied force Fy4. How much 
force is required, in terms of the block’s mass, m, and the angle 
of the ramp, 0? 


> We analyze the forces on the block and introduce notation for 
the other forces besides F,: 





force acting on block 3rd-law partner 

ramp’s normal force on block, | block’s normal force on ramp, 
Fy, a v 
external object's force on | block’s force on external ob- 
block (type irrelevant), Fa « | ject (type irrelevant), y 
planet earth’s gravitational | block’s gravitational force on 
force on block, Fy | earth, + 























Because the block is being pushed up at constant speed, it has 
zero acceleration, and the total force on it must be zero. From 
figure j, we find 


|F | = |Fy| sind 
= mgsin0. 


Since the sine is always less than one, the applied force is always 
less than mg, |.e., pushing the block up the ramp is easier than 
lifting it straight up. This is presumably the principle on which the 
pyramids were constructed: the ancient Egyptians would have 
had a hard time applying the forces of enough slaves to equal the 
full weight of the huge blocks of stone. 


Essentially the same analysis applies to several other simple ma- 
chines, such as the wedge and the screw. 


Fa 


Bais 


i/The applied force F4 pushes 
the block up the frictionless ramp. 


j/lf the block is to move at 
constant velocity, Newton’s first 
law says that the three force 
vectors acting on it must add 
up to zero. To perform vector 
addition, we put the vectors tip 
to tail, and in this case we are 
adding three vectors, so each 
one’s tail goes against the tip of 
the previous one. Since they are 
supposed to add up to zero, the 
third vector’s tip must come back 
to touch the tail of the first vector. 
They form a triangle, and since 
the applied force is perpendicular 
to the normal force, it is a right 
triangle. 


Section 8.3. The force vector and simple machines 225 


k / Example 6 and problem 18 on 
p. 237. 





|/ Example 7. 








A layback example 6 
The figure shows a rock climber using a technique called a lay- 
back. He can make the normal forces Fry; and Fy large, which 
has the side-effect of increasing the frictional forces Fe, and Fro, 
so that he doesn’t slip down due to the gravitational (weight) force 
Fy. The purpose of the problem is not to analyze all of this in de- 
tail, but simply to practice finding the components of the forces 
based on their magnitudes. To keep the notation simple, let's 
write Fry; for |Fyi|, etc. The crack overhangs by a small, positive 
angle 8 + 9°. 


In this example, we determine the x component of Fyy;. The other 
nine components are left as an exercise to the reader (problem 
18, p. 237). 


The easiest method is the one demonstrated in example 5 on 
p. 209. Casting vector F,y;’s shadow on the ground, we can tell 
that it would point to the left, so its x component is negative. The 
only two possibilities for its x component are therefore —Fyy; cos 0 
or —Fyyi sin@. We expect this force to have a large x component 
and a much smaller y. Since 0 is small, cos 9 ~ 1, while sin@ is 
small. Therefore the x component must be — Fy cos 8. 





‘Pushing a broom example 7 
> Figure | shows a man pushing a broom at an angle 0 relative to 
the horizontal. The mass m of the broom is concentrated at the 
brush. If the magnitude of the broom’s acceleration is a, find the 
force F,, that the man must make on the handle. 


> First we analyze all the forces on the brush. 


226 Chapter 8 Vectors and Motion 

















force acting on brush 3rd-law partner 

handle’s normal force brush’s normal force 

on brush, Fy, S| on handle, KR 
earth’s gravitational force brush’s gravitational force 

on brush, mg, { | on earth, ‘i 
floor’s normal force brush’s normal force 

on brush, Fy, + | on floor, L 
floor’s kinetic friction force brush’s kinetic friction force 
on brush, Fx, < | on floor, > 














Newton's second law is: 

Fx +mgQ+ Fy + F; 

aaa m 

where the addition is vector addition. If we actually want to carry 
out the vector addition of the forces, we have to do either graph- 
ical addition (as in example 5) or analytic addition. Let’s do an- 
alytic addition, which means finding all the components of the 
forces, adding the x’s, and adding the y's. 





BI) 


Most of the forces have components that are trivial to express in 
terms of their magnitudes, the exception being Fy, whose com- 
ponents we can determine using the technique demonstrated in 
example 5 on p. 209 and example 6 on p. 226. Using the coordi- 
nate system shown in the figure, the results are: 


Fuy = Fy cos 8 Fry = —F,,sin0 


mgx =0 mgy = —mg 
Fx =0 Fry = Fu 
Fry = —Fr Fry =0 


Note that we don’t yet know the magnitudes Fy, Fry, and Fx. 
That’s all right. First we need to set up Newton’s laws, and then 
we can worry about solving the equations. 


Newton's second law in the x direction gives: 


[1] 


ee Fy,cos 0 — Fx 
_ m 


The acceleration in the vertical direction is zero, so Newton’s sec- 
ond law in the y direction tells us that 


[2] 0 =—Fysin@ —mg+ Fn. 


Finally, we have the relationship between kinetic friction and the 
normal force, 


[3] Fx = ux Fy. 


Equations [1]-[3] are three equations, which we can use to de- 
termine the three unknowns, Fy, Fy, and Fx. Straightforward 


algebra gives 
A+ UG 
Fy =m 
a (ssf ons) 


Section 8.3 The force vector and simple machines 227 





Discussion question A. 


~ 


Discussion question B. 


> Solved problem: A cargo plane page 234, problem 9 
> Solved problem: The angle of repose page 235, problem 11 


> Solved problem: A wagon page 234, problem 10 
Discussion questions 


A __ The figure shows a block being pressed diagonally upward against a 
wall, causing it to slide up the wall. Analyze the forces involved, including 
their directions. 


B_ The figure shows a roller coaster car rolling down and then up under 
the influence of gravity. Sketch the car’s velocity vectors and acceleration 
vectors. Pick an interesting point in the motion and sketch a set of force 
vectors acting on the car whose vector sum could have resulted in the 
right acceleration vector. 





J Calculus with vectors 


Using the unit vector notation introduced in section 7.4, the defini- 
tions of the velocity and acceleration components given in chapter 
6 can be translated into calculus notation as 


yu its ees dz, 
~ at™ " at 





and 


dv; A dvy A dv, A 
a= x Z. 
di" a’ ad 
To make the notation less cumbersome, we generalize the concept 
of the derivative to include derivatives of vectors, so that we can 


abbreviate the above equations as 





dr 
vat 

and 
dv 


In words, to take the derivative of a vector, you take the derivatives 
of its components and make a new vector out of those. This defini- 
tion means that the derivative of a vector function has the familiar 
properties 


def) af 
di. dt 





[c is a constant] 


and 
df+g) df dg 


dt dt dt’ 
The integral of a vector is likewise defined as integrating component 


by component. 





228 Chapter 8 Vectors and Motion 


The second derivative of a vector example 8 
> Two objects have positions as functions of time given by the 
equations 


ry = 32x + ty 
and 
ro = 3t*X% + ty. 
Find both objects’ accelerations using calculus. Could either an- 
swer have been found without calculus? 
> Taking the first derivative of each component, we find 
Vi= 6X + y 
Vo = 120% +y, 
and taking the derivatives again gives acceleration, 
a; = 6X 
ap = 36f°X. 
The first object’s acceleration could have been found without cal- 
culus, simply by comparing the x and y coordinates with the 
constant-acceleration equation Ax = VoAt + sad. The second 
equation, however, isn’t just a second-order polynomial in t, so 


the acceleration isn’t constant, and we really did need calculus to 
find the corresponding acceleration. 


The integral of a vector example 9 
> Starting from rest, a flying saucer of mass m is observed to 
vary its propulsion with mathematical precision according to the 
equation 
F = bt*?x% + ct'®’y. 

(The aliens inform us that the numbers 42 and 137 have a special 
religious significance for them.) Find the saucer’s velocity as a 
function of time. 


> From the given force, we can easily find the acceleration 


a= — 
m 


Dao, C a 
= © 2g 4 = f1379, 
m m 


The velocity vector v is the integral with respect to time of the 
acceleration, 


v= [ade 
= / (Pets <r'srg) dt, 
m m 


Section 8.4 —_[{ Calculus with vectors 


229 


230 


and integrating component by component gives 


= (| Frat) x4 (f Sr°rat) 9 
m m 


439 CC 4138y 
Eagan gic) Ge eee 
4am’ ** 738m! 


where we have omitted the constants of integration, since the 
saucer was starting from rest. 


A fire-extinguisher stunt on ice example 10 
> Prof. Puerile smuggles a fire extinguisher into a skating rink. 
Climbing out onto the ice without any skates on, he sits down and 
pushes off from the wall with his feet, acquiring an initial velocity 
Voy. At tf = 0, he then discharges the fire extinguisher at a 45- 
degree angle so that it applies a force to him that is backward 
and to the left, i.e., along the negative y axis and the positive x 
axis. The fire extinguisher’s force is strong at first, but then dies 
down according to the equation |F| = b — ct, where b and c are 
constants. Find the professor’s velocity as a function of time. 


> Measured counterclockwise from the x axis, the angle of the 
force vector becomes 315°. Breaking the force down into x and 
y components, we have 
Fy = |F| cos 315° 
= (b— ct) 
Fy = |F|sin315° 
= (—b+ct). 
In unit vector notation, this is 
F =(b—ct)X+(—b+ct)y. 


Newton’s second law gives 





a=F/m 
b-—ct, —b+ect. 
= X+ 
J/2m J/2m 


To find the velocity vector as a function of time, we need to inte- 
grate the acceleration vector with respect to time, 


v= [adt 
= [ (Aare a) at 
7 J/2m J/2m 


2 aq | Ween + (b+ ct) y] dt 





Chapter 8 Vectors and Motion 


A vector function can be integrated component by component, so 
this can be broken down into two integrals, 


A 


x y 
v = —— /(b-ct)dt + [Co+ena 
v2m [ v2m ( ) 
bia Sor =pbt+ der 
= {7 2°" st constant #1) x+ {= 2"" + constant #2 | y 
JV2m J2m 
Here the physical significance of the two constants of integration 
is that they give the initial velocity. Constant #1 is therefore zero, 
and constant #2 must equal Vo. The final result is 


ey ae 5ct? PO feels set? es 
= |) —— —— | xt [| + 





Section 8.4 —_[{ Calculus with vectors 


231 


Summary 


The velocity vector points in the direction of the object’s motion. 
Relative motion can be described by vector addition of velocities. 


The acceleration vector need not point in the same direction as 
the object’s motion. We use the word “acceleration” to describe any 
change in an object’s velocity vector, which can be either a change 
in its magnitude or a change in its direction. 


An important application of the vector addition of forces is the 
use of Newton’s first law to analyze mechanical systems. 


232 Chapter 8 Vectors and Motion 


Problems 
Key 


VA computerized answer check is available online. 
{A problem that requires calculus. 
x A difficult problem. 


north ‘ 
‘ 
* direction of motion 
m of glacier relative 
‘\ y 
sooo sto continent, 1.1x10 Tmls 
ss nes ‘ 
See a ‘ 
= ‘ 
direction of motion a 
of fossil relative to Mi 
glacier, 2.3x10 -7 mis A 
Problem 1. 
1 As shown in the diagram, a dinosaur fossil is slowly moving 


down the slope of a glacier under the influence of wind, rain and 
gravity. At the same time, the glacier is moving relative to the 
continent underneath. The dashed lines represent the directions but 
not the magnitudes of the velocities. Pick a scale, and use graphical 
addition of vectors to find the magnitude and the direction of the 
fossil’s velocity relative to the continent. You will need a ruler and 
protractor. Vv 


2 Is it possible for a helicopter to have an acceleration due east 
and a velocity due west? If so, what would be going on? If not, why 
not? 


3 A bird is initially flying horizontally east at 21.1 m/s, but one 
second later it has changed direction so that it is flying horizontally 
and 7° north of east, at the same speed. What are the magnitude 
and direction of its acceleration vector during that one second time 
interval? (Assume its acceleration was roughly constant.) Vv 





Problem 4. 


4 A person of mass M stands in the middle of a tightrope, 
which is fixed at the ends to two buildings separated by a horizontal 
distance L. The rope sags in the middle, stretching and lengthening 
the rope slightly. 


Problems 


233 


(a) If the tightrope walker wants the rope to sag vertically by no 
more than a height h, find the minimum tension, T’, that the rope 
must be able to withstand without breaking, in terms of h, g, M, 
and L. Vv 
(b) Based on your equation, explain why it is not possible to get 
h = 0, and give a physical interpretation. 


5 Your hand presses a block of mass m against a wall with a 
force Fy acting at an angle 0, as shown in the figure. Find the 
minimum and maximum possible values of |Fy| that can keep the 
block stationary, in terms of m, g, 0, and ps, the coefficient of static 
friction between the block and the wall. Check both your answers 
in the case of 9 = 90°, and interpret the case where the maximum 
Problem 5. force is infinite. Vox 





6 A skier of mass m is coasting down a slope inclined at an angle 
? compared to horizontal. Assume for simplicity that the treatment 
of kinetic friction given in chapter 5 is appropriate here, although a 
soft and wet surface actually behaves a little differently. The coeffi- 
cient of kinetic friction acting between the skis and the snow is pz, 
and in addition the skier experiences an air friction force of magni- 
tude bv”, where b is a constant. 

(a) Find the maximum speed that the skier will attain, in terms of 
the variables m, g, 0, 4x, and b. v 
(b) For angles below a certain minimum angle 0,,;,, the equation 
gives a result that is not mathematically meaningful. Find an equa- 
tion for Omin, and give a physical explanation of what is happening 
for 8 < Omin- 


7 A gun is aimed horizontally to the west. The gun is fired, and 
the bullet leaves the muzzle at t = 0. The bullet’s position vector 
as a function of time is r = bx + cty + dt?z, where b, c, and d are 
positive constants. 

(a) What units would b, c, and d need to have for the equation to 
make sense? 

(b) Find the bullet’s velocity and acceleration as functions of time. 
(c) Give physical interpretations of b, c, d, X, y, and @. if 


Fthrust Fiitt 





Problem 9. 


8 Annie Oakley, riding north on horseback at 30 mi/hr, shoots 
| her rifle, aiming horizontally and to the northeast. The muzzle speed 
of the rifle is 140 mi/hr. When the bullet hits a defenseless fuzzy 
animal, what is its speed of impact? Neglect air resistance, and 
ignore the vertical motion of the bullet. > Solution, p. 554 


9 A cargo plane has taken off from a tiny airstrip in the Andes, 
and is climbing at constant speed, at an angle of 0 = 17° with 
respect to horizontal. Its engines supply a thrust of Fihrust = 200 
KN, and the lift from its wings is Fi, = 654 kN. Assume that air 
Problem 10. resistance (drag) is negligible, so the only forces acting are thrust, 
lift, and weight. What is its mass, in kg? > Solution, p. 554 


234 Chapter 8 Vectors and Motion 


10 A wagon is being pulled at constant speed up a slope 6 by a 
rope that makes an angle ¢ with the vertical. 

(a) Assuming negligible friction, show that the tension in the rope 
is given by the equation 


sin 6 
ee 
rT sinfo+¢) 


where Fi is the weight force acting on the wagon. 
(b) Interpret this equation in the special cases of ¢ = 0 and ¢ = 
180° — 0. > Solution, p. 555 


11 The angle of repose is the maximum slope on which an object 
will not slide. On airless, geologically inert bodies like the moon or 
an asteroid, the only thing that determines whether dust or rubble 
will stay on a slope is whether the slope is less steep than the angle 
of repose. (See figure n, p. 272.) 

(a) Find an equation for the angle of repose, deciding for yourself 
what are the relevant variables. 

(b) On an asteroid, where g can be thousands of times lower than 
on Earth, would rubble be able to lie at a steeper angle of repose? 

> Solution, p. 555 


12 The figure shows an experiment in which a cart is released 
from rest at A, and accelerates down the slope through a distance 
x until it passes through a sensor’s light beam. The point of the 
experiment is to determine the cart’s acceleration. At B, a card- 
board vane mounted on the cart enters the light beam, blocking the 
light beam, and starts an electronic timer running. At C, the vane 
emerges from the beam, and the timer stops. 

(a) Find the final velocity of the cart in terms of the width w of 
the vane and the time t, for which the sensor’s light beam was 


blocked. Vv 
(b) Find the magnitude of the cart’s acceleration in terms of the 
measurable quantities x, ty, and w. Vv 


(c) Analyze the forces in which the cart participates, using a table in 
the format introduced in section 5.3. Assume friction is negligible. 
(d) Find a theoretical value for the acceleration of the cart, which 
could be compared with the experimentally observed value extracted 
in part b. Express the theoretical value in terms of the angle 0 of 
the slope, and the strength g of the gravitational field. Vv 


13 The figure shows a boy hanging in three positions: (1) with 
his arms straight up, (2) with his arms at 45 degrees, and (3) with 
his arms at 60 degrees with respect to the vertical. Compare the 
tension in his arms in the three cases. 


sensor 


WwW 


Bec 


Problem 12. 








Problem 13 (Millikan and Gale, 
1920). 


Problems 235 





Problem 15. 





to climber 
Problem 16. 
a 
Problem 17. 


14 Driving down a hill inclined at an angle @ with respect to 
horizontal, you slam on the brakes to keep from hitting a deer. Your 
antilock brakes kick in, and you don’t skid. 

(a) Analyze the forces. (Ignore rolling resistance and air friction.) 
(b) Find the car’s maximum possible deceleration, a (expressed as 
a positive number), in terms of g, 0, and the relevant coefficient of 
friction. Vv 
(c) Explain physically why the car’s mass has no effect on your 
answer. 

(d) Discuss the mathematical behavior and physical interpretation 
of your result for negative values of 0. 

(ce) Do the same for very large positive values of 0. 


15 The figure shows the path followed by Hurricane Irene in 
2005 as it moved north. The dots show the location of the center 
of the storm at six-hour intervals, with lighter dots at the time 
when the storm reached its greatest intensity. Find the time when 
the storm’s center had a velocity vector to the northeast and an 
acceleration vector to the southeast. Explain. 
There are two main approaches to understanding the motion of
objects, one based on force and one on a diﬀerent concept, called en-
ergy. The SI unit of energy is the Joule, but you are probably more
familiar with the calorie, used for measuring food’s energy, and the
kilowatt-hour, the unit the electric company uses for billing you.
Physics students’ previous familiarity with calories and kilowatt-
hours is matched by their universal unfamiliarity with measuring
forces in units of Newtons, but the precise operational deﬁnitions of
the energy concepts are more complex than those of the force con-
cepts, and textbooks, including this one, almost universally place the
force description of physics before the energy description. During
the long period after the introduction of force and before the careful
deﬁnition of energy, students are therefore vulnerable to situations
in which, without realizing it, they are imputing the properties of
energy to phenomena of force.

Incorrect statement: “How can my chair be making an upward force on
my rear end? It has no power!”

Power is a concept related to energy, e.g., a 100-watt lightbulb uses

Section 4.4 What force is not

141

up 100 joules per second of energy. When you sit in a chair, no energy
is used up, so forces can exist between you and the chair without any
need for a source of power.

4. Force is not stored or used up.

Because energy can be stored and used up, people think force

also can be stored or used up.

Incorrect statement: “If you don’t ﬁll up your tank with gas, you’ll run
out of force.”

Energy is what you’ll run out of, not force.

5. Forces need not be exerted by living things or machines.

Transforming energy from one form into another usually requires
some kind of living or mechanical mechanism. The concept is not
applicable to forces, which are an interaction between objects, not
a thing to be transferred or transformed.

Incorrect statement: “How can a wooden bench be making an upward
force on my rear end? It doesn’t have any springs or anything inside it.”

No springs or other internal mechanisms are required.

If the bench
didn’t make any force on you, you would obey Newton’s second law and
fall through it. Evidently it does make a force on you!

6. A force is the direct cause of a change in motion.

I can click a remote control to make my garage door change from
being at rest to being in motion. My ﬁnger’s force on the button,
however, was not the force that acted on the door. When we speak
of a force on an object in physics, we are talking about a force that
acts directly. Similarly, when you pull a reluctant dog along by its
leash, the leash and the dog are making forces on each other, not
your hand and the dog. The dog is not even touching your hand.

self-check B
Which of the following things can be correctly described in terms of
force?

(1) A nuclear submarine is charging ahead at full steam.

(2) A nuclear submarine’s propellers spin in the water.

(3) A nuclear submarine needs to refuel its reactor periodically.
Answer, p. 565

(cid:46)

Discussion questions

Criticize the following incorrect statement: “If you shove a book
A
across a table, friction takes away more and more of its force, until ﬁnally
it stops.”

You hit a tennis ball against a wall. Explain any and all incorrect
B
ideas in the following description of the physics involved: “The ball gets
some force from you when you hit it, and when it hits the wall, it loses part
of that force, so it doesn’t bounce back as fast. The muscles in your arm
are the only things that a force can come from.”

142

Chapter 4

Force and Motion

4.5 Inertial and noninertial frames of reference

One day, you’re driving down the street in your pickup truck, on
your way to deliver a bowling ball. The ball is in the back of the
truck, enjoying its little jaunt and taking in the fresh air and sun-
shine. Then you have to slow down because a stop sign is coming
up. As you brake, you glance in your rearview mirror, and see your
trusty companion accelerating toward you. Did some mysterious
force push it forward? No, it only seems that way because you and
the car are slowing down. The ball is faithfully obeying Newton’s
ﬁrst law, and as it continues at constant velocity it gets ahead rela-
tive to the slowing truck. No forces are acting on it (other than the
same canceling-out vertical forces that were always acting on it).4
The ball only appeared to violate Newton’s ﬁrst law because there
was something wrong with your frame of reference, which was based
on the truck.

k / 1. In a frame of reference that
moves with the truck, the bowl-
ing ball appears to violate New-
ton’s ﬁrst law by accelerating de-
spite having no horizontal forces
on it. 2. In an inertial frame of ref-
erence, which the surface of the
earth approximately is, the bowl-
ing ball obeys Newton’s ﬁrst law.
It moves equal distances in equal
time intervals, i.e., maintains con-
stant velocity.
In this frame of
reference, it is the truck that ap-
pears to have a change in veloc-
ity, which makes sense, since the
road is making a horizontal force
on it.

How, then, are we to tell in which frames of reference Newton’s
laws are valid? It’s no good to say that we should avoid moving
frames of reference, because there is no such thing as absolute rest
or absolute motion. All frames can be considered as being either at
rest or in motion. According to an observer in India, the strip mall
that constituted the frame of reference in panel (b) of the ﬁgure
was moving along with the earth’s rotation at hundreds of miles per
hour.

The reason why Newton’s laws fail in the truck’s frame of refer-

4Let’s assume for simplicity that there is no friction.

Section 4.5

Inertial and noninertial frames of reference

143

ence is not because the truck is moving but because it is accelerating.
(Recall that physicists use the word to refer either to speeding up or
slowing down.) Newton’s laws were working just ﬁne in the moving
truck’s frame of reference as long as the truck was moving at con-
stant velocity. It was only when its speed changed that there was
a problem. How, then, are we to tell which frames are accelerating
and which are not? What if you claim that your truck is not ac-
celerating, and the sidewalk, the asphalt, and the Burger King are
accelerating? The way to settle such a dispute is to examine the
motion of some object, such as the bowling ball, which we know
has zero total force on it. Any frame of reference in which the ball
appears to obey Newton’s ﬁrst law is then a valid frame of reference,
and to an observer in that frame, Mr. Newton assures us that all
the other objects in the universe will obey his laws of motion, not
just the ball.

Valid frames of reference, in which Newton’s laws are obeyed,
are called inertial frames of reference. Frames of reference that are
not inertial are called noninertial frames. In those frames, objects
violate the principle of inertia and Newton’s ﬁrst law. While the
truck was moving at constant velocity, both it and the sidewalk
were valid inertial frames. The truck became an invalid frame of
reference when it began changing its velocity.

You usually assume the ground under your feet is a perfectly
inertial frame of reference, and we made that assumption above. It
isn’t perfectly inertial, however. Its motion through space is quite
complicated, being composed of a part due to the earth’s daily rota-
tion around its own axis, the monthly wobble of the planet caused
by the moon’s gravity, and the rotation of the earth around the sun.
Since the accelerations involved are numerically small, the earth is
approximately a valid inertial frame.

Noninertial frames are avoided whenever possible, and we will
seldom, if ever, have occasion to use them in this course. Sometimes,
however, a noninertial frame can be convenient. Naval gunners, for
instance, get all their data from radars, human eyeballs, and other
detection systems that are moving along with the earth’s surface.
Since their guns have ranges of many miles, the small discrepan-
cies between their shells’ actual accelerations and the accelerations
predicted by Newton’s second law can have eﬀects that accumulate
and become signiﬁcant. In order to kill the people they want to kill,
they have to add small corrections onto the equation a = Ftotal/m.
Doing their calculations in an inertial frame would allow them to
use the usual form of Newton’s second law, but they would have
to convert all their data into a diﬀerent frame of reference, which
would require cumbersome calculations.

144

Chapter 4

Force and Motion

Discussion question

If an object has a linear x − t graph in a certain inertial frame,
A
what is the effect on the graph if we change to a coordinate system with
a different origin? What is the effect if we keep the same origin but re-
verse the positive direction of the x axis? How about an inertial frame
moving alongside the object? What if we describe the object’s motion in
a noninertial frame?

Section 4.5

Inertial and noninertial frames of reference

145

Summary

Selected vocabulary

weight . . . . . . .
inertial frame . .

noninertial frame

the force of gravity on an object, equal to mg
a frame of reference that is not accelerating,
one in which Newton’s ﬁrst law is true
an accelerating frame of reference, in which
Newton’s ﬁrst law is violated

Notation
FW . . . . . . . . weight

Other terminology and notation

net force . . . . .

another way of saying “total force”

Summary

Newton’s ﬁrst law of motion states that if all the forces acting
on an object cancel each other out, then the object continues in the
same state of motion. This is essentially a more reﬁned version of
Galileo’s principle of inertia, which did not refer to a numerical scale
of force.

Newton’s second law of motion allows the prediction of an ob-
ject’s acceleration given its mass and the total force on it, acm =
Ftotal/m. This is only the one-dimensional version of the law; the
full-three dimensional treatment will come in chapter 8, Vectors.
Without the vector techniques, we can still say that the situation
remains unchanged by including an additional set of vectors that
cancel among themselves, even if they are not in the direction of
motion.

Newton’s laws of motion are only true in frames of reference that

are not accelerating, known as inertial frames.

146

Chapter 4

Force and Motion

Problems

A computerized answer check is available online.

Key√
(cid:82) A problem that requires calculus.
(cid:63) A diﬃcult problem.

1
An object is observed to be moving at constant speed in a
certain direction. Can you conclude that no forces are acting on it?
Explain. [Based on a problem by Serway and Faughn.]

2
At low speeds, every car’s acceleration is limited by traction,
not by the engine’s power. Suppose that at low speeds, a certain
car is normally capable of an acceleration of 3 m/s2. If it is towing
a trailer with half as much mass as the car itself, what acceleration
can it achieve? [Based on a problem from PSSC Physics.]

3
(a) Let T be the maximum tension that an elevator’s cable
can withstand without breaking, i.e., the maximum force it can
exert. If the motor is programmed to give the car an acceleration
a (a > 0 is upward), what is the maximum mass that the car can
√
have, including passengers, if the cable is not to break?
(b) Interpret the equation you derived in the special cases of a = 0
and of a downward acceleration of magnitude g. (“Interpret” means
to analyze the behavior of the equation, and connect that to reality,
as in the self-check on page 139.)

4
A helicopter of mass m is taking oﬀ vertically. The only forces
acting on it are the earth’s gravitational force and the force, Fair,
of the air pushing up on the propeller blades.
(a) If the helicopter lifts oﬀ at t = 0, what is its vertical speed at
time t?
(b) Check that the units of your answer to part a make sense.
(c) Discuss how your answer to part a depends on all three variables,
and show that it makes sense. That is, for each variable, discuss
what would happen to the result if you changed it while keeping the
other two variables constant. Would a bigger value give a smaller
result, or a bigger result? Once you’ve ﬁgured out this mathematical
relationship, show that it makes sense physically.
(d) Plug numbers into your equation from part a, using m = 2300
kg, Fair = 27000 N, and t = 4.0 s.

√

5
In the 1964 Olympics in Tokyo, the best men’s high jump was
2.18 m. Four years later in Mexico City, the gold medal in the same
event was for a jump of 2.24 m. Because of Mexico City’s altitude
(2400 m), the acceleration of gravity there is lower than that in
Tokyo by about 0.01 m/s2. Suppose a high-jumper has a mass of
72 kg.
(a) Compare his mass and weight in the two locations.
(b) Assume that he is able to jump with the same initial vertical

Problems

147

Problem 6.

velocity in both locations, and that all other conditions are the same
except for gravity. How much higher should he be able to jump in
√
Mexico City?
(Actually, the reason for the big change between ’64 and ’68 was the
(cid:63)
introduction of the “Fosbury ﬂop.”)

6
A blimp is initially at rest, hovering, when at t = 0 the pilot
turns on the engine driving the propeller. The engine cannot in-
stantly get the propeller going, but the propeller speeds up steadily.
The steadily increasing force between the air and the propeller is
given by the equation F = kt, where k is a constant. If the mass
of the blimp is m, ﬁnd its position as a function of time. (Assume
that during the period of time you’re dealing with, the blimp is not
yet moving fast enough to cause a signiﬁcant backward force due to
air resistance.)

√ (cid:82)

7
A car is accelerating forward along a straight road. If the force
of the road on the car’s wheels, pushing it forward, is a constant 3.0
kN, and the car’s mass is 1000 kg, then how long will the car take
to go from 20 m/s to 50 m/s?

(cid:46) Solution, p. 551

Some garden shears are like a pair of scissors: one sharp blade
8
slices past another.
In the “anvil” type, however, a sharp blade
presses against a ﬂat one rather than going past it. A gardening
book says that for people who are not very physically strong, the
anvil type can make it easier to cut tough branches, because it
concentrates the force on one side. Evaluate this claim based on
Newton’s laws.
[Hint: Consider the forces acting on the branch,
and the motion of the branch.]

9
A uranium atom deep in the earth spits out an alpha particle.
An alpha particle is a fragment of an atom. This alpha particle has
initial speed v, and travels a distance d before stopping in the earth.
(a) Find the force, F , from the dirt that stopped the particle, in
terms of v, d, and its mass, m. Don’t plug in any numbers yet.
√
Assume that the force was constant.
(b) Show that your answer has the right units.
(c) Discuss how your answer to part a depends on all three variables,
and show that it makes sense. That is, for each variable, discuss
what would happen to the result if you changed it while keeping the
other two variables constant. Would a bigger value give a smaller
result, or a bigger result? Once you’ve ﬁgured out this mathematical
relationship, show that it makes sense physically.
(d) Evaluate your result for m = 6.7 × 10−27 kg, v = 2.0 × 104 km/s,
√
and d = 0.71 mm.

148

Chapter 4

Force and Motion

10
You are given a large sealed box, and are not allowed to open
it. Which of the following experiments measure its mass, and which
measure its weight? [Hint: Which experiments would give diﬀerent
results on the moon?]
(a) Put it on a frozen lake, throw a rock at it, and see how fast it
scoots away after being hit.
(b) Drop it from a third-ﬂoor balcony, and measure how loud the
sound is when it hits the ground.
(c) As shown in the ﬁgure, connect it with a spring to the wall, and
watch it vibrate.

(cid:46) Solution, p. 551

While escaping from the palace of the evil Martian em-
11
peror, Sally Spacehound jumps from a tower of height h down to
the ground. Ordinarily the fall would be fatal, but she ﬁres her
blaster riﬂe straight down, producing an upward force of magnitude
FB. This force is insuﬃcient to levitate her, but it does cancel out
some of the force of gravity. During the time t that she is falling,
Sally is unfortunately exposed to ﬁre from the emperor’s minions,
and can’t dodge their shots. Let m be her mass, and g the strength
of gravity on Mars.
(a) Find the time t in terms of the other variables.
(b) Check the units of your answer to part a.
(c) For suﬃciently large values of FB, your answer to part a becomes
√
nonsense — explain what’s going on.

12 When I cook rice, some of the dry grains always stick to the
measuring cup. To get them out, I turn the measuring cup upside-
down and hit the “roof” with my hand so that the grains come oﬀ of
the “ceiling.” (a) Explain why static friction is irrelevant here. (b)
Explain why gravity is negligible. (c) Explain why hitting the cup
works, and why its success depends on hitting the cup hard enough.

13
At the turn of the 20th century, Samuel Langley engaged in
a bitter rivalry with the Wright brothers to develop human ﬂight.
Langley’s design used a catapult for launching. For safety, the cata-
pult was built on the roof of a houseboat, so that any crash would be
into the water. This design required reaching cruising speed within
a ﬁxed, short distance, so large accelerations were required, and
the forces frequently damaged the craft, causing dangerous and em-
barrassing accidents. Langley achieved several uncrewed, unguided
ﬂights, but never succeeded with a human pilot. If the force of the
catapult is ﬁxed by the structural strength of the plane, and the dis-
tance for acceleration by the size of the houseboat, by what factor
is the launch velocity reduced when the plane’s 340 kg is augmented
by the 60 kg mass of a small man?

√

Problem 10, part c.

Problem 13.
The rear wings
of the plane collapse under the
stress of the catapult launch.

Problems

149

14
The tires used in Formula 1 race cars can generate traction
(i.e., force from the road) that is as much as 1.9 times greater than
with the tires typically used in a passenger car. Suppose that we’re
trying to see how fast a car can cover a ﬁxed distance starting from
rest, and traction is the limiting factor. By what factor is this time
reduced when switching from ordinary tires to Formula 1 tires?
√

15
In the ﬁgure, the rock climber has ﬁnished the climb, and his
partner is lowering him back down to the ground at approximately
constant speed. The following is a student’s analysis of the forces
acting on the climber. The arrows give the directions of the forces.

force of the earth’s gravity, ↓
force from the partner’s hands, ↑
force from the rope, ↑

The student says that since the climber is moving down, the sum
of the two upward forces must be slightly less than the downward
force of gravity.

Correct all mistakes in the above analysis.

(cid:46) Solution, p. 551

A bullet of mass m is ﬁred from a pistol, accelerating from

16
rest to a speed v in the barrel’s length L.
(a) What is the force on the bullet? (Assume this force is constant.)
√

(b) Check that the units of your answer to part a make sense.
(c) Check that the dependence of your answer on each of the three
variables makes sense.

[problem by B. Shotwell]

Blocks of mass M1, M2, and M3 are stacked on a table as

17
shown in the ﬁgure. Let the upward direction be positive.
(a) What is the force on block 2 from block 3?
(b) What is the force on block 2 from block 1?

√

√

[problem by B. Shotwell]

Problem 15.

Problem 17.

150

Chapter 4

Force and Motion

Exercise 4: Force and motion

Equipment:

1-meter pieces of butcher paper

wood blocks with hooks

string

masses to put on top of the blocks to increase friction

spring scales (preferably calibrated in Newtons)

Suppose a person pushes a crate, sliding it across the ﬂoor at a certain speed, and then repeats
the same thing but at a higher speed. This is essentially the situation you will act out in this
exercise. What do you think is diﬀerent about her force on the crate in the two situations?
Discuss this with your group and write down your hypothesis:

1. First you will measure the amount of friction between the wood block and the butcher paper
when the wood and paper surfaces are slipping over each other. The idea is to attach a spring
scale to the block and then slide the butcher paper under the block while using the scale to
keep the block from moving with it. Depending on the amount of force your spring scale was
designed to measure, you may need to put an extra mass on top of the block in order to increase
the amount of friction. It is a good idea to use long piece of string to attach the block to the
spring scale, since otherwise one tends to pull at an angle instead of directly horizontally.

First measure the amount of friction force when sliding the butcher paper as slowly as possi-
ble:

Now measure the amount of friction force at a signiﬁcantly higher speed, say 1 meter per second.
(If you try to go too fast, the motion is jerky, and it is impossible to get an accurate reading.)

Discuss your results. Why are we justiﬁed in assuming that the string’s force on the block (i.e.,
the scale reading) is the same amount as the paper’s frictional force on the block?

2. Now try the same thing but with the block moving and the paper standing still. Try two
diﬀerent speeds.

Do your results agree with your original hypothesis? If not, discuss what’s going on. How does
the block “know” how fast to go?

Exercise 4: Force and motion

151

152

Chapter 4

Force and Motion

Chapter 5
Analysis of Forces

5.1 Newton’s third law

Newton created the modern concept of force starting from his insight
that all the eﬀects that govern motion are interactions between two
objects: unlike the Aristotelian theory, Newtonian physics has no
phenomena in which an object changes its own motion.

What forces act on the girl?

153

Is one object always the “order-giver” and the other the “order-
follower”? As an example, consider a batter hitting a baseball. The
bat deﬁnitely exerts a large force on the ball, because the ball ac-
celerates drastically. But if you have ever hit a baseball, you also
know that the ball makes a force on the bat — often with painful
results if your technique is as bad as mine!

How does the ball’s force on the bat compare with the bat’s
force on the ball? The bat’s acceleration is not as spectacular as
the ball’s, but maybe we shouldn’t expect it to be, since the bat’s
mass is much greater. In fact, careful measurements of both objects’
masses and accelerations would show that mballaball is very nearly
equal to −mbatabat, which suggests that the ball’s force on the bat
is of the same magnitude as the bat’s force on the ball, but in the
opposite direction.

Figures a and b show two somewhat more practical laboratory
experiments for investigating this issue accurately and without too
much interference from extraneous forces.

In experiment a, a large magnet and a small magnet are weighed
separately, and then one magnet is hung from the pan of the top
balance so that it is directly above the other magnet. There is an
attraction between the two magnets, causing the reading on the top
scale to increase and the reading on the bottom scale to decrease.
The large magnet is more “powerful” in the sense that it can pick
up a heavier paperclip from the same distance, so many people have
a strong expectation that one scale’s reading will change by a far
Instead, we ﬁnd that the two
diﬀerent amount than the other.
changes are equal in magnitude but opposite in direction: the force
of the bottom magnet pulling down on the top one has the same
strength as the force of the top one pulling up on the bottom one.

In experiment b, two people pull on two spring scales. Regardless
of who tries to pull harder, the two forces as measured on the spring
scales are equal. Interposing the two spring scales is necessary in
order to measure the forces, but the outcome is not some artiﬁcial
result of the scales’ interactions with each other. If one person slaps
another hard on the hand, the slapper’s hand hurts just as much
as the slappee’s, and it doesn’t matter if the recipient of the slap
tries to be inactive. (Punching someone in the mouth causes just
as much force on the ﬁst as on the lips. It’s just that the lips are
more delicate. The forces are equal, but not the levels of pain and
injury.)

Newton, after observing a series of results such as these, decided

that there must be a fundamental law of nature at work:

a / Two magnets exert
on each other.

forces

b / Two people’s hands exert
forces on each other.

c / Rockets work by pushing
exhaust gases out
the back.
Newton’s third law says that if the
rocket exerts a backward force
on the gases,
the gases must
make an equal forward force on
the rocket. Rocket engines can
function above the atmosphere,
unlike propellers and jets, which
the
work by pushing against
surrounding air.

154

Chapter 5 Analysis of Forces

Newton’s third law

Forces occur in equal and opposite pairs: whenever object A exerts
a force on object B, object B must also be exerting a force on object
A. The two forces are equal in magnitude and opposite in direction.

Two modern, high-precision tests of the third law are described

on p. 806.

In one-dimensional situations, we can use plus and minus signs
to indicate the directions of forces, and Newton’s third law can be
written succinctly as FA on B = −FB on A.

self-check A
Figure d analyzes swimming using Newton’s third law. Do a similar
(cid:46) Answer, p. 565
analysis for a sprinter leaving the starting line.

There is no cause and eﬀect relationship between the two forces
in Newton’s third law. There is no “original” force, and neither one
is a response to the other. The pair of forces is a relationship, like
marriage, not a back-and-forth process like a tennis match. Newton
came up with the third law as a generalization about all the types of
forces with which he was familiar, such as frictional and gravitational
forces. When later physicists discovered a new type of force, such
as the force that holds atomic nuclei together, they had to check
whether it obeyed Newton’s third law. So far, no violation of the
third law has ever been discovered, whereas the ﬁrst and second
laws were shown to have limitations by Einstein and the pioneers of
atomic physics.

The English vocabulary for describing forces is unfortunately
rooted in Aristotelianism, and often implies incorrectly that forces
are one-way relationships. It is unfortunate that a half-truth such as
“the table exerts an upward force on the book” is so easily expressed,
while a more complete and correct description ends up sounding
awkward or strange: “the table and the book interact via a force,”
or “the table and book participate in a force.”

To students, it often sounds as though Newton’s third law im-
plies nothing could ever change its motion, since the two equal and
opposite forces would always cancel. The two forces, however, are
always on two diﬀerent objects, so it doesn’t make sense to add
them in the ﬁrst place — we only add forces that are acting on the
same object. If two objects are interacting via a force and no other
forces are involved, then both objects will accelerate — in opposite
directions!

d / A swimmer doing the breast
stroke pushes backward against
the water. By Newton’s third law,
the water pushes forward on him.

e / Newton’s
law does
third
not mean that forces always can-
cel out so that nothing can ever
move.
If these two ice skaters,
initially at rest, push against each
other, they will both move.

Section 5.1 Newton’s third law

155

f / It doesn’t make sense for the
man to talk about using the
woman’s money to cancel out his
bar tab, because there is no good
reason to combine his debts and
her assets. Similarly, it doesn’t
make sense to refer to the equal
and opposite forces of Newton’s
third law as canceling.
It only
makes sense to add up forces
that are acting on the same ob-
ject, whereas two forces related
to each other by Newton’s third
law are always acting on two dif-
ferent objects.

A mnemonic for using Newton’s third law correctly

Mnemonics are tricks for memorizing things. For instance, the
musical notes that lie between the lines on the treble clef spell the
word FACE, which is easy to remember. Many people use the
mnemonic “SOHCAHTOA” to remember the deﬁnitions of the sine,
cosine, and tangent in trigonometry. I have my own modest oﬀering,
POFOSTITO, which I hope will make it into the mnemonics hall of
fame. It’s a way to avoid some of the most common problems with
applying Newton’s third law correctly:

example 1
A book lying on a table
(cid:46) A book is lying on a table. What force is the Newton’s-third-law
partner of the earth’s gravitational force on the book?

Answer: Newton’s third law works like “B on A, A on B,” so the
partner must be the book’s gravitational force pulling upward on
the planet earth. Yes, there is such a force! No, it does not cause
the earth to do anything noticeable.

Incorrect answer: The table’s upward force on the book is the
Newton’s-third-law partner of the earth’s gravitational force on the
book.

This answer violates two out of three of the commandments of
POFOSTITO. The forces are not of the same type, because the
table’s upward force on the book is not gravitational. Also, three

156

Chapter 5 Analysis of Forces

objects are involved instead of two: the book, the table, and the
planet earth.

Pushing a box up a hill
example 2
(cid:46) A person is pushing a box up a hill. What force is related by
Newton’s third law to the person’s force on the box?

(cid:46) The box’s force on the person.

Incorrect answer: The person’s force on the box is opposed by
friction, and also by gravity.

This answer fails all three parts of the POFOSTITO test, the
most obvious of which is that three forces are referred to instead
of a pair.

If we could violate Newton’s third law. . .
example 3
If we could violate Newton’s third law, we could do strange and
wonderful things. Newton’s third laws says that the unequal mag-
nets in ﬁgure a on p. 154 should exert equal forces on each
other, and this is what we actually ﬁnd when we do the experi-
ment shown in that ﬁgure. But suppose instead that it worked as
most people intuitively expect. What if the third law was violated,
so that the big magnet made more force on the small one than the
small one made on the big one? To make the analysis simple, we
add some extra nonmagnetic material to the small magnet in ﬁg-
ure g/1, so that it has the same mass and size as the big one. We
also attach springs. When we release the magnets, g/2, the weak
one is accelerated strongly, while the strong one barely moves. If
we put them inside a box, g/3, the recoiling strong magnet bangs
hard against the side of the box, and the box mysteriously accel-
erates itself. The process can be repeated indeﬁnitely for free, so
we have a magic box that propels itself without needing fuel. We
can make it into a perpetual-motion car, g/4. If Newton’s third law
was violated, we’d never have to pay for gas!

Optional

topic: Newton’s
third law and action at a dis-
tance
Newton’s third law is completely
symmetric in the sense that nei-
ther force constitutes a delayed
response to the other. Newton’s
third law does not even mention
time, and the forces are supposed
to agree at any given instant.
This creates an interesting situ-
ation when it comes to noncon-
tact forces. Suppose two people
are holding magnets, and when
one person waves or wiggles her
magnet, the other person feels an
effect on his.
In this way they
can send signals to each other
from opposite sides of a wall, and
if Newton’s third law is correct, it
would seem that the signals are
transmitted instantly, with no time
lag. The signals are indeed trans-
mitted quite quickly, but experi-
ments with electrically controlled
magnets show that the signals do
not leap the gap instantly:
they
travel at the same speed as light,
which is an extremely high speed
but not an inﬁnite one.

Is this a contradiction to New-
ton’s third law? Not really. Ac-
cording to current theories, there
are no true noncontact
forces.
Action at a distance does not ex-
ist. Although it appears that the
wiggling of one magnet affects
the other with no need for any-
thing to be in contact with any-
thing, what really happens is that
wiggling a magnet creates a rip-
ple in the magnetic ﬁeld pattern
that exists even in empty space.
The magnet shoves the ripples
out with a kick and receives a kick
in return,
in strict obedience to
Newton’s third law. The ripples
spread out in all directions, and
the ones that hit the other magnet
then interact with it, again obeying
Newton’s third law.

g / Example 3. This doesn’t actually happen!

Section 5.1 Newton’s third law

157

(cid:46) Solved problem: More about example 2

page 183, problem 20

(cid:46) Solved problem: Why did it accelerate?

page 183, problem 18

Discussion questions

When you ﬁre a gun, the exploding gases push outward in all
A
directions, causing the bullet to accelerate down the barrel. What third-
law pairs are involved? [Hint: Remember that the gases themselves are
an object.]

Tam Anh grabs Sarah by the hand and tries to pull her. She tries
B
to remain standing without moving. A student analyzes the situation as
follows. “If Tam Anh’s force on Sarah is greater than her force on him,
he can get her to move. Otherwise, she’ll be able to stay where she is.”
What’s wrong with this analysis?

You hit a tennis ball against a wall. Explain any and all incorrect
C
ideas in the following description of the physics involved: “According to
Newton’s third law, there has to be a force opposite to your force on the
ball. The opposite force is the ball’s mass, which resists acceleration, and
also air resistance.”

5.2 Classiﬁcation and behavior of forces

One of the most basic and important tasks of physics is to classify
the forces of nature. I have already referred informally to “types” of
forces such as friction, magnetism, gravitational forces, and so on.
Classiﬁcation systems are creations of the human mind, so there is
always some degree of arbitrariness in them. For one thing, the level
of detail that is appropriate for a classiﬁcation system depends on
what you’re trying to ﬁnd out. Some linguists, the “lumpers,” like to
emphasize the similarities among languages, and a few extremists
have even tried to ﬁnd signs of similarities between words in lan-
guages as diﬀerent as English and Chinese, lumping the world’s lan-
guages into only a few large groups. Other linguists, the “splitters,”
might be more interested in studying the diﬀerences in pronuncia-
tion between English speakers in New York and Connecticut. The
splitters call the lumpers sloppy, but the lumpers say that science
isn’t worthwhile unless it can ﬁnd broad, simple patterns within the
seemingly complex universe.

Scientiﬁc classiﬁcation systems are also usually compromises be-
tween practicality and naturalness. An example is the question of
how to classify ﬂowering plants. Most people think that biological
classiﬁcation is about discovering new species, naming them, and
classifying them in the class-order-family-genus-species system ac-
cording to guidelines set long ago. In reality, the whole system is in
a constant state of ﬂux and controversy. One very practical way of
classifying ﬂowering plants is according to whether their petals are
separate or joined into a tube or cone — the criterion is so clear that
it can be applied to a plant seen from across the street. But here
practicality conﬂicts with naturalness. For instance, the begonia has

158

Chapter 5 Analysis of Forces

h / A
system.

scientiﬁc

classiﬁcation

separate petals and the pumpkin has joined petals, but they are so
similar in so many other ways that they are usually placed within
the same order. Some taxonomists have come up with classiﬁcation
criteria that they claim correspond more naturally to the apparent
relationships among plants, without having to make special excep-
tions, but these may be far less practical, requiring for instance the
examination of pollen grains under an electron microscope.

In physics, there are two main systems of classiﬁcation for forces.
At this point in the course, you are going to learn one that is very
practical and easy to use, and that splits the forces up into a rel-
atively large number of types: seven very common ones that we’ll
discuss explicitly in this chapter, plus perhaps ten less important
ones such as surface tension, which we will not bother with right
now.

Physicists, however, are obsessed with ﬁnding simple patterns,
so recognizing as many as ﬁfteen or twenty types of forces strikes
them as distasteful and overly complex. Since about the year 1900,
physics has been on an aggressive program to discover ways in which
these many seemingly diﬀerent types of forces arise from a smaller
number of fundamental ones. For instance, when you press your
hands together, the force that keeps them from passing through each
other may seem to have nothing to do with electricity, but at the
atomic level, it actually does arise from electrical repulsion between
atoms. By about 1950, all the forces of nature had been explained
as arising from four fundamental types of forces at the atomic and
nuclear level, and the lumping-together process didn’t stop there.
By the 1960’s the length of the list had been reduced to three, and
some theorists even believe that they may be able to reduce it to
two or one. Although the uniﬁcation of the forces of nature is one of
the most beautiful and important achievements of physics, it makes
much more sense to start this course with the more practical and
easy system of classiﬁcation. The uniﬁed system of four forces will
be one of the highlights of the end of your introductory physics
sequence.

The practical classiﬁcation scheme which concerns us now can
be laid out in the form of the tree shown in ﬁgure i. The most
speciﬁc types of forces are shown at the tips of the branches, and
it is these types of forces that are referred to in the POFOSTITO
mnemonic. For example, electrical and magnetic forces belong to
the same general group, but Newton’s third law would never relate
an electrical force to a magnetic force.

The broadest distinction is that between contact and noncontact
forces, which has been discussed in ch. 4. Among the contact forces,
we distinguish between those that involve solids only and those that
have to do with ﬂuids, a term used in physics to include both gases
and liquids.

Section 5.2 Classiﬁcation and behavior of forces

159

i / A practical classiﬁcation scheme for forces.

It should not be necessary to memorize this diagram by rote.
It is better to reinforce your memory of this system by calling to
mind your commonsense knowledge of certain ordinary phenomena.
For instance, we know that the gravitational attraction between us
and the planet earth will act even if our feet momentarily leave the
ground, and that although magnets have mass and are aﬀected by
gravity, most objects that have mass are nonmagnetic.

example 4
Hitting a wall
(cid:46) A bullet, ﬂying horizontally, hits a steel wall. What type of force
is there between the bullet and the wall?

(cid:46) Starting at the bottom of the tree, we determine that the force
is a contact force, because it only occurs once the bullet touches
the wall. Both objects are solid. The wall forms a vertical plane.
If the nose of the bullet was some shape like a sphere, you might
imagine that it would only touch the wall at one point. Realisti-
cally, however, we know that a lead bullet will ﬂatten out a lot on
impact, so there is a surface of contact between the two, and its

160

Chapter 5 Analysis of Forces

orientation is vertical. The effect of the force on the bullet is to
stop the horizontal motion of the bullet, and this horizontal ac-
celeration must be produced by a horizontal force. The force is
therefore perpendicular to the surface of contact, and it’s also re-
pulsive (tending to keep the bullet from entering the wall), so it
must be a normal force.

Diagram i is meant to be as simple as possible while including
most of the forces we deal with in everyday life.
If you were an
insect, you would be much more interested in the force of surface
tension, which allowed you to walk on water. I have not included
the nuclear forces, which are responsible for holding the nuclei of
atoms, because they are not evident in everyday life.

You should not be afraid to invent your own names for types of
forces that do not ﬁt into the diagram. For instance, the force that
holds a piece of tape to the wall has been left oﬀ of the tree, and if
you were analyzing a situation involving scotch tape, you would be
absolutely right to refer to it by some commonsense name such as
“sticky force.”

On the other hand, if you are having trouble classifying a certain
force, you should also consider whether it is a force at all. For
instance, if someone asks you to classify the force that the earth has
because of its rotation, you would have great diﬃculty creating a
place for it on the diagram. That’s because it’s a type of motion,
not a type of force!

Normal forces

A normal force, FN , is a force that keeps one solid object from
passing through another. “Normal” is simply a fancy word for “per-
pendicular,” meaning that the force is perpendicular to the surface
of contact. Intuitively, it seems the normal force magically adjusts
itself to provide whatever force is needed to keep the objects from
occupying the same space. If your muscles press your hands together
gently, there is a gentle normal force. Press harder, and the normal
force gets stronger. How does the normal force know how strong to
be? The answer is that the harder you jam your hands together,
the more compressed your ﬂesh becomes. Your ﬂesh is acting like
a spring: more force is required to compress it more. The same is
true when you push on a wall. The wall ﬂexes imperceptibly in pro-
portion to your force on it. If you exerted enough force, would it be
possible for two objects to pass through each other? No, typically
the result is simply to strain the objects so much that one of them
breaks.

Gravitational forces

As we’ll discuss in more detail later in the course, a gravitational
force exists between any two things that have mass. In everyday life,

Section 5.2 Classiﬁcation and behavior of forces

161

the gravitational force between two cars or two people is negligible,
so the only noticeable gravitational forces are the ones between the
earth and various human-scale objects. We refer to these planet-
earth-induced gravitational forces as weight forces, and as we have
already seen, their magnitude is given by |FW | = mg.

(cid:46) Solved problem: Weight and mass

page 184, problem 26

Static and kinetic friction

If you have pushed a refrigerator across a kitchen ﬂoor, you have
felt a certain series of sensations. At ﬁrst, you gradually increased
your force on the refrigerator, but it didn’t move. Finally, you sup-
plied enough force to unstick the fridge, and there was a sudden
jerk as the fridge started moving. Once the fridge was unstuck, you
could reduce your force signiﬁcantly and still keep it moving.

While you were gradually increasing your force, the ﬂoor’s fric-
tional force on the fridge increased in response. The two forces on
the fridge canceled, and the fridge didn’t accelerate. How did the
ﬂoor know how to respond with just the right amount of force? Fig-
ure j shows one possible model of friction that explains this behavior.
(A scientiﬁc model is a description that we expect to be incomplete,
approximate, or unrealistic in some ways, but that nevertheless suc-
ceeds in explaining a variety of phenomena.) Figure j/1 shows a
microscopic view of the tiny bumps and holes in the surfaces of the
ﬂoor and the refrigerator. The weight of the fridge presses the two
surfaces together, and some of the bumps in one surface will settle
as deeply as possible into some of the holes in the other surface. In
j/2, your leftward force on the fridge has caused it to ride up a little
higher on the bump in the ﬂoor labeled with a small arrow. Still
more force is needed to get the fridge over the bump and allow it to
start moving. Of course, this is occurring simultaneously at millions
of places on the two surfaces.

Once you had gotten the fridge moving at constant speed, you
found that you needed to exert less force on it. Since zero total force
is needed to make an object move with constant velocity, the ﬂoor’s
rightward frictional force on the fridge has apparently decreased
somewhat, making it easier for you to cancel it out. Our model also
gives a plausible explanation for this fact: as the surfaces slide past
each other, they don’t have time to settle down and mesh with one
another, so there is less friction.

Even though this model is intuitively appealing and fairly suc-
cessful, it should not be taken too seriously, and in some situations
it is misleading. For instance, fancy racing bikes these days are
made with smooth tires that have no tread — contrary to what
we’d expect from our model, this does not cause any decrease in
friction. Machinists know that two very smooth and clean metal

that correctly ex-
j / A model
plains many properties of friction.
The microscopic bumps and
holes in two surfaces dig into
each other.

k / Static friction: the tray doesn’t
slip on the waiter’s ﬁngers.

l / Kinetic friction: the car skids.

162

Chapter 5 Analysis of Forces

surfaces may stick to each other ﬁrmly and be very diﬃcult to slide
apart. This cannot be explained in our model, but makes more
sense in terms of a model in which friction is described as arising
from chemical bonds between the atoms of the two surfaces at their
points of contact: very ﬂat surfaces allow more atoms to come in
contact.

Since friction changes its behavior dramatically once the sur-
faces come unstuck, we deﬁne two separate types of frictional forces.
Static friction is friction that occurs between surfaces that are not
slipping over each other. Slipping surfaces experience kinetic fric-
tion. The forces of static and kinetic friction, notated Fs and Fk, are
always parallel to the surface of contact between the two objects.

self-check B
1. When a baseball player slides in to a base, is the friction static, or
kinetic?

2. A mattress stays on the roof of a slowly accelerating car.
friction static, or kinetic?

Is the

3. Does static friction create heat? Kinetic friction?

(cid:46) Answer, p. 565

The maximum possible force of static friction depends on what
kinds of surfaces they are, and also on how hard they are being
pressed together. The approximate mathematical relationships can
be expressed as follows:

Fs,max = µsFN ,

where µs is a unitless number, called the coeﬃcient of static friction,
which depends on what kinds of surfaces they are. The maximum
force that static friction can supply, µsFN , represents the boundary
between static and kinetic friction. It depends on the normal force,
which is numerically equal to whatever force is pressing the two
surfaces together.
In terms of our model, if the two surfaces are
being pressed together more ﬁrmly, a greater sideways force will be
required in order to make the irregularities in the surfaces ride up
and over each other.

Note that just because we use an adjective such as “applied” to
refer to a force, that doesn’t mean that there is some special type
of force called the “applied force.” The applied force could be any
type of force, or it could be the sum of more than one force trying
to make an object move.

self-check C
The arrows in ﬁgure m show the forces of the tree trunk on the partridge.
(cid:46) Answer, p. 565
Describe the forces the bird makes on the tree.

The force of kinetic friction on each of the two objects is in the
direction that resists the slippage of the surfaces. Its magnitude is

m / Many landfowl, even those
that are competent ﬂiers, prefer
to escape from a predator by
running upward rather than by
ﬂying. This partridge is running
up a vertical tree trunk. Humans
can’t walk up walls because there
is no normal force and therefore
no frictional force; when FN = 0,
the maximum force of static
friction Fs,max = µsFN is also
The partridge, however,
zero.
has wings that it can ﬂap in order
to create a force between it and
the air.
Typically when a bird
ﬂaps its wings, the resulting force
from the air is in the direction
that would tend to lift
the bird
up.
In this situation, however,
the partridge changes its style
of ﬂapping so that the direction
is reversed.
force
between the feet and the tree
allows a nonzero static frictional
force. The mechanism is similar
to that of a spoiler ﬁn on a racing
car. Some evolutionary biologists
believe that when vertebrate
ﬂight ﬁrst evolved, in dinosaurs,
there was ﬁrst a stage in which
the wings were used only as an
aid in running up steep inclines,
and only later a transition to
ﬂight. (Redrawn from a ﬁgure by
K.P. Dial.)

The normal

Section 5.2 Classiﬁcation and behavior of forces

163

n / We choose a coordinate sys-
tem in which the applied force,
i.e., the force trying to move the
objects, is positive. The friction
force is then negative, since it is
in the opposite direction. As you
increase the applied force,
the
force of static friction increases to
match it and cancel it out, until the
maximum force of static friction is
surpassed. The surfaces then be-
gin slipping past each other, and
the friction force becomes smaller
in absolute value.

usually well approximated as

Fk = µkFN

where µk is the coeﬃcient of kinetic friction. Kinetic friction is
usually more or less independent of velocity.

self-check D
Can a frictionless surface exert a normal force? Can a frictional force
(cid:46) Answer, p. 565
exist without a normal force?

If you try to accelerate or decelerate your car too quickly, the
forces between your wheels and the road become too great, and they
begin slipping. This is not good, because kinetic friction is weaker
than static friction, resulting in less control. Also, if this occurs
while you are turning, the car’s handling changes abruptly because
the kinetic friction force is in a diﬀerent direction than the static
friction force had been: contrary to the car’s direction of motion,
rather than contrary to the forces applied to the tire.

Most people respond with disbelief when told of the experimen-
tal evidence that both static and kinetic friction are approximately
independent of the amount of surface area in contact. Even after
doing a hands-on exercise with spring scales to show that it is true,
many students are unwilling to believe their own observations, and
insist that bigger tires “give more traction.” In fact, the main rea-
son why you would not want to put small tires on a big heavy car
is that the tires would burst!

Although many people expect that friction would be propor-
tional to surface area, such a proportionality would make predictions
contrary to many everyday observations. A dog’s feet, for example,
have very little surface area in contact with the ground compared
to a human’s feet, and yet we know that a dog can often win a
tug-of-war with a person.

164

Chapter 5 Analysis of Forces

The reason a smaller surface area does not lead to less friction
is that the force between the two surfaces is more concentrated,
causing their bumps and holes to dig into each other more deeply.

self-check E
Find the direction of each of the forces in ﬁgure o.

(cid:46) Answer, p. 565

o / 1. The cliff’s normal force on
the climber’s feet. 2. The track’s
static frictional force on the wheel
of the accelerating dragster. 3.
The ball’s normal
force on the
bat.

Locomotives
example 5
Looking at a picture of a locomotive, p, we notice two obvious
things that are different from an automobile. Where a car typi-
cally has two drive wheels, a locomotive normally has many —
ten in this example. (Some also have smaller, unpowered wheels
in front of and behind the drive wheels, but this example doesn’t.)
Also, cars these days are generally built to be as light as possi-
ble for their size, whereas locomotives are very massive, and no
effort seems to be made to keep their weight low.
(The steam
locomotive in the photo is from about 1900, but this is true even
for modern diesel and electric trains.)

p / Example 5.

The reason locomotives are built to be so heavy is for traction.
The upward normal force of the rails on the wheels, FN , cancels
the downward force of gravity, FW , so ignoring plus and minus
signs, these two forces are equal in absolute value, FN = FW .
Given this amount of normal force, the maximum force of static
friction is Fs = µsFN = µsFW . This static frictional force, of the
rails pushing forward on the wheels, is the only force that can
accelerate the train, pull it uphill, or cancel out the force of air
resistance while cruising at constant speed. The coefﬁcient of
static friction for steel on steel is about 1/4, so no locomotive can
If the
pull with a force greater than about 1/4 of its own weight.

Section 5.2 Classiﬁcation and behavior of forces

165

engine is capable of supplying more than that amount of force, the
result will be simply to break static friction and spin the wheels.

The reason this is all so different from the situation with a car is
that a car isn’t pulling something else. If you put extra weight in
a car, you improve the traction, but you also increase the inertia
of the car, and make it just as hard to accelerate. In a train, the
inertia is almost all in the cars being pulled, not in the locomotive.

The other fact we have to explain is the large number of driv-
ing wheels. First, we have to realize that increasing the num-
ber of driving wheels neither increases nor decreases the total
amount of static friction, because static friction is independent of
the amount of surface area in contact. (The reason four-wheel-
drive is good in a car is that if one or more of the wheels is slip-
ping on ice or in mud, the other wheels may still have traction.
This isn’t typically an issue for a train, since all the wheels experi-
ence the same conditions.) The advantage of having more driving
wheels on a train is that it allows us to increase the weight of the
locomotive without crushing the rails, or damaging bridges.

Fluid friction

Try to drive a nail into a waterfall and you will be confronted
with the main diﬀerence between solid friction and ﬂuid friction.
Fluid friction is purely kinetic; there is no static ﬂuid friction. The
nail in the waterfall may tend to get dragged along by the water
ﬂowing past it, but it does not stick in the water. The same is true
for gases such as air: recall that we are using the word “ﬂuid” to
include both gases and liquids.

Unlike kinetic friction between solids, ﬂuid friction increases
rapidly with velocity. It also depends on the shape of the object,
which is why a ﬁghter jet is more streamlined than a Model T. For
objects of the same shape but diﬀerent sizes, ﬂuid friction typically
scales up with the cross-sectional area of the object, which is one
of the main reasons that an SUV gets worse mileage on the freeway
than a compact car.

friction

depends

q / Fluid
on
the ﬂuid’s pattern of ﬂow, so it is
more complicated than friction
between solids, and there are
no simple, universally applicable
formulas to calculate it.
From
top to bottom: supersonic wind
tunnel, vortex created by a crop
duster, series of vortices created
by a single object, turbulence.

166

Chapter 5 Analysis of Forces

Discussion questions

A student states that when he tries to push his refrigerator, the
A
reason it won’t move is because Newton’s third law says there’s an equal
and opposite frictional force pushing back. After all, the static friction force
is equal and opposite to the applied force. How would you convince him
he is wrong?

Kinetic friction is usually more or less independent of velocity. How-
B
ever, inexperienced drivers tend to produce a jerk at the last moment of
deceleration when they stop at a stop light. What does this tell you about
the kinetic friction between the brake shoes and the brake drums?

Some of the following are correct descriptions of types of forces that
C
could be added on as new branches of the classiﬁcation tree. Others are
not really types of forces, and still others are not force phenomena at all.
In each case, decide what’s going on, and if appropriate, ﬁgure out how
you would incorporate them into the tree.

sticky force
opposite force

ﬂowing force

surface tension
horizontal force
motor force
canceled force

makes tape stick to things
the force that Newton’s third law says relates to ev-
ery force you make
the force that water carries with it as it ﬂows out of a
hose
lets insects walk on water
a force that is horizontal
the force that a motor makes on the thing it is turning
a force that is being canceled out by some other
force

5.3 Analysis of forces

Newton’s ﬁrst and second laws deal with the total of all the forces
exerted on a speciﬁc object, so it is very important to be able to
ﬁgure out what forces there are. Once you have focused your atten-
tion on one object and listed the forces on it, it is also helpful to
describe all the corresponding forces that must exist according to
Newton’s third law. We refer to this as “analyzing the forces” in
which the object participates.

r / What do the golf ball and
the shark have in common? Both
use the same trick to reduce ﬂuid
friction. The dimples on the golf
ball modify the pattern of ﬂow of
the air around it, counterintuitively
reducing friction. Recent studies
have shown that sharks can
accomplish the same thing by
raising, or “bristling,” the scales
on their skin at high speeds.

of

s / The wheelbases
the
Hummer H3 and the Toyota Prius
are surprisingly similar, differing
by only 10%. The main difference
in shape is that the Hummer is
much taller and wider. It presents
a much greater cross-sectional
area to the wind, and this is the
main reason that it uses about 2.5
times more gas on the freeway.

Section 5.3 Analysis of forces

167

A barge
example 6
A barge is being pulled to the right along a canal by teams of horses on the shores. Analyze all the forces in
which the barge participates.

force acting on barge
ropes’ normal forces on barge, →
water’s ﬂuid friction force on barge, ←
planet earth’s gravitational force on barge, ↓
water’s “ﬂoating” force on barge, ↑

force related to it by Newton’s third law
barge’s normal force on ropes, ←
barge’s ﬂuid friction force on water, →
barge’s gravitational force on earth, ↑
barge’s “ﬂoating” force on water, ↓

Here I’ve used the word “ﬂoating” force as an example of a sensible invented term for a type of force not
classiﬁed on the tree on p. 160. A more formal technical term would be “hydrostatic force.”
Note how the pairs of forces are all structured as “A’s force on B, B’s force on A”: ropes on barge and barge
on ropes; water on barge and barge on water. Because all the forces in the left column are forces acting on
the barge, all the forces in the right column are forces being exerted by the barge, which is why each entry in
the column begins with “barge.”

Often you may be unsure whether you have forgotten one of the

forces. Here are three strategies for checking your list:

1. See what physical result would come from the forces you’ve
found so far. Suppose, for instance, that you’d forgotten the
“ﬂoating” force on the barge in the example above. Looking
at the forces you’d found, you would have found that there
was a downward gravitational force on the barge which was
not canceled by any upward force. The barge isn’t supposed
to sink, so you know you need to ﬁnd a fourth, upward force.

2. Another technique for ﬁnding missing forces is simply to go
through the list of all the common types of forces and see if
any of them apply.

3. Make a drawing of the object, and draw a dashed boundary
line around it that separates it from its environment. Look for
points on the boundary where other objects come in contact
with your object. This strategy guarantees that you’ll ﬁnd
every contact force that acts on the object, although it won’t
help you to ﬁnd non-contact forces.

Fiﬁ
example 7
(cid:46) Fiﬁ is an industrial espionage dog who loves doing her job and
looks great doing it. She leaps through a window and lands at
initial horizontal speed vo on a conveyor belt which is itself moving
at the greater speed vb. Unfortunately the coefﬁcient of kinetic
friction µk between her foot-pads and the belt is fairly low, so she
skids for a time ∆t, during which the effect on her coiffure is un
d ´esastre. Find ∆t.

t / Example 7.

168

Chapter 5 Analysis of Forces

(cid:46) We analyze the forces:

force acting on Fiﬁ

gravitational
planet
earth’s
↓
force FW = mg on Fiﬁ,
belt’s kinetic frictional force Fk
→
on Fiﬁ,
belt’s normal force FN on Fiﬁ, ↑ Fiﬁ’s normal force on belt,

force related to it by Newton’s
third law
force on
Fiﬁ’s gravitational
↑
earth,
Fiﬁ’s kinetic frictional force on
←
belt,
↓

Checking the analysis of the forces as described on p. 168:

(1) The physical result makes sense. The left-hand column con-
sists of forces ↓→↑. We’re describing the time when she’s moving
horizontally on the belt, so it makes sense that we have two ver-
tical forces that could cancel. The rightward force is what will
accelerate her until her speed matches that of the belt.

(2) We’ve included every relevant type of force from the tree on
p. 160.

(3) We’ve included forces from the belt, which is the only object
in contact with Fiﬁ.

The purpose of the analysis is to let us set up equations con-
taining enough information to solve the problem. Using the gen-
eralization of Newton’s second law given on p. 137, we use the
horizontal force to determine the horizontal acceleration, and sep-
arately require the vertical forces to cancel out.

Let positive x be to the right. Newton’s second law gives

(→)

a = Fk /m

Although it’s the horizontal motion we care about, the only way to
ﬁnd Fk is via the relation Fk = µk FN , and the only way to ﬁnd FN
is from the ↑↓ forces. The two vertical forces must cancel, which
means they have to be of equal strength:

(↑↓)

FN − mg = 0.

Using the constant-acceleration equation a = ∆v /∆t, we have

∆t =

=

=

∆v
a
vb − vo
µk mg/m
vb − vo
µk g

.

s =

m/s
m/s2 ,

The units check out:

Section 5.3 Analysis of forces

169

where µk is omitted as a factor because it’s unitless.

We should also check that the dependence on the variables makes
sense. If Fiﬁ puts on her rubber ninja booties, increasing µk , then
dividing by a larger number gives a smaller result for ∆t; this
makes sense physically, because the greater friction will cause
her to come up to the belt’s speed more quickly. The dependence
on g is similar; more gravity would press her harder against the
Increasing vb increases ∆t, which
belt, improving her traction.
makes sense because it will take her longer to get up to a bigger
speed. Since vo is subtracted, the dependence of ∆t on it is the
other way around, and that makes sense too, because if she can
land with a greater speed, she has less speeding up left to do.

u / Example 8.

Forces don’t have to be in pairs or at right angles
example 8
In ﬁgure u, the three horses are arranged symmetrically at 120
degree intervals, and are all pulling on the central knot. Let’s say
the knot is at rest and at least momentarily in equilibrium. The
analysis of forces on the knot is as follows.

170

Chapter 5 Analysis of Forces

force acting on knot

rope’s normal

top rope’s normal
knot,
left
knot,
right rope’s normal
knot,

force on
↑
force on

force on

force related to it by Newton’s
third law
knot’s normal
rope,
knot’s normal
rope,
knot’s normal
rope,

force on top
↓
force on left

force on right

In our previous examples, the forces have all run along two per-
pendicular lines, and they often canceled in pairs. This example
shows that neither of these always happens. Later in the book
we’ll see how to handle forces that are at arbitrary angles, using
mathematical objects called vectors. But even without knowing
about vectors, we already know what directions to draw the ar-
rows in the table, since a rope can only pull parallel to itself at its
ends. And furthermore, we can say something about the forces:
by symmetry, we expect them all to be equal in strength. (If the
knot was not in equilibrium, then this symmetry would be broken.)

This analysis also demonstrates that it’s all right to leave out de-
tails if they aren’t of interest and we don’t intend to include them
in our model. We called the forces normal forces, but we can’t ac-
tually tell whether they are normal forces or frictional forces. They
are probably some combination of those, but we don’t include
such details in this model, since aren’t interested in describing the
internal physics of the knot. This is an example of a more general
fact about science, which is that science doesn’t describe reality.
It describes simpliﬁed models of reality, because reality is always
too complex to model exactly.

Section 5.3 Analysis of forces

171

Discussion questions

In the example of the barge going down the canal, I referred to
A
a “ﬂoating” or “hydrostatic” force that keeps the boat from sinking. If you
were adding a new branch on the force-classiﬁcation tree to represent this
force, where would it go?

The earth’s gravitational force on you, i.e., your weight, is always
B
equal to mg, where m is your mass. So why can you get a shovel to go
deeper into the ground by jumping onto it? Just because you’re jumping,
that doesn’t mean your mass or weight is any greater, does it?

5.4 Transmission of forces by low-mass

objects

You’re walking your dog. The dog wants to go faster than you do,
and the leash is taut. Does Newton’s third law guarantee that your
force on your end of the leash is equal and opposite to the dog’s
force on its end? If they’re not exactly equal, is there any reason
why they should be approximately equal?

If there was no leash between you, and you were in direct contact
with the dog, then Newton’s third law would apply, but Newton’s
third law cannot relate your force on the leash to the dog’s force
on the leash, because that would involve three separate objects.
Newton’s third law only says that your force on the leash is equal
and opposite to the leash’s force on you,

FyL = −FLy,

and that the dog’s force on the leash is equal and opposite to its

force on the dog

FdL = −FLd.

Still, we have a strong intuitive expectation that whatever force we
make on our end of the leash is transmitted to the dog, and vice-
versa. We can analyze the situation by concentrating on the forces
that act on the leash, FdL and FyL. According to Newton’s second
law, these relate to the leash’s mass and acceleration:

FdL + FyL = mLaL.

The leash is far less massive then any of the other objects involved,
and if mL is very small, then apparently the total force on the leash
is also very small, FdL + FyL ≈ 0, and therefore

FdL ≈ −FyL.

Thus even though Newton’s third law does not apply directly to
these two forces, we can approximate the low-mass leash as if it was
not intervening between you and the dog. It’s at least approximately
as if you and the dog were acting directly on each other, in which
case Newton’s third law would have applied.

172

Chapter 5 Analysis of Forces

In general, low-mass objects can be treated approximately as if
they simply transmitted forces from one object to another. This can
be true for strings, ropes, and cords, and also for rigid objects such
as rods and sticks.

then
v / If we imagine dividing a taut rope up into small segments,
any segment has forces pulling outward on it at each end.
If the rope
is of negligible mass, then all the forces equal +T or −T , where T , the
tension, is a single number.

If you look at a piece of string under a magnifying glass as you
pull on the ends more and more strongly, you will see the ﬁbers
straightening and becoming taut. Diﬀerent parts of the string are
apparently exerting forces on each other. For instance, if we think of
the two halves of the string as two objects, then each half is exerting
a force on the other half. If we imagine the string as consisting of
many small parts, then each segment is transmitting a force to the
next segment, and if the string has very little mass, then all the
forces are equal in magnitude. We refer to the magnitude of the
forces as the tension in the string, T .

The term “tension” refers only to internal forces within the
string. If the string makes forces on objects at its ends, then those
forces are typically normal or frictional forces (example 9).

w / The Golden Gate Bridge’s
roadway is held up by the tension
in the vertical cables.

Section 5.4

Transmission of forces by low-mass objects

173

Types of force made by ropes
(cid:46) Analyze the forces in ﬁgures x/1 and x/2.

example 9

(cid:46) In all cases, a rope can only make “pulling” forces, i.e., forces
that are parallel to its own length and that are toward itself, not
away from itself. You can’t push with a rope!

In x/1, the rope passes through a type of hook, called a carabiner,
used in rock climbing and mountaineering. Since the rope can
only pull along its own length, the direction of its force on the
carabiner must be down and to the right. This is perpendicular to
the surface of contact, so the force is a normal force.

force acting on carabiner

rope’s normal force on cara-
biner

force related to it by Newton’s
third law
carabiner’s normal
rope

force on

(There are presumably other forces acting on the carabiner from
other hardware above it.)

In ﬁgure x/2, the rope can only exert a net force at its end that
is parallel to itself and in the pulling direction, so its force on the
hand is down and to the left. This is parallel to the surface of
contact, so it must be a frictional force. If the rope isn’t slipping
through the hand, we have static friction. Friction can’t exist with-
out normal forces. These forces are perpendicular to the surface
of contact. For simplicity, we show only two pairs of these normal
forces, as if the hand were a pair of pliers.

x / Example 9.
The forces
between the rope and other
objects are normal and frictional
forces.

force acting on person

rope’s static frictional force on
person
rope’s
person
rope’s
person

normal

normal

force

force

on

on

force related to it by Newton’s
third law
person’s static frictional force
on rope
person’s
rope
person’s
rope

normal

normal

force

force

on

on

(There are presumably other forces acting on the person as well,
such as gravity.)

If a rope goes over a pulley or around some other object, then
the tension throughout the rope is approximately equal so long as
the pulley has negligible mass and there is not too much friction. A
rod or stick can be treated in much the same way as a string, but
it is possible to have either compression or tension.

Discussion question

A When you step on the gas pedal, is your foot’s force being transmitted
in the sense of the word used in this section?

174

Chapter 5 Analysis of Forces

5.5 Objects under strain

A string lengthens slightly when you stretch it. Similarly, we have
already discussed how an apparently rigid object such as a wall is
actually ﬂexing when it participates in a normal force.
In other
cases, the eﬀect is more obvious. A spring or a rubber band visibly
elongates when stretched.

Common to all these examples is a change in shape of some kind:
lengthening, bending, compressing, etc. The change in shape can
be measured by picking some part of the object and measuring its
position, x. For concreteness, let’s imagine a spring with one end
attached to a wall. When no force is exerted, the unﬁxed end of the
spring is at some position xo. If a force acts at the unﬁxed end, its
position will change to some new value of x. The more force, the
greater the departure of x from xo.

y / Deﬁning the quantities F , x,
and xo in Hooke’s law.

Back in Newton’s time, experiments like this were considered
cutting-edge research, and his contemporary Hooke is remembered
today for doing them and for coming up with a simple mathematical
generalization called Hooke’s law:

F ≈ k(x − xo).

[force required to stretch a spring; valid

for small forces only]

Here k is a constant, called the spring constant, that depends on
If too much force is applied, the spring
how stiﬀ the object is.
exhibits more complicated behavior, so the equation is only a good
approximation if the force is suﬃciently small. Usually when the
force is so large that Hooke’s law is a bad approximation, the force
ends up permanently bending or breaking the spring.

Although Hooke’s law may seem like a piece of trivia about
springs, it is actually far more important than that, because all

Section 5.5 Objects under strain

175

solid objects exert Hooke’s-law behavior over some range of suﬃ-
ciently small forces. For example, if you push down on the hood of
a car, it dips by an amount that is directly proportional to the force.
(But the car’s behavior would not be as mathematically simple if
you dropped a boulder on the hood!)

(cid:46) Solved problem: Combining springs

page 182, problem 14

(cid:46) Solved problem: Young’s modulus

page 182, problem 16

Discussion question

A car is connected to its axles through big, stiff springs called shock
A
absorbers, or “shocks.” Although we’ve discussed Hooke’s law above only
in the case of stretching a spring, a car’s shocks are continually going
through both stretching and compression.
In this situation, how would
you interpret the positive and negative signs in Hooke’s law?

5.6 Simple Machines: the pulley

Even the most complex machines, such as cars or pianos, are built
out of certain basic units called simple machines. The following are
some of the main functions of simple machines:

transmitting a force: The chain on a bicycle transmits a force
from the crank set to the rear wheel.

changing the direction of a force: If you push down on a see-
saw, the other end goes up.

changing the speed and precision of motion: When you make
the “come here” motion, your biceps only moves a couple of
centimeters where it attaches to your forearm, but your arm
moves much farther and more rapidly.

changing the amount of force: A lever or pulley can be used
to increase or decrease the amount of force.

You are now prepared to understand one-dimensional simple ma-
chines, of which the pulley is the main example.

z / Example 10.

A pulley
example 10
(cid:46) Farmer Bill says this pulley arrangement doubles the force of
his tractor. Is he just a dumb hayseed, or does he know what he’s
doing?

176

Chapter 5 Analysis of Forces

(cid:46) To use Newton’s ﬁrst law, we need to pick an object and con-
sider the sum of the forces on it. Since our goal is to relate the
tension in the part of the cable attached to the stump to the ten-
sion in the part attached to the tractor, we should pick an object
to which both those cables are attached, i.e., the pulley itself. The
tension in a string or cable remains approximately constant as it
passes around an idealized pulley. 1 There are therefore two left-
ward forces acting on the pulley, each equal to the force exerted
by the tractor. Since the acceleration of the pulley is essentially
zero, the forces on it must be canceling out, so the rightward force
of the pulley-stump cable on the pulley must be double the force
exerted by the tractor. Yes, Farmer Bill knows what he’s talking
about.

1This was asserted in section 5.4 without proof. Essentially it holds because
of symmetry. E.g., if the U-shaped piece of rope in ﬁgure z had unequal tension
in its two legs, then this would have to be caused by some asymmetry between
clockwise and counterclockwise rotation. But such an asymmetry can only be
caused by friction or inertia, which we assume don’t exist.

Section 5.6 Simple Machines: the pulley

177

Summary

Selected vocabulary

repulsive . . . . .

attractive . . . .

oblique . . . . . .

normal force . . .

static friction . .

kinetic friction .

ﬂuid . . . . . . . .
ﬂuid friction . . .

spring constant .

Notation
FN . . . . . . . . .
Fs . . . . . . . . .
Fk . . . . . . . . .
µs . . . . . . . . .

µk . . . . . . . . .

k . . . . . . . . . .

Summary

describes a force that tends to push the two
participating objects apart
describes a force that tends to pull the two
participating objects together
describes a force that acts at some other angle,
one that is not a direct repulsion or attraction
the force that keeps two objects from occupy-
ing the same space
a friction force between surfaces that are not
slipping past each other
a friction force between surfaces that are slip-
ping past each other
a gas or a liquid
a friction force in which at least one of the
object is is a ﬂuid
the constant of proportionality between force
and elongation of a spring or other object un-
der strain

a normal force
a static frictional force
a kinetic frictional force
the coeﬃcient of static friction; the constant of
proportionality between the maximum static
frictional force and the normal force; depends
on what types of surfaces are involved
the coeﬃcient of kinetic friction; the constant
of proportionality between the kinetic fric-
tional force and the normal force; depends on
what types of surfaces are involved
the spring constant; the constant of propor-
tionality between the force exerted on an ob-
ject and the amount by which the object is
lengthened or compressed

Newton’s third law states that forces occur in equal and opposite
pairs. If object A exerts a force on object B, then object B must
simultaneously be exerting an equal and opposite force on object A.
Each instance of Newton’s third law involves exactly two objects,
and exactly two forces, which are of the same type.

There are two systems for classifying forces. We are presently
using the more practical but less fundamental one. In this system,
forces are classiﬁed by whether they are repulsive, attractive, or
oblique; whether they are contact or noncontact forces; and whether

178

Chapter 5 Analysis of Forces

the two objects involved are solids or ﬂuids.

Static friction adjusts itself to match the force that is trying to
make the surfaces slide past each other, until the maximum value is
reached,

Fs,max = µsFN .

Once this force is exceeded, the surfaces slip past one another, and
kinetic friction applies,

Fk = µkFN .

Both types of frictional force are nearly independent of surface area,
and kinetic friction is usually approximately independent of the
speed at which the surfaces are slipping. The direction of the force
is in the direction that would tend to stop or prevent slipping.

A good ﬁrst step in applying Newton’s laws of motion to any
physical situation is to pick an object of interest, and then to list
all the forces acting on that object. We classify each force by its
type, and ﬁnd its Newton’s-third-law partner, which is exerted by
the object on some other object.

When two objects are connected by a third low-mass object,

their forces are transmitted to each other nearly unchanged.

Objects under strain always obey Hooke’s law to a good approx-
imation, as long as the force is small. Hooke’s law states that the
stretching or compression of the object is proportional to the force
exerted on it,

F ≈ k(x − xo).

Summary

179

Problems

A computerized answer check is available online.

Key√
(cid:82) A problem that requires calculus.
(cid:63) A diﬃcult problem.

1
A little old lady and a pro football player collide head-on.
Compare their forces on each other, and compare their accelerations.
Explain.

Problem 1.

The earth is attracted to an object with a force equal and
2
If this is true,
opposite to the force of the earth on the object.
why is it that when you drop an object, the earth does not have an
acceleration equal and opposite to that of the object?

3
When you stand still, there are two forces acting on you,
the force of gravity (your weight) and the normal force of the ﬂoor
pushing up on your feet. Are these forces equal and opposite? Does
Newton’s third law relate them to each other? Explain.

In problems 4-8, analyze the forces using a table in the format shown
in section 5.3. Analyze the forces in which the italicized object par-
ticipates.

Problem 6.

4
Some people put a spare car key in a little magnetic box that
they stick under the chassis of their car. Let’s say that the box is
stuck directly underneath a horizontal surface, and the car is parked.
(See instructions above.)

Analyze two examples of objects at rest relative to the earth
5
that are being kept from falling by forces other than the normal
force. Do not use objects in outer space, and do not duplicate
problem 4 or 8. (See instructions above.)

6
A person is rowing a boat, with her feet braced. She is doing
the part of the stroke that propels the boat, with the ends of the
oars in the water (not the part where the oars are out of the water).
(See instructions above.)

Problem 7.

A farmer is in a stall with a cow when the cow decides to press
7
him against the wall, pinning him with his feet oﬀ the ground. An-
alyze the forces in which the farmer participates. (See instructions
above.)

180

Chapter 5 Analysis of Forces

A propeller plane is cruising east at constant speed and alti-

8
tude. (See instructions above.)

9
Today’s tallest buildings are really not that much taller than
the tallest buildings of the 1940’s. One big problem with making an
even taller skyscraper is that every elevator needs its own shaft run-
ning the whole height of the building. So many elevators are needed
to serve the building’s thousands of occupants that the elevator
shafts start taking up too much of the space within the building.
An alternative is to have elevators that can move both horizontally
and vertically: with such a design, many elevator cars can share a
few shafts, and they don’t get in each other’s way too much because
they can detour around each other. In this design, it becomes im-
possible to hang the cars from cables, so they would instead have to
ride on rails which they grab onto with wheels. Friction would keep
them from slipping. The ﬁgure shows such a frictional elevator in
its vertical travel mode. (The wheels on the bottom are for when it
needs to switch to horizontal motion.)
(a) If the coeﬃcient of static friction between rubber and steel is
µs, and the maximum mass of the car plus its passengers is M ,
how much force must there be pressing each wheel against the rail
in order to keep the car from slipping? (Assume the car is not
√
accelerating.)
(b) Show that your result has physically reasonable behavior with
respect to µs. In other words, if there was less friction, would the
wheels need to be pressed more ﬁrmly or less ﬁrmly? Does your
equation behave that way?

Unequal masses M and m are suspended from a pulley as

10
shown in the ﬁgure.
(a) Analyze the forces in which mass m participates, using a table
[The forces in which the other
in the format shown in section 5.3.
mass participates will of course be similar, but not numerically the
same.]
(b) Find the magnitude of the accelerations of the two masses.
[Hints: (1) Pick a coordinate system, and use positive and nega-
tive signs consistently to indicate the directions of the forces and
accelerations. (2) The two accelerations of the two masses have to
be equal in magnitude but of opposite signs, since one side eats up
rope at the same rate at which the other side pays it out. (3) You
need to apply Newton’s second law twice, once to each mass, and
then solve the two equations for the unknowns: the acceleration, a,
√
and the tension in the rope, T .]
(c) Many people expect that in the special case of M = m, the two
masses will naturally settle down to an equilibrium position side by
side. Based on your answer from part b, is this correct?
(d) Find the tension in the rope, T .
(e) Interpret your equation from part d in the special case where one
of the masses is zero. Here “interpret” means to ﬁgure out what hap-
pens mathematically, ﬁgure out what should happen physically, and

√

Problem 8.

Problem 9.

Problem 10.

Problems

181

connect the two.

11
A tugboat of mass m pulls a ship of mass M , accelerating it.
The speeds are low enough that you can ignore ﬂuid friction acting
on their hulls, although there will of course need to be ﬂuid friction
acting on the tug’s propellers.
(a) Analyze the forces in which the tugboat participates, using a
table in the format shown in section 5.3. Don’t worry about vertical
forces.
(b) Do the same for the ship.
(c) If the force acting on the tug’s propeller is F , what is the tension,
T , in the cable connecting the two ships? [Hint: Write down two
equations, one for Newton’s second law applied to each object. Solve
√
these for the two unknowns T and a.]
(d) Interpret your answer in the special cases of M = 0 and M = ∞.

12
Someone tells you she knows of a certain type of Central
American earthworm whose skin, when rubbed on polished dia-
mond, has µk > µs. Why is this not just empirically unlikely but
logically suspect?

13
In the system shown in the ﬁgure, the pulleys on the left and
right are ﬁxed, but the pulley in the center can move to the left or
right. The two masses are identical. Show that the mass on the left
will have an upward acceleration equal to g/5. Assume all the ropes
and pulleys are massless and frictionless.

14
The ﬁgure shows two diﬀerent ways of combining a pair of
identical springs, each with spring constant k. We refer to the top
setup as parallel, and the bottom one as a series arrangement.
(a) For the parallel arrangement, analyze the forces acting on the
connector piece on the left, and then use this analysis to determine
the equivalent spring constant of the whole setup. Explain whether
the combined spring constant should be interpreted as being stiﬀer
or less stiﬀ.
(b) For the series arrangement, analyze the forces acting on each
spring and ﬁgure out the same things.

(cid:46) Solution, p. 552

Generalize the results of problem 14 to the case where the

15
two spring constants are unequal.

(a) Using the solution of problem 14, which is given in the
16
back of the book, predict how the spring constant of a ﬁber will
depend on its length and cross-sectional area.
(b) The constant of proportionality is called the Young’s modulus,
E, and typical values of the Young’s modulus are about 1010 to
1011. What units would the Young’s modulus have in the SI (meter-
kilogram-second) system?

(cid:46) Solution, p. 552

Problem 13.

Problem 14.

182

Chapter 5 Analysis of Forces

17
This problem depends on the results of problems 14 and
16, whose solutions are in the back of the book. When atoms form
chemical bonds, it makes sense to talk about the spring constant of
the bond as a measure of how “stiﬀ” it is. Of course, there aren’t
really little springs — this is just a mechanical model. The purpose
of this problem is to estimate the spring constant, k, for a single
bond in a typical piece of solid matter. Suppose we have a ﬁber,
like a hair or a piece of ﬁshing line, and imagine for simplicity that
it is made of atoms of a single element stacked in a cubical manner,
as shown in the ﬁgure, with a center-to-center spacing b. A typical
value for b would be about 10−10 m.
(a) Find an equation for k in terms of b, and in terms of the Young’s
modulus, E, deﬁned in problem 16 and its solution.
(b) Estimate k using the numerical data given in problem 16.
(c) Suppose you could grab one of the atoms in a diatomic molecule
like H2 or O2, and let the other atom hang vertically below it. Does
the bond stretch by any appreciable fraction due to gravity?

18
In each case, identify the force that causes the acceleration,
and give its Newton’s-third-law partner. Describe the eﬀect of the
partner force. (a) A swimmer speeds up. (b) A golfer hits the ball
oﬀ of the tee. (c) An archer ﬁres an arrow. (d) A locomotive slows
down.

(cid:46) Solution, p. 552

Ginny has a plan. She is going to ride her sled while her dog
19
Foo pulls her, and she holds on to his leash. However, Ginny hasn’t
taken physics, so there may be a problem: she may slide right oﬀ
the sled when Foo starts pulling.
(a) Analyze all the forces in which Ginny participates, making a
table as in section 5.3.
(b) Analyze all the forces in which the sled participates.
(c) The sled has mass m, and Ginny has mass M . The coeﬃcient
of static friction between the sled and the snow is µ1, and µ2 is
the corresponding quantity for static friction between the sled and
her snow pants. Ginny must have a certain minimum mass so that
she will not slip oﬀ the sled. Find this in terms of the other three
√
variables.
(d) Interpreting your equation from part c, under what conditions
will there be no physically realistic solution for M ? Discuss what
this means physically.

20
Example 2 on page 157 involves a person pushing a box up a
hill. The incorrect answer describes three forces. For each of these
three forces, give the force that it is related to by Newton’s third
law, and state the type of force.

(cid:46) Solution, p. 553

21
Example 10 on page 176 describes a force-doubling setup
involving a pulley. Make up a more complicated arrangement, using
two pulleys, that would multiply the force by four. The basic idea
is to take the output of one force doubler and feed it into the input

Problem 17.

Problem 19.

Problems

183

of a second one.

22
Pick up a heavy object such as a backpack or a chair, and
stand on a bathroom scale. Shake the object up and down. What
do you observe? Interpret your observations in terms of Newton’s
third law.

23
A cop investigating the scene of an accident measures the
length L of a car’s skid marks in order to ﬁnd out its speed v at
the beginning of the skid. Express v in terms of L and any other
relevant variables.

√

24
The following reasoning leads to an apparent paradox; explain
what’s wrong with the logic. A baseball player hits a ball. The ball
and the bat spend a fraction of a second in contact. During that
time they’re moving together, so their accelerations must be equal.
Newton’s third law says that their forces on each other are also
equal. But a = F/m, so how can this be, since their masses are
unequal? (Note that the paradox isn’t resolved by considering the
force of the batter’s hands on the bat. Not only is this force very
small compared to the ball-bat force, but the batter could have just
thrown the bat at the ball.)

25

This problem has been deleted.

(a) Compare the mass of a one-liter water bottle on earth,
(cid:46) Solution, p. 553

26
on the moon, and in interstellar space.
(b) Do the same for its weight.

27
An ice skater builds up some speed, and then coasts across
the ice passively in a straight line. (a) Analyze the forces, using a
table in the format shown in section 5.3.
(b) If his initial speed is v, and the coeﬃcient of kinetic friction is µk,
ﬁnd the maximum theoretical distance he can glide before coming
√
to a stop. Ignore air resistance.
(c) Show that your answer to part b has the right units.
(d) Show that your answer to part b depends on the variables in a
way that makes sense physically.
(e) Evaluate your answer numerically for µk = 0.0046, and a world-
record speed of 14.58 m/s. (The coeﬃcient of friction was measured
by De Koning et al., using special skates worn by real speed skaters.)
√

(f) Comment on whether your answer in part e seems realistic. If it
doesn’t, suggest possible reasons why.

184

Chapter 5 Analysis of Forces

28
Mountain climbers with masses m and M are roped together
while crossing a horizontal glacier when a vertical crevasse opens up
under the climber with mass M . The climber with mass m drops
down on the snow and tries to stop by digging into the snow with
the pick of an ice ax. Alas, this story does not have a happy ending,
because this doesn’t provide enough friction to stop. Both m and M
continue accelerating, with M dropping down into the crevasse and
m being dragged across the snow, slowed only by the kinetic friction
with coeﬃcient µk acting between the ax and the snow. There is no
signiﬁcant friction between the rope and the lip of the crevasse.
(a) Find the acceleration a.
(b) Check the units of your result.
(c) Check the dependence of your equation on the variables. That
means that for each variable, you should determine what its eﬀect
on a should be physically, and then what your answer from part a
says its eﬀect would be mathematically.

√

Problem 28.

29
The ﬁgure shows a column in the shape of a woman, holding
up the roof of part of the Parthenon. Analyze the forces in which
she participates, using a table in the format shown in section 5.3.

(cid:46) Solution, p. 553

30
Problem 15, p. 150, which has a solution in the back of the
book, was an analysis of the forces acting on a rock climber being
lowered back down on the rope. Expand that analysis into a table
in the format shown in section 5.3, which includes the types of the
forces and their Newton’s-third-law partners.

Problem 29.

The ﬁgure shows a man trying to push his car out of the mud.
31
(a) Suppose that he isn’t able to move the car. Analyze the forces
in which the car participates, using a table in the format shown in
section 5.3. (b) In the situation described above, consider the forces
that act on the car, and compare their strengths. (c) The man takes
a nap, eats some chocolate, and now feels stronger. Now he is able
to move the car, and the car is currently moving at constant speed.
Discuss the strengths of the forces at this time, in relation to one
another. (d) The man gets tired again. He is still pushing, but the
car, although still moving, begins to decelerate. Again, discuss the
strengths of the forces in relation to one another.

Problem 31.

Problems

185

32
The ﬁgure shows a mountaineer doing a vertical rappel. Her
anchor is a big boulder. The American Mountain Guides Association
suggests as a rule of thumb that in this situation, the boulder should
be at least as big as a refrigerator, and should be sitting on a surface
that is horizontal rather than sloping. The goal of this problem is
to estimate what coeﬃcient of static friction µs between the boulder
and the ledge is required if this setup is to hold the person’s body
weight. For comparison, reference books meant for civil engineers
building walls out of granite blocks state that granite on granite
typically has a µs ≈ 0.6. We expect the result of our calculation
to be much less than this, both because a large margin of safety
is desired and because the coeﬃcient could be much lower if, for
example, the surface was sandy rather than clean. We will assume
that there is no friction where the rope goes over the lip of the cliﬀ,
although in reality this friction signiﬁcantly reduces the load on the
boulder.
(a) Let m be the mass of the climber, V the volume of the boulder,
ρ its density, and g the strength of the gravitational ﬁeld. Find the
√
minimum value of µs.
(b) Show that the units of your answer make sense.
(c) Check that its dependence on the variables makes sense.
(d) Evaluate your result numerically. The volume of my refrigerator
is about 0.7 m3, the density of granite is about 2.7 g/cm3, and
standards bodies use a body mass of 80 kg for testing climbing
√
equipment.

A toy manufacturer is playtesting teﬂon booties that slip
33
on over your shoes. In the parking lot, giggling engineers ﬁnd that
when they start with an initial speed of 1.2 m/s, they glide for 2.0 m
before coming to a stop. What is the coeﬃcient of friction between
the asphalt and the booties?

[problem by B. Shotwell]

√

34
Blocks M1 and M2 are stacked as shown, with M2 on top.
M2 is connected by a string to the wall, and M1 is pulled to the
right with a force F big enough to get M1 to move. The coeﬃcient
of kinetic friction has the same value µk among all surfaces (i.e., the
block-block and ground-block interfaces).
(a) Analyze the forces in which each block participates, as in section
5.3.
(b) Determine the tension in the string.
(c) Find the acceleration of the block of mass M1.

√

√

[problem by B. Shotwell]

Problem 32.

Problem 34.

Problem 35.

35
A person can pull with a maximum force F . What is the
maximum mass that the person can lift with the pulley setup shown
in the ﬁgure?

[problem by B. Shotwell]

√

186

Chapter 5 Analysis of Forces

36
Blocks of mass m1 and m2 rest, as shown in the ﬁgure, on a
frictionless plane, and are squeezed by forces of magnitude F1 and
F2.
(a) Find the force f that acts between the two blocks.
(b) Check that your answer makes sense in the symmetric case where
F1 = F2 and m1 = m2.
(c) Find the conditions under which your answer to part a gives
f = 0, and check that it makes sense.

√

Problem 36.

Problems

187

Motion in Three
Dimensions

188

190

Chapter 6
Newton’s Laws in Three
Dimensions

6.1 Forces have no perpendicular effects

Suppose you could shoot a riﬂe and arrange for a second bullet to
be dropped from the same height at the exact moment when the
ﬁrst left the barrel. Which would hit the ground ﬁrst? Nearly
everyone expects that the dropped bullet will reach the dirt ﬁrst,

191

and Aristotle would have agreed. Aristotle would have described it
like this. The shot bullet receives some forced motion from the gun.
It travels forward for a split second, slowing down rapidly because
there is no longer any force to make it continue in motion. Once
it is done with its forced motion, it changes to natural motion, i.e.
falling straight down. While the shot bullet is slowing down, the
dropped bullet gets on with the business of falling, so according to
Aristotle it will hit the ground ﬁrst.

a / A bullet is shot from a gun, and another bullet is simultaneously dropped from the same height. 1.
Aristotelian physics says that the horizontal motion of the shot bullet delays the onset of falling, so the dropped
bullet hits the ground ﬁrst. 2. Newtonian physics says the two bullets have the same vertical motion, regardless
of their different horizontal motions.

Luckily, nature isn’t as complicated as Aristotle thought! To
convince yourself that Aristotle’s ideas were wrong and needlessly
complex, stand up now and try this experiment. Take your keys
out of your pocket, and begin walking briskly forward. Without
speeding up or slowing down, release your keys and let them fall
while you continue walking at the same pace.

You have found that your keys hit the ground right next to your
feet. Their horizontal motion never slowed down at all, and the
whole time they were dropping, they were right next to you. The
horizontal motion and the vertical motion happen at the same time,
and they are independent of each other. Your experiment proves
that the horizontal motion is unaﬀected by the vertical motion, but
it’s also true that the vertical motion is not changed in any way by
the horizontal motion. The keys take exactly the same amount of
time to get to the ground as they would have if you simply dropped
them, and the same is true of the bullets: both bullets hit the ground

192

Chapter 6 Newton’s Laws in Three Dimensions

simultaneously.

These have been our ﬁrst examples of motion in more than one
dimension, and they illustrate the most important new idea that
is required to understand the three-dimensional generalization of
Newtonian physics:

Forces have no perpendicular eﬀects.

When a force acts on an object, it has no eﬀect on the part of the
object’s motion that is perpendicular to the force.

In the examples above, the vertical force of gravity had no eﬀect
on the horizontal motions of the objects. These were examples of
projectile motion, which interested people like Galileo because of
its military applications. The principle is more general than that,
however. For instance, if a rolling ball is initially heading straight
for a wall, but a steady wind begins blowing from the side, the ball
does not take any longer to get to the wall. In the case of projectile
motion, the force involved is gravity, so we can say more speciﬁcally
that the vertical acceleration is 9.8 m/s2, regardless of the horizontal
motion.

self-check A
In the example of the ball being blown sideways, why doesn’t the ball
(cid:46)
take longer to get there, since it has to travel a greater distance?
Answer, p. 566

Relationship to relative motion

These concepts are directly related to the idea that motion is rel-
ative. Galileo’s opponents argued that the earth could not possibly
be rotating as he claimed, because then if you jumped straight up in
the air you wouldn’t be able to come down in the same place. Their
argument was based on their incorrect Aristotelian assumption that
once the force of gravity began to act on you and bring you back
down, your horizontal motion would stop. In the correct Newtonian
theory, the earth’s downward gravitational force is acting before,
during, and after your jump, but has no eﬀect on your motion in
the perpendicular (horizontal) direction.

If Aristotle had been correct, then we would have a handy way
to determine absolute motion and absolute rest: jump straight up
in the air, and if you land back where you started, the surface from
which you jumped must have been in a state of rest. In reality, this
test gives the same result as long as the surface under you is an
inertial frame. If you try this in a jet plane, you land back on the
same spot on the deck from which you started, regardless of whether
the plane is ﬂying at 500 miles per hour or parked on the runway.
The method would in fact only be good for detecting whether the

Section 6.1

Forces have no perpendicular effects

193

plane was accelerating.

Discussion questions

The following is an incorrect explanation of a fact about target

A
shooting:

“Shooting a high-powered riﬂe with a high muzzle velocity is different from
shooting a less powerful gun. With a less powerful gun, you have to aim
quite a bit above your target, but with a more powerful one you don’t have
to aim so high because the bullet doesn’t drop as fast.”

Explain why it’s incorrect. What is the correct explanation?

You have thrown a rock, and it is ﬂying through the air in an arc. If
B
the earth’s gravitational force on it is always straight down, why doesn’t it
just go straight down once it leaves your hand?

Consider the example of the bullet that is dropped at the same
C
moment another bullet is ﬁred from a gun. What would the motion of the
two bullets look like to a jet pilot ﬂying alongside in the same direction as
the shot bullet and at the same horizontal speed?

6.2 Coordinates and components

’Cause we’re all
Bold as love,
Just ask the axis.

Jimi Hendrix

How do we convert these ideas into mathematics? Figure b shows
a good way of connecting the intuitive ideas to the numbers. In one
dimension, we impose a number line with an x coordinate on a
certain stretch of space. In two dimensions, we imagine a grid of
squares which we label with x and y values, as shown in ﬁgure b.

But of course motion doesn’t really occur in a series of discrete
hops like in chess or checkers. Figure c shows a way of conceptual-
izing the smooth variation of the x and y coordinates. The ball’s
shadow on the wall moves along a line, and we describe its position
with a single coordinate, y, its height above the ﬂoor. The wall
shadow has a constant acceleration of -9.8 m/s2. A shadow on the
ﬂoor, made by a second light source, also moves along a line, and we
describe its motion with an x coordinate, measured from the wall.

c / The shadow on the wall
shows the ball’s y motion,
the
shadow on the ﬂoor its x motion.

194

Chapter 6 Newton’s Laws in Three Dimensions

b / This object experiences a force that pulls it down toward the
In each equal time interval, it moves three units to
bottom of the page.
the right. At the same time, its vertical motion is making a simple pattern
of +1, 0, −1, −2, −3, −4, . . . units. Its motion can be described by an x
coordinate that has zero acceleration and a y coordinate with constant
acceleration. The arrows labeled x and y serve to explain that we are
deﬁning increasing x to the right and increasing y as upward.

The velocity of the ﬂoor shadow is referred to as the x component
of the velocity, written vx. Similarly we can notate the acceleration
of the ﬂoor shadow as ax. Since vx is constant, ax is zero.

Similarly, the velocity of the wall shadow is called vy, its accel-

eration ay. This example has ay = −9.8 m/s2.

Because the earth’s gravitational force on the ball is acting along
the y axis, we say that the force has a negative y component, Fy,
but Fx = Fz = 0.

The general idea is that we imagine two observers, each of whom
perceives the entire universe as if it was ﬂattened down to a single
line. The y-observer, for instance, perceives y, vy, and ay, and will
infer that there is a force, Fy, acting downward on the ball. That
is, a y component means the aspect of a physical phenomenon, such
as velocity, acceleration, or force, that is observable to someone who
can only see motion along the y axis.

All of this can easily be generalized to three dimensions. In the
example above, there could be a z-observer who only sees motion
toward or away from the back wall of the room.

Section 6.2 Coordinates and components

195

A car going over a cliff
example 1
(cid:46) The police ﬁnd a car at a distance w = 20 m from the base of a
cliff of height h = 100 m. How fast was the car going when it went
over the edge? Solve the problem symbolically ﬁrst, then plug in
the numbers.

(cid:46) Let’s choose y pointing up and x pointing away from the cliff.
The car’s vertical motion was independent of its horizontal mo-
tion, so we know it had a constant vertical acceleration of a =
−g = −9.8 m/s2. The time it spent in the air is therefore related
to the vertical distance it fell by the constant-acceleration equa-
tion

d / Example 1.

or

Solving for ∆t gives

∆y =

−h =

1
2

1
2

ay ∆t 2,

(−g)∆t 2.

∆t =

(cid:115)

2h
g

.

Since the vertical force had no effect on the car’s horizontal mo-
tion, it had ax = 0, i.e., constant horizontal velocity. We can apply
the constant-velocity equation

i.e.,

vx =

∆x
∆t

,

vx =

w
∆t

.

We now substitute for ∆t to ﬁnd

which simpliﬁes to

(cid:115)

vx = w/

2h
g

,

vx = w

(cid:114) g
2h

.

Plugging in numbers, we ﬁnd that the car’s speed when it went
over the edge was 4 m/s, or about 10 mi/hr.

Projectiles move along parabolas.

What type of mathematical curve does a projectile follow through
space? To ﬁnd out, we must relate x to y, eliminating t. The rea-
soning is very similar to that used in the example above. Arbitrarily

196

Chapter 6 Newton’s Laws in Three Dimensions

choosing x = y = t = 0 to be at the top of the arc, we conveniently
have x = ∆x, y = ∆y, and t = ∆t, so

y =

1
2

ayt2

x = vxt

(ay < 0)

We solve the second equation for t = x/vx and eliminate t in the
ﬁrst equation:

y =

1
2

ay

(cid:19)2

.

(cid:18) x
vx

Since everything in this equation is a constant except for x and y,
we conclude that y is proportional to the square of x. As you may
or may not recall from a math class, y ∝ x2 describes a parabola.

(cid:46) Solved problem: A cannon

page 200, problem 5

Discussion question

At the beginning of this section I represented the motion of a projec-
A
tile on graph paper, breaking its motion into equal time intervals. Suppose
instead that there is no force on the object at all. It obeys Newton’s ﬁrst law
and continues without changing its state of motion. What would the corre-
sponding graph-paper diagram look like? If the time interval represented
by each arrow was 1 second, how would you relate the graph-paper dia-
gram to the velocity components vx and vy ?

Make up several different coordinate systems oriented in different

B
ways, and describe the ax and ay of a falling object in each one.

6.3 Newton’s laws in three dimensions

It is now fairly straightforward to extend Newton’s laws to three
dimensions:

Newton’s ﬁrst law
If all three components of the total force on an object are zero,
then it will continue in the same state of motion.

Newton’s second law
The components of an object’s acceleration are predicted by
the equations

ax = Fx,total/m,
ay = Fy,total/m,
az = Fz,total/m.

and

e / A parabola can be deﬁned as
the shape made by cutting a cone
parallel to its side. A parabola is
also the graph of an equation of
the form y ∝ x 2.

f / Each water droplet
a parabola.
parabolas are bigger.

follows
The faster drops’

Newton’s third law
If two objects A and B interact via forces, then the compo-
nents of their forces on each other are equal and opposite:

FA on B,x = −FB on A,x,
FA on B,y = −FB on A,y,
FA on B,z = −FB on A,z.

and

g / Example 2.

Section 6.3 Newton’s laws in three dimensions

197

Forces in perpendicular directions on the same objectexample 2
(cid:46) An object is initially at rest. Two constant forces begin acting on
it, and continue acting on it for a while. As suggested by the two
arrows, the forces are perpendicular, and the rightward force is
stronger. What happens?

(cid:46) Aristotle believed, and many students still do, that only one force
can “give orders” to an object at one time. They therefore think
that the object will begin speeding up and moving in the direction
of the stronger force. In fact the object will move along a diagonal.
In the example shown in the ﬁgure, the object will respond to the
large rightward force with a large acceleration component to the
right, and the small upward force will give it a small acceleration
component upward. The stronger force does not overwhelm the
weaker force, or have any effect on the upward motion at all. The
force components simply add together:

Fx,total = F1,x + (cid:8)(cid:8)(cid:8)(cid:42) 0

F2,x

Fy,total = (cid:26)

(cid:26)(cid:26)(cid:62)0
F1,y + F2,y

Discussion question

The ﬁgure shows two trajectories, made by splicing together lines
A
and circular arcs, which are unphysical for an object that is only being
acted on by gravity. Prove that they are impossible based on Newton’s
laws.

198

Chapter 6 Newton’s Laws in Three Dimensions

Summary

Selected vocabulary

component . . . .

parabola . . . . .

Notation

x, y, z . . . . . .
vx, vy, vz . . . . .

ax, ay, az . . . . .

Summary

the part of a velocity, acceleration, or force
that would be perceptible to an observer who
could only see the universe projected along a
certain one-dimensional axis
the mathematical curve whose graph has y
proportional to x2

an object’s positions along the x, y, and z axes
the x, y, and z components of an object’s ve-
locity; the rates of change of the object’s x, y,
and z coordinates
the x, y, and z components of an object’s ac-
celeration; the rates of change of vx, vy, and
vz

A force does not produce any eﬀect on the motion of an object
in a perpendicular direction. The most important application of
this principle is that the horizontal motion of a projectile has zero
acceleration, while the vertical motion has an acceleration equal to g.
That is, an object’s horizontal and vertical motions are independent.
The arc of a projectile is a parabola.

Motion in three dimensions is measured using three coordinates,
x, y, and z. Each of these coordinates has its own corresponding
velocity and acceleration. We say that the velocity and acceleration
both have x, y, and z components

Newton’s second law is readily extended to three dimensions by
rewriting it as three equations predicting the three components of
the acceleration,

ax = Fx,total/m,
ay = Fy,total/m,
az = Fz,total/m,

and likewise for the ﬁrst and third laws.

Summary

199

Problems

A computerized answer check is available online.

Key√
(cid:82) A problem that requires calculus.
(cid:63) A diﬃcult problem.

(a) A ball is thrown straight up with velocity v. Find an
√

1
equation for the height to which it rises.
(b) Generalize your equation for a ball thrown at an angle θ above
horizontal, in which case its initial velocity components are vx =
√
v cos θ and vy = v sin θ.

2
At the 2010 Salinas Lettuce Festival Parade, the Lettuce Queen
drops her bouquet while riding on a ﬂoat moving toward the right.
Sketch the shape of its trajectory in her frame of reference, and
compare with the shape seen by one of her admirers standing on
the sidewalk.

3
Two daredevils, Wendy and Bill, go over Niagara Falls. Wendy
sits in an inner tube, and lets the 30 km/hr velocity of the river throw
her out horizontally over the falls. Bill paddles a kayak, adding an
extra 10 km/hr to his velocity. They go over the edge of the falls
at the same moment, side by side. Ignore air friction. Explain your
reasoning.
(a) Who hits the bottom ﬁrst?
(b) What is the horizontal component of Wendy’s velocity on im-
pact?
(c) What is the horizontal component of Bill’s velocity on impact?
(d) Who is going faster on impact?

A baseball pitcher throws a pitch clocked at vx = 73.3 miles/hour.

4
He throws horizontally. By what amount, d, does the ball drop by
the time it reaches home plate, L = 60.0 feet away?
(a) First ﬁnd a symbolic answer in terms of L, vx, and g.
(b) Plug in and ﬁnd a numerical answer. Express your answer
in units of ft. (Note: 1 foot=12 inches, 1 mile=5280 feet, and 1
inch=2.54 cm)

√

√

Problem 4.

5
A cannon standing on a ﬂat ﬁeld ﬁres a cannonball with a
muzzle velocity v, at an angle θ above horizontal. The cannonball

200

Chapter 6 Newton’s Laws in Three Dimensions

thus initially has velocity components vx = v cos θ and vy = v sin θ.
(a) Show that the cannon’s range (horizontal distance to where the
cannonball falls) is given by the equation R = (2v2/g) sin θ cos θ .
(b) Interpret your equation in the cases of θ = 0 and θ = 90◦.

(cid:46) Solution, p. 553

6
Assuming the result of problem 5 for the range of a projectile,
R = (2v2/g) sin θ cos θ, show that the maximum range is for θ = 45◦.

(cid:82)

7
Two cars go over the same speed bump in a parking lot,
Maria’s Maserati at 25 miles per hour and Park’s Porsche at 37.
How many times greater is the vertical acceleration of the Porsche?
Hint: Remember that acceleration depends both on how much the
velocity changes and on how much time it takes to change.

√

8
You’re running oﬀ a cliﬀ into a pond. The cliﬀ is h = 5.0 m
above the water, but the cliﬀ is not strictly vertical; it slopes down
to the pond at an angle of θ = 20◦ with respect to the vertical. You
want to ﬁnd the minimum speed you need to jump oﬀ the cliﬀ in
order to land in the water.
(a) Find a symbolic answer in terms of h, θ, and g.
(b) Check that the units of your answer to part a make sense.
(c) Check that the dependence on the variables g, h, and θ makes
sense, and check the special cases θ = 0 and θ = 90◦.
(d) Plug in numbers to ﬁnd the numerical result.

√

√

[problem by B. Shotwell]

9
Two footballs, one white and one green, are on the ground and
kicked by two diﬀerent footballers. The white ball, which is kicked
straight upward with initial speed v0, rises to height H. The green
ball is hit with twice the initial speed but reaches the same height.
(a) What is the y-component of the green ball’s initial velocity vec-
√
tor? Give your answer in terms of v0 alone.
(b) Which ball is in the air for a longer amount of time?
(c) What is the range of the green ball? Your answer should only
depend on H.

[problem by B. Shotwell]

√

10

This problem is now problem 26 on p. 238.

11
The ﬁgure shows a vertical cross-section of a cylinder. A gun
at the top shoots a bullet horizontally. What is the minimum speed
at which the bullet must be shot in order to completely clear the
(cid:63)
cylinder?

Problem 8.

Problem 11.

Problems

201

202

Chapter 6 Newton’s Laws in Three Dimensions

Chapter 7
Vectors

7.1 Vector notation

The idea of components freed us from the conﬁnes of one-dimensional
physics, but the component notation can be unwieldy, since every
one-dimensional equation has to be written as a set of three separate
equations in the three-dimensional case. Newton was stuck with the
component notation until the day he died, but eventually someone
suﬃciently lazy and clever ﬁgured out a way of abbreviating three
equations as one.

(a)

−→
F A on B = −

−→
F B on A

stands for

(b)

−→
F total =

−→
F 1 +

−→
F 2 + . . .

stands for

(c) −→a = ∆−→v

∆t

stands for

FA on B,x = −FB on A,x
FA on B,y = −FB on A,y
FA on B,z = −FB on A,z
Ftotal,x = F1,x + F2,x + . . .
Ftotal,y = F1,y + F2,y + . . .
Ftotal,z = F1,z + F2,z + . . .
ax = ∆vx/∆t
ay = ∆vy/∆t
az = ∆vz/∆t

Example (a) shows both ways of writing Newton’s third law. Which
would you rather write?

a / Vectors are used in aerial nav-
igation.

203

The idea is that each of the algebra symbols with an arrow writ-
ten on top, called a vector, is actually an abbreviation for three
diﬀerent numbers, the x, y, and z components. The three compo-
nents are referred to as the components of the vector, e.g., Fx is the
−→
F . The notation with an arrow on top
x component of the vector
is good for handwritten equations, but is unattractive in a printed
book, so books use boldface, F, to represent vectors. After this
point, I’ll use boldface for vectors throughout this book.

Quantities can be classiﬁed as vectors or scalars. In a phrase like
“a
to the northeast,” it makes sense to ﬁll in the blank with
“force” or “velocity,” which are vectors, but not with “mass” or
“time,” which are scalars. Any nonzero vector has both a direction
and an amount. The amount is called its magnitude. The notation
for the magnitude of a vector A is |A|, like the absolute value sign
used with scalars.

Often, as in example (b), we wish to use the vector notation to
represent adding up all the x components to get a total x component,
etc. The plus sign is used between two vectors to indicate this type
of component-by-component addition. Of course, vectors are really
triplets of numbers, not numbers, so this is not the same as the use
of the plus sign with individual numbers. But since we don’t want to
have to invent new words and symbols for this operation on vectors,
we use the same old plus sign, and the same old addition-related
words like “add,” “sum,” and “total.” Combining vectors this way
is called vector addition.

Similarly, the minus sign in example (a) was used to indicate
negating each of the vector’s three components individually. The
equals sign is used to mean that all three components of the vector
on the left side of an equation are the same as the corresponding
components on the right.

Example (c) shows how we abuse the division symbol in a similar
manner. When we write the vector ∆v divided by the scalar ∆t,
we mean the new vector formed by dividing each one of the velocity
components by ∆t.

It’s not hard to imagine a variety of operations that would com-
bine vectors with vectors or vectors with scalars, but only four of
them are required in order to express Newton’s laws:

204

Chapter 7 Vectors

operation
vector + vector Add component by component to

deﬁnition

make a new set of three numbers.

vector · scalar

vector − vector Subtract component by component
to make a new set of three numbers.
Multiply each component of the vec-
tor by the scalar.
Divide each component of the vector
by the scalar.

vector/scalar

As an example of an operation that is not useful for physics, there
just aren’t any useful physics applications for dividing a vector by
another vector component by component. In optional section 7.5,
we discuss in more detail the fundamental reasons why some vector
operations are useful and others useless.

We can do algebra with vectors, or with a mixture of vectors
and scalars in the same equation. Basically all the normal rules of
algebra apply, but if you’re not sure if a certain step is valid, you
should simply translate it into three component-based equations and
see if it works.

Order of addition
example 1
(cid:46) If we are adding two force vectors, F + G, is it valid to assume
as in ordinary algebra that F + G is the same as G + F?

(cid:46) To tell if this algebra rule also applies to vectors, we simply
translate the vector notation into ordinary algebra notation.
In
terms of ordinary numbers, the components of the vector F + G
would be Fx + Gx , Fy + Gy , and Fz + Gz, which are certainly the
same three numbers as Gx + Fx , Gy + Fy , and Gz + Fz. Yes, F + G
is the same as G + F.

It is useful to deﬁne a symbol r for the vector whose components

are x, y, and z, and a symbol ∆r made out of ∆x, ∆y, and ∆z.

Although this may all seem a little formidable, keep in mind that
it amounts to nothing more than a way of abbreviating equations!
Also, to keep things from getting too confusing the remainder of this
chapter focuses mainly on the ∆r vector, which is relatively easy to
visualize.

self-check A
Translate the equations vx = ∆x/∆t, vy = ∆y /∆t, and vz = ∆z/∆t for
motion with constant velocity into a single equation in vector notation.
(cid:46) Answer, p. 566

Section 7.1 Vector notation

205

Drawing vectors as arrows

A vector in two dimensions can be easily visualized by drawing
an arrow whose length represents its magnitude and whose direction
represents its direction. The x component of a vector can then be
visualized as the length of the shadow it would cast in a beam of
light projected onto the x axis, and similarly for the y component.
Shadows with arrowheads pointing back against the direction of the
positive axis correspond to negative components.

In this type of diagram, the negative of a vector is the vector
with the same magnitude but in the opposite direction. Multiplying
a vector by a scalar is represented by lengthening the arrow by that
factor, and similarly for division.

b / The x and y components
of a vector can be thought of as
the shadows it casts onto the x
and y axes.

self-check B
Given vector Q represented by an arrow in ﬁgure c, draw arrows repre-
(cid:46) Answer, p.
senting the vectors 1.5Q and −Q.
566

This leads to a way of deﬁning vectors and scalars that reﬂects

how physicists think in general about these things:

c / Self-check B.

deﬁnition of vectors and scalars

A general type of measurement (force, velocity, . . . ) is a vector if it
can be drawn as an arrow so that rotating the paper produces the
same result as rotating the actual quantity. A type of quantity that
never changes at all under rotation is a scalar.

For example, a force reverses itself under a 180-degree rotation,
but a mass doesn’t. We could have deﬁned a vector as something
that had both a magnitude and a direction, but that would have left
out zero vectors, which don’t have a direction. A zero vector is a
legitimate vector, because it behaves the same way under rotations
as a zero-length arrow, which is simply a dot.

A remark for those who enjoy brain-teasers: not everything is
a vector or a scalar. An American football is distorted compared
to a sphere, and we can measure the orientation and amount of
that distortion quantitatively. The distortion is not a vector, since
a 180-degree rotation brings it back to its original state. Something
similar happens with playing cards, ﬁgure d. For some subatomic
particles, such as electrons, 360 degrees isn’t even enough; a 720-
degree rotation is needed to put them back the way they were!

Discussion questions

You drive to your friend’s house. How does the magnitude of your ∆r

A
vector compare with the distance you’ve added to the car’s odometer?

d / A playing card returns to
its original state when rotated by
180 degrees.

206

Chapter 7 Vectors

7.2 Calculations with magnitude and direction

If you ask someone where Las Vegas is compared to Los Angeles,
they are unlikely to say that the ∆x is 290 km and the ∆y is 230
km, in a coordinate system where the positive x axis is east and the
y axis points north. They will probably say instead that it’s 370
km to the northeast. If they were being precise, they might give the
direction as 38◦ counterclockwise from east. In two dimensions, we
can always specify a vector’s direction like this, using a single angle.
A magnitude plus an angle suﬃce to specify everything about the
vector. The following two examples show how we use trigonometry
and the Pythagorean theorem to go back and forth between the x−y
and magnitude-angle descriptions of vectors.

Finding magnitude and angle from components
example 2
(cid:46) Given that the ∆r vector from LA to Las Vegas has ∆x = 290 km
and ∆y = 230 km, how would we ﬁnd the magnitude and direction
of ∆r?

(cid:46) We ﬁnd the magnitude of ∆r from the Pythagorean theorem:

|∆r| =

(cid:113)

∆x 2 + ∆y 2

= 370 km

We know all three sides of the triangle, so the angle θ can be
found using any of the inverse trig functions. For example, we
know the opposite and adjacent sides, so

e / Examples 2 and 3.

θ = tan−1 ∆y
∆x
= 38◦.

example 3
Finding components from magnitude and angle
(cid:46) Given that the straight-line distance from Los Angeles to Las
Vegas is 370 km, and that the angle θ in the ﬁgure is 38◦, how
can the x and y components of the ∆r vector be found?

(cid:46) The sine and cosine of θ relate the given information to the
information we wish to ﬁnd:

cos θ =

sin θ =

∆x
|∆r|
∆y
|∆r|

Solving for the unknowns gives

∆x = |∆r| cos θ
= 290 km

∆y = |∆r| sin θ
= 230 km.

and

Section 7.2 Calculations with magnitude and direction

207

The following example shows the correct handling of the plus

and minus signs, which is usually the main cause of mistakes.

example 4
Negative components
(cid:46) San Diego is 120 km east and 150 km south of Los Angeles. An
airplane pilot is setting course from San Diego to Los Angeles. At
what angle should she set her course, measured counterclock-
wise from east, as shown in the ﬁgure?

(cid:46) If we make the traditional choice of coordinate axes, with x
pointing to the right and y pointing up on the map, then her ∆x is
negative, because her ﬁnal x value is less than her initial x value.
Her ∆y is positive, so we have

f / Example 4.

∆x = −120 km
∆y = 150 km.

If we work by analogy with example 2, we get

θ = tan−1 ∆y
∆x
= tan−1(−1.25)
= −51◦.

According to the usual way of deﬁning angles in trigonometry,
a negative result means an angle that lies clockwise from the x
axis, which would have her heading for the Baja California. What
went wrong? The answer is that when you ask your calculator to
take the arctangent of a number, there are always two valid pos-
sibilities differing by 180◦. That is, there are two possible angles
whose tangents equal -1.25:

tan 129◦ = −1.25
tan −51◦ = −1.25

You calculator doesn’t know which is the correct one, so it just
picks one. In this case, the one it picked was the wrong one, and
it was up to you to add 180◦to it to ﬁnd the right answer.

208

Chapter 7 Vectors

g / Example 5.

A shortcut
example 5
(cid:46) A split second after nine o’clock, the hour hand on a clock dial
has moved clockwise past the nine-o’clock position by some im-
perceptibly small angle φ. Let positive x be to the right and posi-
tive y up. If the hand, with length (cid:96), is represented by a ∆r vector
going from the dial’s center to the tip of the hand, ﬁnd this vector’s
∆x.

(cid:46) The following shortcut is the easiest way to work out examples
like these, in which a vector’s direction is known relative to one
of the axes. We can tell that ∆r will have a large, negative x
component and a small, positive y . Since ∆x < 0, there are
really only two logical possibilities: either ∆x = −(cid:96) cos φ, or ∆x =
−(cid:96) sin φ. Because φ is small, cos φ is large and sin φ is small.
We conclude that ∆x = −(cid:96) cos φ.

A typical application of this technique to force vectors is given in
example 6 on p. 226.

Discussion question

In example 4, we dealt with components that were negative. Does it

A
make sense to classify vectors as positive and negative?

Section 7.2 Calculations with magnitude and direction

209

7.3 Techniques for adding vectors

Vector addition is one of the three essential mathematical skills,
summarized on pp.545-546, that you need for success in this course.

Addition of vectors given their components

The easiest type of vector addition is when you are in possession
of the components, and want to ﬁnd the components of their sum.

Adding components
example 6
(cid:46) Given the ∆x and ∆y values from the previous examples, ﬁnd
the ∆x and ∆y from San Diego to Las Vegas.

(cid:46)

∆xtotal = ∆x1 + ∆x2

= −120 km + 290 km

= 170 km

∆ytotal = ∆y1 + ∆y2

= 150 km + 230 km

= 380

h / Example 6.

Note how the signs of the x components take care of the west-
ward and eastward motions, which partially cancel.

i / Vectors can be added graph-
ically by placing them tip to tail,
and then drawing a vector from
the tail of the ﬁrst vector to the tip
of the second vector.

Addition of vectors given their magnitudes and directions

In this case, you must ﬁrst translate the magnitudes and di-
rections into components, and the add the components. In our San
Diego-Los Angeles-Las Vegas example, we can simply string together
the preceding examples; this is done on p. 546.

Graphical addition of vectors

Often the easiest way to add vectors is by making a scale drawing
on a piece of paper. This is known as graphical addition, as opposed
to the analytic techniques discussed previously. (It has nothing to
do with x − y graphs or graph paper. “Graphical” here simply
means drawing. It comes from the Greek verb “grapho,” to write,
like related English words including “graphic.”)

210

Chapter 7 Vectors

LA to Vegas, graphically
example 7
(cid:46) Given the magnitudes and angles of the ∆r vectors from San
Diego to Los Angeles and from Los Angeles to Las Vegas, ﬁnd
the magnitude and angle of the ∆r vector from San Diego to Las
Vegas.

(cid:46) Using a protractor and a ruler, we make a careful scale draw-
ing, as shown in ﬁgure j. The protractor can be conveniently
aligned with the blue rules on the notebook paper. A scale of
1 mm → 2 km was chosen for this solution because it was as big
as possible (for accuracy) without being so big that the drawing
wouldn’t ﬁt on the page. With a ruler, we measure the distance
from San Diego to Las Vegas to be 206 mm, which corresponds
to 412 km. With a protractor, we measure the angle θ to be 65◦.

j / Example 7.

Even when we don’t intend to do an actual graphical calculation
with a ruler and protractor, it can be convenient to diagram the
addition of vectors in this way. With ∆r vectors, it intuitively makes
sense to lay the vectors tip-to-tail and draw the sum vector from the
tail of the ﬁrst vector to the tip of the second vector. We can do
the same when adding other vectors such as force vectors.

self-check C
How would you subtract vectors graphically?

(cid:46) Answer, p. 566

Section 7.3

Techniques for adding vectors

211

Discussion questions

If you’re doing graphical addition of vectors, does it matter which
A
vector you start with and which vector you start from the other vector’s
tip?

If you add a vector with magnitude 1 to a vector of magnitude 2,

B
what magnitudes are possible for the vector sum?

Which of these examples of vector addition are correct, and which

C
are incorrect?

7.4 (cid:63) Unit vector notation
When we want to specify a vector by its components, it can be cum-
bersome to have to write the algebra symbol for each component:

∆x = 290 km, ∆y = 230 km

A more compact notation is to write

∆r = (290 km)ˆx + (230 km)ˆy,

where the vectors ˆx, ˆy, and ˆz, called the unit vectors, are deﬁned
as the vectors that have magnitude equal to 1 and directions lying
along the x, y, and z axes. In speech, they are referred to as “x-hat”
and so on.

A slightly diﬀerent, and harder to remember, version of this
notation is unfortunately more prevalent. In this version, the unit
vectors are called ˆi, ˆj, and ˆk:

∆r = (290 km)ˆi + (230 km)ˆj.

212

Chapter 7 Vectors

7.5 (cid:63) Rotational invariance
Let’s take a closer look at why certain vector operations are use-
ful and others are not. Consider the operation of multiplying two
vectors component by component to produce a third vector:

Rx = PxQx
Ry = PyQy
Rz = PzQz

As a simple example, we choose vectors P and Q to have length
1, and make them perpendicular to each other, as shown in ﬁgure
k/1. If we compute the result of our new vector operation using the
coordinate system in k/2, we ﬁnd:

Rx = 0
Ry = 0
Rz = 0.

The x component is zero because Px = 0, the y component is zero
because Qy = 0, and the z component is of course zero because
both vectors are in the x − y plane. However, if we carry out the
same operations in coordinate system k/3, rotated 45 degrees with
respect to the previous one, we ﬁnd

Rx = 1/2
Ry = −1/2
Rz = 0.

The operation’s result depends on what coordinate system we use,
and since the two versions of R have diﬀerent lengths (one being zero
and the other nonzero), they don’t just represent the same answer
expressed in two diﬀerent coordinate systems. Such an operation
will never be useful in physics, because experiments show physics
works the same regardless of which way we orient the laboratory
building! The useful vector operations, such as addition and scalar
multiplication, are rotationally invariant, i.e., come out the same
regardless of the orientation of the coordinate system.

Calibrating an electronic compass
example 8
Some smart phones and GPS units contain electronic compasses
that can sense the direction of the earth’s magnetic ﬁeld vector,
notated B. Because all vectors work according to the same rules,
you don’t need to know anything special about magnetism in or-
der to understand this example. Unlike a traditional compass that
uses a magnetized needle on a bearing, an electronic compass
has no moving parts. It contains two sensors oriented perpendic-
ular to one another, and each sensor is only sensitive to the com-
ponent of the earth’s ﬁeld that lies along its own axis. Because a

k / Component-by-component
multiplication of the vectors in 1
would produce different vectors
in coordinate systems 2 and 3.

Section 7.5

(cid:63) Rotational invariance

213

choice of coordinates is arbitrary, we can take one of these sen-
sors as deﬁning the x axis and the other the y . Given the two
components Bx and By , the device’s computer chip can compute
the angle of magnetic north relative to its sensors, tan−1(By /Bx ).

All compasses are vulnerable to errors because of nearby mag-
netic materials, and in particular it may happen that some part
of the compass’s own housing becomes magnetized. In an elec-
tronic compass, rotational invariance provides a convenient way
of calibrating away such effects by having the user rotate the de-
vice in a horizontal circle.

Suppose that when the compass is oriented in a certain way, it
measures Bx = 1.00 and By = 0.00 (in certain units). We then
expect that when it is rotated 90 degrees clockwise, the sensors
will detect Bx = 0.00 and By = 1.00.

But imagine instead that we get Bx = 0.20 and By = 0.80. This
would violate rotational invariance, since rotating the coordinate
system is supposed to give a different description of the same
vector. The magnitude appears to have changed from 1.00 to
√
0.202 + 0.802 = 0.82, and a vector can’t change its magnitude
just because you rotate it. The compass’s computer chip ﬁgures
out that some effect, possibly a slight magnetization of its hous-
ing, must be adding an erroneous 0.2 units to all the Bx readings,
because subtracting this amount from all the Bx values gives vec-
tors that have the same magnitude, satisfying rotational invari-
ance.

214

Chapter 7 Vectors

Summary

Selected vocabulary

vector . . . . . . .

magnitude . . . .
scalar . . . . . . .

a quantity that has both an amount (magni-
tude) and a direction in space
the “amount” associated with a vector
a quantity that has no direction in space, only
an amount

Notation

A . . . . . . . . .
−→
A . . . . . . . . .
|A|
. . . . . . . .
r . . . . . . . . . .
∆r . . . . . . . . .

ˆx, ˆy, ˆz . . . . . .

ˆi, ˆj, ˆk . . . . . . .

a vector with components Ax, Ay, and Az
handwritten notation for a vector
the magnitude of vector A
the vector whose components are x, y, and z
the vector whose components are ∆x, ∆y, and
∆z
(optional topic) unit vectors; the vectors with
magnitude 1 lying along the x, y, and z axes
a harder to remember notation for the unit
vectors

Other terminology and notation

displacement vec-
tor . . . . . . . . .
speed . . . . . . .

Summary

a name for the symbol ∆r

the magnitude of the velocity vector, i.e., the
velocity stripped of any information about its
direction

A vector is a quantity that has both a magnitude (amount) and
a direction in space, as opposed to a scalar, which has no direction.
The vector notation amounts simply to an abbreviation for writing
the vector’s three components.

In two dimensions, a vector can be represented either by its two
components or by its magnitude and direction. The two ways of
describing a vector can be related by trigonometry.

The two main operations on vectors are addition of a vector to

a vector, and multiplication of a vector by a scalar.

Vector addition means adding the components of two vectors
to form the components of a new vector. In graphical terms, this
corresponds to drawing the vectors as two arrows laid tip-to-tail and
drawing the sum vector from the tail of the ﬁrst vector to the tip
of the second one. Vector subtraction is performed by negating the
vector to be subtracted and then adding.

Multiplying a vector by a scalar means multiplying each of its
components by the scalar to create a new vector. Division by a
scalar is deﬁned similarly.

Summary

215

Problem 1.

Problems

A computerized answer check is available online.

Key√
(cid:82) A problem that requires calculus.
(cid:63) A diﬃcult problem.

1
The ﬁgure shows vectors A and B. Graphically calculate
the following, as in ﬁgure i on p. 210, self-check C on p. 211, and
self-check B on p. 206.

A + B, A − B, B − A, −2B, A − 2B

No numbers are involved.

Phnom Penh is 470 km east and 250 km south of Bangkok.

2
Hanoi is 60 km east and 1030 km north of Phnom Penh.
(a) Choose a coordinate system, and translate these data into ∆x
and ∆y values with the proper plus and minus signs.
(b) Find the components of the ∆r vector pointing from Bangkok
√
to Hanoi.

If you walk 35 km at an angle 25◦ counterclockwise from east,
3
and then 22 km at 230◦ counterclockwise from east, ﬁnd the distance
√
and direction from your starting point to your destination.

4
A machinist is drilling holes in a piece of aluminum according
to the plan shown in the ﬁgure. She starts with the top hole, then
moves to the one on the left, and then to the one on the right. Since
this is a high-precision job, she ﬁnishes by moving in the direction
and at the angle that should take her back to the top hole, and
checks that she ends up in the same place. What are the distance
√
and direction from the right-hand hole to the top one?

Problem 4.

216

Chapter 7 Vectors

5
Suppose someone proposes a new operation in which a vector
A and a scalar B are added together to make a new vector C like
this:

Cx = Ax + B
Cy = Ay + B
Cz = Az + B

Prove that this operation won’t be useful in physics, because it’s
not rotationally invariant.

Problems

217

218

Chapter 7 Vectors

Chapter 8
Vectors and Motion

In 1872, capitalist and former California governor Leland Stanford
asked photographer Eadweard Muybridge if he would work for him
on a project to settle a $25,000 bet (a princely sum at that time).
Stanford’s friends were convinced that a trotting horse always had
at least one foot on the ground, but Stanford claimed that there was
a moment during each cycle of the motion when all four feet were
in the air. The human eye was simply not fast enough to settle the
question. In 1878, Muybridge ﬁnally succeeded in producing what
amounted to a motion picture of the horse, showing conclusively
that all four feet did leave the ground at one point. (Muybridge was
a colorful ﬁgure in San Francisco history, and his acquittal for the
murder of his wife’s lover was considered the trial of the century in
California.)

The losers of the bet had probably been inﬂuenced by Aris-
totelian reasoning, for instance the expectation that a leaping horse
would lose horizontal velocity while in the air with no force to push
it forward, so that it would be more eﬃcient for the horse to run
without leaping. But even for students who have converted whole-

219

heartedly to Newtonianism, the relationship between force and ac-
celeration leads to some conceptual diﬃculties, the main one being
a problem with the true but seemingly absurd statement that an
object can have an acceleration vector whose direction is not the
same as the direction of motion. The horse, for instance, has nearly
constant horizontal velocity, so its ax is zero. But as anyone can tell
you who has ridden a galloping horse, the horse accelerates up and
down. The horse’s acceleration vector therefore changes back and
forth between the up and down directions, but is never in the same
direction as the horse’s motion.
In this chapter, we will examine
more carefully the properties of the velocity, acceleration, and force
vectors. No new principles are introduced, but an attempt is made
to tie things together and show examples of the power of the vector
formulation of Newton’s laws.

racing

greyhound’s
a / The
velocity vector is in the direction
of its motion, i.e., tangent to its
curved path.

8.1 The velocity vector

For motion with constant velocity, the velocity vector is

v = ∆r/∆t.

[only for constant velocity]

The ∆r vector points in the direction of the motion, and dividing
it by the scalar ∆t only changes its length, not its direction, so the
velocity vector points in the same direction as the motion. When the
velocity is not constant, i.e., when the x − t, y − t, and z − t graphs
are not all linear, we use the slope-of-the-tangent-line approach to
deﬁne the components vx, vy, and vz, from which we assemble the
velocity vector. Even when the velocity vector is not constant, it
still points along the direction of motion.

Vector addition is the correct way to generalize the one-dimensional

concept of adding velocities in relative motion, as shown in the fol-
lowing example:

Velocity vectors in relative motion
example 1
(cid:46) You wish to cross a river and arrive at a dock that is directly
across from you, but the river’s current will tend to carry you
downstream. To compensate, you must steer the boat at an an-
gle. Find the angle θ, given the magnitude, |vW L|, of the water’s
velocity relative to the land, and the maximum speed, |vBW |, of
which the boat is capable relative to the water.

(cid:46) The boat’s velocity relative to the land equals the vector sum of
its velocity with respect to the water and the water’s velocity with
respect to the land,

vBL = vBW + vW L.

If the boat is to travel straight across the river, i.e., along the y
axis, then we need to have vBL,x = 0. This x component equals
the sum of the x components of the other two vectors,

vBL,x = vBW ,x + vW L,x ,

b / Example 1.

220

Chapter 8 Vectors and Motion

or

0 = −|vBW | sin θ + |vW L|.

Solving for θ, we ﬁnd sin θ = |vW L|/|vBW |, so

θ = sin−1 |vW L|
|vBW |

.

(cid:46) Solved problem: Annie Oakley

page 234, problem 8

Discussion questions

Is it possible for an airplane to maintain a constant velocity vector
A
but not a constant |v|? How about the opposite – a constant |v| but not a
constant velocity vector? Explain.

New York and Rome are at about the same latitude, so the earth’s
B
rotation carries them both around nearly the same circle. Do the two cities
have the same velocity vector (relative to the center of the earth)? If not,
is there any way for two cities to have the same velocity vector?

8.2 The acceleration vector

When the acceleration is constant, we can deﬁne the acceleration
vector as

c / A change in the magni-
tude of the velocity vector implies
an acceleration.

a = ∆v/∆t,

[only for constant acceleration]

which can be written in terms of initial and ﬁnal velocities as

a = (vf − vi)/∆t.

[only for constant acceleration]

Otherwise, we can use the type of graphical deﬁnition described in
section 8.1 for the velocity vector.

Now there are two ways in which we could have a nonzero accel-
eration. Either the magnitude or the direction of the velocity vector
could change. This can be visualized with arrow diagrams as shown
in ﬁgures c and d. Both the magnitude and direction can change
simultaneously, as when a car accelerates while turning. Only when
the magnitude of the velocity changes while its direction stays con-
stant do we have a ∆v vector and an acceleration vector along the
same line as the motion.

self-check A
(1) In ﬁgure c, is the object speeding up, or slowing down? (2) What
would the diagram look like if vi was the same as vf ? (3) Describe how
the ∆v vector is different depending on whether an object is speeding
(cid:46) Answer, p. 566
up or slowing down.

d / A change in the direction
of
the velocity vector also pro-
duces a nonzero ∆v vector, and
acceleration
a
thus
vector, ∆v/∆t.

nonzero

Section 8.2

The acceleration vector

221

The acceleration vector points in the direction that an accelerom-

eter would point, as in ﬁgure e.

e / The car has just swerved to
the right. The air freshener hang-
ing from the rear-view mirror acts
as an accelerometer, showing
that the acceleration vector is to
the right.

self-check B
In projectile motion, what direction does the acceleration vector have?
(cid:46) Answer, p. 566

f / Example 2.

Rappelling
example 2
In ﬁgure f, the rappeller’s velocity has long periods of gradual
change interspersed with short periods of rapid change. These
correspond to periods of small acceleration and force, and peri-
ods of large acceleration and force.

222

Chapter 8 Vectors and Motion

g / Example 3.

The galloping horse
example 3
Figure g on page 223 shows outlines traced from the ﬁrst, third,
ﬁfth, seventh, and ninth frames in Muybridge’s series of pho-
tographs of the galloping horse. The estimated location of the
horse’s center of mass is shown with a circle, which bobs above
and below the horizontal dashed line.

If we don’t care about calculating velocities and accelerations in
any particular system of units, then we can pretend that the time
between frames is one unit. The horse’s velocity vector as it
moves from one point to the next can then be found simply by
drawing an arrow to connect one position of the center of mass to
the next. This produces a series of velocity vectors which alter-
nate between pointing above and below horizontal.

The ∆v vector is the vector which we would have to add onto one
velocity vector in order to get the next velocity vector in the series.
The ∆v vector alternates between pointing down (around the time
when the horse is in the air, B) and up (around the time when the
horse has two feet on the ground, D).

Section 8.2

The acceleration vector

223

Discussion questions

A When a car accelerates, why does a bob hanging from the rearview
mirror swing toward the back of the car? Is it because a force throws it
backward? If so, what force? Similarly, describe what happens in the
other cases described above.

Superman is guiding a crippled spaceship into port. The ship’s
B
engines are not working. If Superman suddenly changes the direction of
his force on the ship, does the ship’s velocity vector change suddenly? Its
acceleration vector? Its direction of motion?

8.3 The force vector and simple machines

Force is relatively easy to intuit as a vector. The force vector points
in the direction in which it is trying to accelerate the object it is
acting on.

Since force vectors are so much easier to visualize than accel-
eration vectors, it is often helpful to ﬁrst ﬁnd the direction of the
(total) force vector acting on an object, and then use that to ﬁnd
the direction of the acceleration vector. Newton’s second law tells
us that the two must be in the same direction.

A component of a force vector
example 4
Figure h, redrawn from a classic 1920 textbook, shows a boy
pulling another child on a sled. His force has both a horizontal
component and a vertical one, but only the horizontal one accel-
erates the sled. (The vertical component just partially cancels the
force of gravity, causing a decrease in the normal force between
the runners and the snow.) There are two triangles in the ﬁgure.
One triangle’s hypotenuse is the rope, and the other’s is the mag-
nitude of the force. These triangles are similar, so their internal
angles are all the same, but they are not the same triangle. One
is a distance triangle, with sides measured in meters, the other
a force triangle, with sides in newtons.
In both cases, the hori-
It does not make
zontal leg is 93% as long as the hypotenuse.
sense, however, to compare the sizes of the triangles — the force
triangle is not smaller in any meaningful sense.

h / Example 4.

224

Chapter 8 Vectors and Motion

Pushing a block up a ramp
example 5
(cid:46) Figure i shows a block being pushed up a frictionless ramp at
constant speed by an externally applied force FA. How much
force is required, in terms of the block’s mass, m, and the angle
of the ramp, θ?

(cid:46) We analyze the forces on the block and introduce notation for
the other forces besides FA:

i / The applied force FA pushes
the block up the frictionless ramp.

3rd-law partner

force acting on block
ramp’s normal force on block, block’s normal force on ramp,
FN ,
external object’s force on
block (type irrelevant), FA
planet earth’s gravitational
↓
force on block, FW

block’s force on external ob-
ject (type irrelevant),
block’s gravitational force on
↑
earth,

Because the block is being pushed up at constant speed, it has
zero acceleration, and the total force on it must be zero. From
ﬁgure j, we ﬁnd

|FA| = |FW | sin θ
= mg sin θ.

j / If
the block is to move at
constant velocity, Newton’s ﬁrst
law says that
the three force
vectors acting on it must add
up to zero. To perform vector
addition, we put the vectors tip
to tail, and in this case we are
adding three vectors, so each
one’s tail goes against the tip of
the previous one. Since they are
supposed to add up to zero, the
third vector’s tip must come back
to touch the tail of the ﬁrst vector.
They form a triangle, and since
the applied force is perpendicular
to the normal force, it is a right
triangle.

Since the sine is always less than one, the applied force is always
less than mg, i.e., pushing the block up the ramp is easier than
lifting it straight up. This is presumably the principle on which the
pyramids were constructed:
the ancient Egyptians would have
had a hard time applying the forces of enough slaves to equal the
full weight of the huge blocks of stone.

Essentially the same analysis applies to several other simple ma-
chines, such as the wedge and the screw.

Section 8.3

The force vector and simple machines

225

k / Example 6 and problem 18 on
p. 237.

A layback
example 6
The ﬁgure shows a rock climber using a technique called a lay-
back. He can make the normal forces FN1 and FN2 large, which
has the side-effect of increasing the frictional forces FF 1 and FF 2,
so that he doesn’t slip down due to the gravitational (weight) force
FW . The purpose of the problem is not to analyze all of this in de-
tail, but simply to practice ﬁnding the components of the forces
based on their magnitudes. To keep the notation simple, let’s
write FN1 for |FN1|, etc. The crack overhangs by a small, positive
angle θ ≈ 9◦.

In this example, we determine the x component of FN1. The other
nine components are left as an exercise to the reader (problem
18, p. 237).

The easiest method is the one demonstrated in example 5 on
p. 209. Casting vector FN1’s shadow on the ground, we can tell
that it would point to the left, so its x component is negative. The
only two possibilities for its x component are therefore −FN1 cos θ
or −FN1 sin θ. We expect this force to have a large x component
and a much smaller y. Since θ is small, cos θ ≈ 1, while sin θ is
small. Therefore the x component must be −FN1 cos θ.

Pushing a broom
example 7
(cid:46) Figure l shows a man pushing a broom at an angle θ relative to
the horizontal. The mass m of the broom is concentrated at the
brush. If the magnitude of the broom’s acceleration is a, ﬁnd the
force FH that the man must make on the handle.

(cid:46) First we analyze all the forces on the brush.

l / Example 7.

226

Chapter 8 Vectors and Motion

force acting on brush
handle’s normal force
on brush, FH ,
earth’s gravitational force
on brush, mg,
ﬂoor’s normal force
on brush, FN ,
ﬂoor’s kinetic friction force
on brush, Fk ,

Newton’s second law is:

3rd-law partner
brush’s normal force
on handle,
brush’s gravitational force

↓ on earth,

brush’s normal force

↑ on ﬂoor,

↑

↓

brush’s kinetic friction force

← on ﬂoor,

→

,

a =

FH + mg + FN + Fk
m
where the addition is vector addition. If we actually want to carry
out the vector addition of the forces, we have to do either graph-
ical addition (as in example 5) or analytic addition. Let’s do an-
alytic addition, which means ﬁnding all the components of the
forces, adding the x’s, and adding the y’s.

Most of the forces have components that are trivial to express in
terms of their magnitudes, the exception being FH , whose com-
ponents we can determine using the technique demonstrated in
example 5 on p. 209 and example 6 on p. 226. Using the coordi-
nate system shown in the ﬁgure, the results are:

FHx = FH cos θ FHy = −FH sin θ
mgx = 0
FNx = 0
Fkx = −Fk

mgy = −mg
FNy = FN
Fky = 0

Note that we don’t yet know the magnitudes FH , FN , and Fk .
That’s all right. First we need to set up Newton’s laws, and then
we can worry about solving the equations.

Newton’s second law in the x direction gives:

[1]

a =

FH cos θ − Fk
m

The acceleration in the vertical direction is zero, so Newton’s sec-
ond law in the y direction tells us that

[2]

0 = −FH sin θ − mg + FN .

Finally, we have the relationship between kinetic friction and the
normal force,

[3]

Fk = µk FN .

Equations [1]-[3] are three equations, which we can use to de-
termine the three unknowns, FH , FN , and Fk . Straightforward
algebra gives

FH = m

(cid:18)

a + µk g
cos θ − µk sin θ

(cid:19)

Section 8.3

The force vector and simple machines

227

(cid:46) Solved problem: A cargo plane

page 234, problem 9

(cid:46) Solved problem: The angle of repose

page 235, problem 11

(cid:46) Solved problem: A wagon

page 234, problem 10

Discussion question A.

Discussion questions

Discussion question B.

A The ﬁgure shows a block being pressed diagonally upward against a
wall, causing it to slide up the wall. Analyze the forces involved, including
their directions.

B The ﬁgure shows a roller coaster car rolling down and then up under
the inﬂuence of gravity. Sketch the car’s velocity vectors and acceleration
vectors. Pick an interesting point in the motion and sketch a set of force
vectors acting on the car whose vector sum could have resulted in the
right acceleration vector.

8.4 (cid:82) Calculus with vectors
Using the unit vector notation introduced in section 7.4, the deﬁni-
tions of the velocity and acceleration components given in chapter
6 can be translated into calculus notation as

and

v =

dx
dt

ˆx +

dy
dt

ˆy +

dz
dt

ˆz

a =

dvx
dt

ˆx +

dvy
dt

ˆy +

dvz
dt

ˆz.

To make the notation less cumbersome, we generalize the concept
of the derivative to include derivatives of vectors, so that we can
abbreviate the above equations as

and

v =

dr
dt

a =

dv
dt

.

In words, to take the derivative of a vector, you take the derivatives
of its components and make a new vector out of those. This deﬁni-
tion means that the derivative of a vector function has the familiar
properties

d(cf )
dt

= c

df
dt

[c is a constant]

and

d(f + g)
dt

dg
dt
The integral of a vector is likewise deﬁned as integrating component
by component.

df
dt

=

+

.

228

Chapter 8 Vectors and Motion

The second derivative of a vector
example 8
(cid:46) Two objects have positions as functions of time given by the
equations

and

r1 = 3t 2 ˆx + t ˆy

r2 = 3t 4 ˆx + t ˆy.

Find both objects’ accelerations using calculus. Could either an-
swer have been found without calculus?

(cid:46) Taking the ﬁrst derivative of each component, we ﬁnd

v1 = 6t ˆx + ˆy
v2 = 12t 3 ˆx + ˆy,

and taking the derivatives again gives acceleration,

a1 = 6ˆx
a2 = 36t 2 ˆx.

The ﬁrst object’s acceleration could have been found without cal-
culus, simply by comparing the x and y coordinates with the
constant-acceleration equation ∆x = vo∆t + 1
2 a∆t 2. The second
equation, however, isn’t just a second-order polynomial in t, so
the acceleration isn’t constant, and we really did need calculus to
ﬁnd the corresponding acceleration.

example 9
The integral of a vector
(cid:46) Starting from rest, a ﬂying saucer of mass m is observed to
vary its propulsion with mathematical precision according to the
equation

F = bt 42 ˆx + ct 137 ˆy.

(The aliens inform us that the numbers 42 and 137 have a special
religious signiﬁcance for them.) Find the saucer’s velocity as a
function of time.

(cid:46) From the given force, we can easily ﬁnd the acceleration

a =

=

F
m
b
m

t 42 ˆx +

c
m

t 137 ˆy.

The velocity vector v is the integral with respect to time of the
acceleration,

v =

=

(cid:90)

a dt
(cid:90) (cid:18) b
m

t 42 ˆx +

(cid:19)

dt,

t 137 ˆy

c
m

Section 8.4

(cid:82) Calculus with vectors

229

and integrating component by component gives

(cid:18)(cid:90) b
m

(cid:19)

ˆx +

t 42 dt

b
43m

t 43 ˆx +

c
138m

=

=

(cid:18)(cid:90) c
m

t 138 ˆy,

t 137 dt

(cid:19)

ˆy

where we have omitted the constants of integration, since the
saucer was starting from rest.

A ﬁre-extinguisher stunt on ice
example 10
(cid:46) Prof. Puerile smuggles a ﬁre extinguisher into a skating rink.
Climbing out onto the ice without any skates on, he sits down and
pushes off from the wall with his feet, acquiring an initial velocity
vo ˆy. At t = 0, he then discharges the ﬁre extinguisher at a 45-
degree angle so that it applies a force to him that is backward
and to the left, i.e., along the negative y axis and the positive x
axis. The ﬁre extinguisher’s force is strong at ﬁrst, but then dies
down according to the equation |F| = b − ct, where b and c are
constants. Find the professor’s velocity as a function of time.

(cid:46) Measured counterclockwise from the x axis, the angle of the
force vector becomes 315◦. Breaking the force down into x and
y components, we have

Fx = |F| cos 315◦
= (b − ct)
Fy = |F| sin 315◦
= (−b + ct).

In unit vector notation, this is

F = (b − ct)ˆx + (−b + ct)ˆy.

Newton’s second law gives

a = F/m

=

b − ct
√
2m

ˆx +

√

−b + ct
2m

ˆy.

To ﬁnd the velocity vector as a function of time, we need to inte-
grate the acceleration vector with respect to time,

v =

=

=

230

Chapter 8 Vectors and Motion

(cid:90)

a dt
(cid:90) (cid:18) b − ct
√
2m
(cid:90)
(cid:2)(b − ct) ˆx + (−b + ct) ˆy(cid:3) dt

−b + ct
2m

ˆx +

dt

√

√

(cid:19)

ˆy

1

2m

A vector function can be integrated component by component, so
this can be broken down into two integrals,

v =

√

(b − ct) dt +

(−b + ct) dt

(cid:90)

ˆx
2m
bt − 1
2 ct 2
√
2m

(cid:32)

=

(cid:90)

√

ˆy
2m
(cid:33)

+ constant #1

ˆx +

(cid:32)

2 ct 2

−bt + 1
√
2m

(cid:33)

+ constant #2

ˆy

Here the physical signiﬁcance of the two constants of integration
is that they give the initial velocity. Constant #1 is therefore zero,
and constant #2 must equal vo. The ﬁnal result is

(cid:32)

v =

bt − 1
2 ct 2
√
2m

(cid:33)

(cid:32)

ˆx +

2 ct 2

−bt + 1
√
2m

(cid:33)

+ vo

ˆy.

Section 8.4

(cid:82) Calculus with vectors

231

Summary

The velocity vector points in the direction of the object’s motion.
Relative motion can be described by vector addition of velocities.

The acceleration vector need not point in the same direction as
the object’s motion. We use the word “acceleration” to describe any
change in an object’s velocity vector, which can be either a change
in its magnitude or a change in its direction.

An important application of the vector addition of forces is the

use of Newton’s ﬁrst law to analyze mechanical systems.

232

Chapter 8 Vectors and Motion

Problems

A computerized answer check is available online.

Key√
(cid:82) A problem that requires calculus.
(cid:63) A diﬃcult problem.

Problem 1.

As shown in the diagram, a dinosaur fossil is slowly moving
1
down the slope of a glacier under the inﬂuence of wind, rain and
gravity. At the same time, the glacier is moving relative to the
continent underneath. The dashed lines represent the directions but
not the magnitudes of the velocities. Pick a scale, and use graphical
addition of vectors to ﬁnd the magnitude and the direction of the
fossil’s velocity relative to the continent. You will need a ruler and
protractor.

√

2
Is it possible for a helicopter to have an acceleration due east
and a velocity due west? If so, what would be going on? If not, why
not?

A bird is initially ﬂying horizontally east at 21.1 m/s, but one
3
second later it has changed direction so that it is ﬂying horizontally
and 7◦ north of east, at the same speed. What are the magnitude
and direction of its acceleration vector during that one second time
interval? (Assume its acceleration was roughly constant.)

√

Problem 4.

4
A person of mass M stands in the middle of a tightrope,
which is ﬁxed at the ends to two buildings separated by a horizontal
distance L. The rope sags in the middle, stretching and lengthening
the rope slightly.

Problems

233

(a) If the tightrope walker wants the rope to sag vertically by no
more than a height h, ﬁnd the minimum tension, T , that the rope
must be able to withstand without breaking, in terms of h, g, M ,
√
and L.
(b) Based on your equation, explain why it is not possible to get
h = 0, and give a physical interpretation.

5
Your hand presses a block of mass m against a wall with a
force FH acting at an angle θ, as shown in the ﬁgure. Find the
minimum and maximum possible values of |FH | that can keep the
block stationary, in terms of m, g, θ, and µs, the coeﬃcient of static
friction between the block and the wall. Check both your answers
in the case of θ = 90◦, and interpret the case where the maximum
(cid:63)
force is inﬁnite.

√

6
A skier of mass m is coasting down a slope inclined at an angle
θ compared to horizontal. Assume for simplicity that the treatment
of kinetic friction given in chapter 5 is appropriate here, although a
soft and wet surface actually behaves a little diﬀerently. The coeﬃ-
cient of kinetic friction acting between the skis and the snow is µk,
and in addition the skier experiences an air friction force of magni-
tude bv2, where b is a constant.
(a) Find the maximum speed that the skier will attain, in terms of
√
the variables m, g, θ, µk, and b.
(b) For angles below a certain minimum angle θmin, the equation
gives a result that is not mathematically meaningful. Find an equa-
tion for θmin, and give a physical explanation of what is happening
√
for θ < θmin.

7
A gun is aimed horizontally to the west. The gun is ﬁred, and
the bullet leaves the muzzle at t = 0. The bullet’s position vector
as a function of time is r = bˆx + ctˆy + dt2ˆz, where b, c, and d are
positive constants.
(a) What units would b, c, and d need to have for the equation to
make sense?
(b) Find the bullet’s velocity and acceleration as functions of time.
(c) Give physical interpretations of b, c, d, ˆx, ˆy, and ˆz.

(cid:82)

8
Annie Oakley, riding north on horseback at 30 mi/hr, shoots
her riﬂe, aiming horizontally and to the northeast. The muzzle speed
of the riﬂe is 140 mi/hr. When the bullet hits a defenseless fuzzy
animal, what is its speed of impact? Neglect air resistance, and
ignore the vertical motion of the bullet.

(cid:46) Solution, p. 554

9
A cargo plane has taken oﬀ from a tiny airstrip in the Andes,
and is climbing at constant speed, at an angle of θ = 17◦ with
respect to horizontal. Its engines supply a thrust of Fthrust = 200
kN, and the lift from its wings is Flift = 654 kN. Assume that air
resistance (drag) is negligible, so the only forces acting are thrust,
lift, and weight. What is its mass, in kg?

(cid:46) Solution, p. 554

Problem 5.

Problem 9.

Problem 10.

234

Chapter 8 Vectors and Motion

A wagon is being pulled at constant speed up a slope θ by a

10
rope that makes an angle φ with the vertical.
(a) Assuming negligible friction, show that the tension in the rope
is given by the equation

FT =

sin θ
sin(θ + φ)

FW ,

where FW is the weight force acting on the wagon.
(b) Interpret this equation in the special cases of φ = 0 and φ =
180◦ − θ.

(cid:46) Solution, p. 555

The angle of repose is the maximum slope on which an object
11
will not slide. On airless, geologically inert bodies like the moon or
an asteroid, the only thing that determines whether dust or rubble
will stay on a slope is whether the slope is less steep than the angle
of repose. (See ﬁgure n, p. 272.)
(a) Find an equation for the angle of repose, deciding for yourself
what are the relevant variables.
(b) On an asteroid, where g can be thousands of times lower than
on Earth, would rubble be able to lie at a steeper angle of repose?

(cid:46) Solution, p. 555

12
The ﬁgure shows an experiment in which a cart is released
from rest at A, and accelerates down the slope through a distance
x until it passes through a sensor’s light beam. The point of the
experiment is to determine the cart’s acceleration. At B, a card-
board vane mounted on the cart enters the light beam, blocking the
light beam, and starts an electronic timer running. At C, the vane
emerges from the beam, and the timer stops.
(a) Find the ﬁnal velocity of the cart in terms of the width w of
the vane and the time tb for which the sensor’s light beam was
√
blocked.
(b) Find the magnitude of the cart’s acceleration in terms of the
√
measurable quantities x, tb, and w.
(c) Analyze the forces in which the cart participates, using a table in
the format introduced in section 5.3. Assume friction is negligible.
(d) Find a theoretical value for the acceleration of the cart, which
could be compared with the experimentally observed value extracted
in part b. Express the theoretical value in terms of the angle θ of
the slope, and the strength g of the gravitational ﬁeld.

√

13
The ﬁgure shows a boy hanging in three positions: (1) with
his arms straight up, (2) with his arms at 45 degrees, and (3) with
his arms at 60 degrees with respect to the vertical. Compare the
tension in his arms in the three cases.

Problem 12.

Problem 13 (Millikan and Gale,
1920).

Problems

235

14
Driving down a hill inclined at an angle θ with respect to
horizontal, you slam on the brakes to keep from hitting a deer. Your
antilock brakes kick in, and you don’t skid.
(a) Analyze the forces. (Ignore rolling resistance and air friction.)
(b) Find the car’s maximum possible deceleration, a (expressed as
a positive number), in terms of g, θ, and the relevant coeﬃcient of
√
friction.
(c) Explain physically why the car’s mass has no eﬀect on your
answer.
(d) Discuss the mathematical behavior and physical interpretation
of your result for negative values of θ.
(e) Do the same for very large positive values of θ.

15
The ﬁgure shows the path followed by Hurricane Irene in
2005 as it moved north. The dots show the location of the center
of the storm at six-hour intervals, with lighter dots at the time
when the storm reached its greatest intensity. Find the time when
the storm’s center had a velocity vector to the northeast and an
acceleration vector to the southeast. Explain.

For safety, mountain climbers often wear a climbing harness
16
and tie in to other climbers on a rope team or to anchors such as
pitons or snow anchors. When using anchors, the climber usually
wants to tie in to more than one, both for extra strength and for
redundancy in case one fails. The ﬁgure shows such an arrangement,
with the climber hanging from a pair of anchors forming a symmetric
“Y” at an angle θ. The metal piece at the center is called a carabiner.
The usual advice is to make θ < 90◦; for large values of θ, the stress
placed on the anchors can be many times greater than the actual
load L, so that two anchors are actually less safe than one.
(a) Find the force S at each anchor in terms of L and θ.
(b) Verify that your answer makes sense in the case of θ = 0.
(c) Interpret your answer in the case of θ = 180◦.
(d) What is the smallest value of θ for which S equals or exceeds
L, so that for larger angles a failure of at least one anchor is more
√
likely than it would have been with a single anchor?

√

17
(a) The person with mass m hangs from the rope, hauling the
box of mass M up a slope inclined at an angle θ. There is friction
between the box and the slope, described by the usual coeﬃcients
of friction. The pulley, however, is frictionless. Find the magnitude
√
of the box’s acceleration.
(b) Show that the units of your answer make sense.
(c) Check the physical behavior of your answer in the special cases
of M = 0 and θ = −90◦.

Problem 15.

Problem 16.

Problem 17.

236

Chapter 8 Vectors and Motion

18
Complete example 6 on p. 226 by expressing the remaining
nine x and y components of the forces in terms of the ﬁve magnitudes
and the small, positive angle θ ≈ 9◦ by which the crack overhangs.

√

Problem 16 discussed a possible correct way of setting up
19
a redundant anchor for mountaineering. The ﬁgure for this prob-
lem shows an incorrect way of doing it, by arranging the rope in
a triangle (which we’ll take to be isoceles). One of the bad things
about the triangular arrangement is that it requires more force from
the anchors, making them more likely to fail. (a) Using the same
√
notation as in problem 16, ﬁnd S in terms of L and θ.
(b) Verify that your answer makes sense in the case of θ = 0, and
compare with the correct setup.

20
A telephone wire of mass m is strung between two poles,
making an angle θ with the horizontal at each end. (a) Find the
√
tension at the center.
(b) Which is greater, the tension at the center or at the ends?

The ﬁgure shows an arcade game called skee ball that is
21
similar to bowling. The player rolls the ball down a horizontal alley.
The ball then rides up a curved lip and is launched at an initial
speed u, at an angle α above horizontal. Suppose we want the ball
to go into a hole that is at horizontal distance (cid:96) and height h, as
shown in the ﬁgure.
(a) Find the initial speed u that is required, in terms of the other
√
variables and g.
(b) Check that your answer to part a has units that make sense.
(c) Check that your answer to part a depends on g in a way that
makes sense. This means that you should ﬁrst determine on physical
grounds whether increasing g should increase u, or decrease it. Then
see whether your answer to part a has this mathematical behavior.
(d) Do the same for the dependence on h.
(e) Interpret your equation in the case where α = 90◦.
(f) Interpret your equation in the case where tan α = h/(cid:96).
(g) Find u numerically if h = 70 cm, (cid:96) = 60 cm, and α = 65◦.

√

Problem 19.

Problem 20.

Problem 21.

Problems

237

Problem 23.

Problem 24.

22
A plane ﬂies toward a city directly north and a distance D
away. The wind speed is u, and the plane’s speed with respect to
the wind is v.
(a) If the wind is blowing from the west (towards the east), what
√
direction should the plane head (what angle west of north)?
(b) How long does it take the plane to get to the city?
(c) Check that your answer to part b has units that make sense.
(d) Comment on the behavior of your answer in the case where
u = v.

[problem by B. Shotwell]

√

23
A force F is applied to a box of mass M at an angle θ below
the horizontal (see ﬁgure). The coeﬃcient of static friction between
the box and the ﬂoor is µs, and the coeﬃcient of kinetic friction
between the two surfaces is µk.
(a) What is the magnitude of the normal force on the box from the
√
ﬂoor?
(b) What is the minimum value of F to get the box to start moving
√
from rest?
(c) What is the value of F so that the box will move with constant
√
velocity (assuming it is already moving)?
(d) If θ is greater than some critical angle θcrit, it is impossible to
have the scenario described in part c. What is θcrit?

√

[problem by B. Shotwell]

(a) A mass M is at rest on a ﬁxed, frictionless ramp inclined
24
at angle θ with respect to the horizontal. The mass is connected
to the force probe, as shown. What is the reading on the force
√
probe?
(b) Check that your answer to part a makes sense in the special
cases θ = 0 and θ = 90◦.

[problem by B. Shotwell]

25
The ﬁgure shows a rock climber wedged into a dihedral or
“open book” consisting of two vertical walls of rock at an angle θ rel-
ative to one another. This position can be maintained without any
ledges or holds, simply by pressing the feet against the walls. The
left hand is being used just for a little bit of balance. (a) Find the
minimum coeﬃcient of friction between the rubber climbing shoes
and the rock. (b) Interpret the behavior of your expression at ex-
treme values of θ. (c) Steven Won has done tabletop experiments
using climbing shoes on the rough back side of a granite slab from
a kitchen countertop, and has estimated µs = 1.17. Find the corre-
sponding maximum value of θ.

(cid:46) Solution, p. 556

You throw a rock horizontally from the edge of the roof of
26
a building of height h with speed v0. What is the (positive) angle
between the ﬁnal velocity vector and the horizontal when the rock
hits the ground?

[problem by B. Shotwell]

√

Problem 25.

238

Chapter 8 Vectors and Motion

27
The ﬁgure shows a block acted on by two external forces,
each of magnitude F . One of the forces is horizontal, but the other
is applied at a downward angle θ. Gravity is negligible compared to
these forces. The block rests on a surface with friction described by
a coeﬃcient of friction µs. (a) Find the minimum value of µs that
√
is required if the block is to remain at rest.
(b) Show that this expression has the correct limit as θ approaches
zero.

(cid:82)

Problem 27.

Problems

239

Exercise 8: Vectors and motion

Each diagram on page ?? shows the motion of an object in an x − y plane. Each dot is one
location of the object at one moment in time. The time interval from one dot to the next is
always the same, so you can think of the vector that connects one dot to the next as a v vector,
and subtract to ﬁnd ∆v vectors.

1. Suppose the object in diagram 1 is moving from the top left to the bottom right. Deduce
whatever you can about the force acting on it. Does the force always have the same magnitude?
The same direction?

Invent a physical situation that this diagram could represent.

What if you reinterpret the diagram by reversing the object’s direction of motion? Redo the
construction of one of the ∆v vectors and see what happens.

2. What can you deduce about the force that is acting in diagram 2?

Invent a physical situation that diagram 2 could represent.

3. What can you deduce about the force that is acting in diagram 3?

Invent a physical situation.

240

Chapter 8 Vectors and Motion

Exercise 8: Vectors and motion

241

242

Chapter 8 Vectors and Motion

Chapter 9
Circular Motion

9.1 Conceptual framework

I now live ﬁfteen minutes from Disneyland, so my friends and family
in my native Northern California think it’s a little strange that I’ve
never visited the Magic Kingdom again since a childhood trip to the
south. The truth is that for me as a preschooler, Disneyland was
not the Happiest Place on Earth. My mother took me on a ride in
which little cars shaped like rocket ships circled rapidly around a
central pillar. I knew I was going to die. There was a force trying to
throw me outward, and the safety features of the ride would surely
have been inadequate if I hadn’t screamed the whole time to make
sure Mom would hold on to me. Afterward, she seemed surprisingly
indiﬀerent to the extreme danger we had experienced.

Circular motion does not produce an outward force

My younger self’s understanding of circular motion was partly
right and partly wrong. I was wrong in believing that there was a
force pulling me outward, away from the center of the circle. The
easiest way to understand this is to bring back the parable of the
bowling ball in the pickup truck from chapter 4. As the truck makes
a left turn, the driver looks in the rearview mirror and thinks that
some mysterious force is pulling the ball outward, but the truck
is accelerating, so the driver’s frame of reference is not an inertial
frame. Newton’s laws are violated in a noninertial frame, so the ball
appears to accelerate without any actual force acting on it. Because
we are used to inertial frames, in which accelerations are caused by

243

reference,

a / 1. In the turning truck’s frame
the ball appears
of
to violate Newton’s laws, dis-
playing a sideways acceleration
that is not the result of a force-
interaction with any other object.
2.
In an inertial frame of refer-
ence, such as the frame ﬁxed to
the earth’s surface, the ball obeys
Newton’s ﬁrst law. No forces are
acting on it, and it continues mov-
ing in a straight line. It is the truck
that is participating in an interac-
tion with the asphalt, the truck that
accelerates as it should according
to Newton’s second law.

crane

b / This
halteres
help it to maintain its orientation
in ﬂight.

ﬂy’s

forces, the ball’s acceleration creates a vivid illusion that there must
be an outward force.

In an inertial frame everything makes more sense. The ball has
no force on it, and goes straight as required by Newton’s ﬁrst law.
The truck has a force on it from the asphalt, and responds to it
by accelerating (changing the direction of its velocity vector) as
Newton’s second law says it should.

example 1
The halteres
Another interesting example is an insect organ called the hal-
teres, a pair of small knobbed limbs behind the wings, which vi-
brate up and down and help the insect to maintain its orientation
in ﬂight. The halteres evolved from a second pair of wings pos-
sessed by earlier insects. Suppose, for example, that the halteres
are on their upward stroke, and at that moment an air current
causes the ﬂy to pitch its nose down. The halteres follow New-
ton’s ﬁrst law, continuing to rise vertically, but in the ﬂy’s rotating
frame of reference, it seems as though they have been subjected
to a backward force. The ﬂy has special sensory organs that per-
ceive this twist, and help it to correct itself by raising its nose.

Circular motion does not persist without a force

I was correct, however, on a diﬀerent point about the Disneyland
ride. To make me curve around with the car, I really did need some
force such as a force from my mother, friction from the seat, or a
normal force from the side of the car. (In fact, all three forces were
probably adding together.) One of the reasons why Galileo failed to

244

Chapter 9 Circular Motion

c / 1. An overhead view of a per-
son swinging a rock on a rope. A
force from the string is required
to make the rock’s velocity vector
keep changing direction. 2. If the
string breaks, the rock will follow
Newton’s ﬁrst law and go straight
instead of continuing around the
circle.

d / Sparks
ﬂy
tangents to a grinding wheel.

away

along

reﬁne the principle of inertia into a quantitative statement like New-
ton’s ﬁrst law is that he was not sure whether motion without a force
would naturally be circular or linear. In fact, the most impressive
examples he knew of the persistence of motion were mostly circular:
the spinning of a top or the rotation of the earth, for example. New-
ton realized that in examples such as these, there really were forces
at work. Atoms on the surface of the top are prevented from ﬂying
oﬀ straight by the ordinary force that keeps atoms stuck together in
solid matter. The earth is nearly all liquid, but gravitational forces
pull all its parts inward.

Uniform and nonuniform circular motion

Circular motion always involves a change in the direction of the
velocity vector, but it is also possible for the magnitude of the ve-
locity to change at the same time. Circular motion is referred to as
uniform if |v| is constant, and nonuniform if it is changing.

Your speedometer tells you the magnitude of your car’s velocity
vector, so when you go around a curve while keeping your speedome-
ter needle steady, you are executing uniform circular motion. If your
speedometer reading is changing as you turn, your circular motion
is nonuniform. Uniform circular motion is simpler to analyze math-
ematically, so we will attack it ﬁrst and then pass to the nonuniform
case.

self-check A
Which of these are examples of uniform circular motion and which are
nonuniform?

(1) the clothes in a clothes dryer (assuming they remain against the
inside of the drum, even at the top)

(2) a rock on the end of a string being whirled in a vertical circle
Answer, p. 566

(cid:46)

Section 9.1 Conceptual framework

245

e / To make the brick go in a
circle,
I had to exert an inward
force on the rope.

Only an inward force is required for uniform circular motion.

Figure c showed the string pulling in straight along a radius of
the circle, but many people believe that when they are doing this
they must be “leading” the rock a little to keep it moving along.
That is, they believe that the force required to produce uniform
circular motion is not directly inward but at a slight angle to the
radius of the circle. This intuition is incorrect, which you can easily
verify for yourself now if you have some string handy.
It is only
while you are getting the object going that your force needs to be at
an angle to the radius. During this initial period of speeding up, the
motion is not uniform. Once you settle down into uniform circular
motion, you only apply an inward force.

If you have not done the experiment for yourself, here is a theo-
retical argument to convince you of this fact. We have discussed in
chapter 6 the principle that forces have no perpendicular eﬀects. To
keep the rock from speeding up or slowing down, we only need to
make sure that our force is perpendicular to its direction of motion.
We are then guaranteed that its forward motion will remain unaf-
fected: our force can have no perpendicular eﬀect, and there is no
other force acting on the rock which could slow it down. The rock
requires no forward force to maintain its forward motion, any more
than a projectile needs a horizontal force to “help it over the top”
of its arc.

g / When a car is going straight
at constant speed,
the forward
and backward forces on it are
canceling out, producing a total
force of zero. When it moves
in a circle at constant speed,
there are three forces on it, but
the forward and backward forces
cancel out, so the vector sum is
an inward force.

f / A series of three hammer taps makes the rolling ball trace a tri-
angle, seven hammers a heptagon. If the number of hammers was large
enough, the ball would essentially be experiencing a steady inward force,
and it would go in a circle. In no case is any forward force necessary.

246

Chapter 9 Circular Motion

Why, then, does a car driving in circles in a parking lot stop
executing uniform circular motion if you take your foot oﬀ the gas?
The source of confusion here is that Newton’s laws predict an ob-
ject’s motion based on the total force acting on it. A car driving in
circles has three forces on it

(1) an inward force from the asphalt, controlled with the steering

wheel;

(2) a forward force from the asphalt, controlled with the gas

pedal; and

(3) backward forces from air resistance and rolling resistance.

You need to make sure there is a forward force on the car so that
the backward forces will be exactly canceled out, creating a vector
sum that points directly inward.

h / Example 2.

example 2
A motorcycle making a turn
The motorcyclist in ﬁgure h is moving along an arc of a circle. It
looks like he’s chosen to ride the slanted surface of the dirt at a
place where it makes just the angle he wants, allowing him to get
the force he needs on the tires as a normal force, without needing
any frictional force. The dirt’s normal force on the tires points up
and to our left. The vertical component of that force is canceled
by gravity, while its horizontal component causes him to curve.

In uniform circular motion, the acceleration vector is inward.

Since experiments show that the force vector points directly
inward, Newton’s second law implies that the acceleration vector
points inward as well. This fact can also be proven on purely kine-
matical grounds, and we will do so in the next section.

Clock-comparison tests of Newton’s ﬁrst law
example 3
Immediately after his original statement of the ﬁrst law in the Prin-
cipia Mathematica, Newton offers the supporting example of a
spinning top, which only slows down because of friction. He de-
scribes the different parts of the top as being held together by
“cohesion,” i.e., internal forces. Because these forces act toward
the center, they don’t speed up or slow down the motion. The ap-
plicability of the ﬁrst law, which only describes linear motion, may
be more clear if we simply take ﬁgure f as a model of rotation.
Between hammer taps, the ball experiences no force, so by the
ﬁrst law it doesn’t speed up or slow down.

Suppose that we want to subject the ﬁrst law to a stringent exper-
imental test.1 The law predicts that if we use a clock to measure
the rate of rotation of an object spinning frictionlessly, it won’t “nat-
urally” slow down as Aristotle would have expected. But what is
a clock but something with hands that rotate at a ﬁxed rate? In

1Page 81 lists places in this book where we describe experimental tests of

Newton’s ﬁrst law.

Section 9.1 Conceptual framework

247

other words, we are comparing one clock with another. This is
called a clock-comparison experiment. Suppose that the laws of
physics weren’t purely Newtonian, and there really was a very
slight Aristotelian tendency for motion to slow down in the ab-
sence of friction. If we compare two clocks, they should both slow
down, but if they aren’t the same type of clock, then it seems un-
likely that they would slow down at exactly the same rate, and
over time they should drift further and further apart.

High-precision clock-comparison experiments have been done
using a variety of clocks.
In atomic clocks, the thing spinning
is an atom. Astronomers can observe the rotation of collapsed
stars called pulars, which, unlike the earth, can rotate with almost
no disturbance due to geological activity or friction induced by the
tides.
In these experiments, the pulsars are observed to match
the rates of the atomic clocks with a drift of less than about 10−6
seconds over a period of 10 years.2 Atomic clocks using atoms
of different elements drift relative to one another by no more than
about 10−16 per year.3

It is not presently possible to do experiments with a similar level of
precision using human-scale rotating objects. However, a set of
gyroscopes aboard the Gravity Probe B satellite were allowed to
spin weightlessly in a vacuum, without any physical contact that
would have caused kinetic friction. Their rotation was extremely
accurately monitored for the purposes of another experiment (a
test of Einstein’s theory of general relativity, which was the pur-
pose of the mission), and they were found to be spinning down so
gradually that they would have taken about 10,000 years to slow
down by a factor of two. This rate was consistent with estimates
of the amount of friction to be expected from the small amount of
residual gas present in the vacuum chambers.

A subtle point in the interpretation of these experiments is that if
there was a slight tendency for motion to slow down, we would
have to decide what it was supposed to slow down relative to.
A straight-line motion that is slowing down in some frame of ref-
erence can always be described as speeding up in some other
appropriately chosen frame (problem 12, p. 90).
If the laws of
physics did have this slight Aristotelianism mixed in, we could wait
for the anomalous acceleration or deceleration to stop. The ob-
ject we were observing would then deﬁne a special or “preferred”
frame of reference. Standard theories of physics do not have
such a preferred frame, and clock-comparison experiments can
be viewed as tests of the existence of such a frame. Another test
for the existence of a preferred frame is described on p. 277.

2Matsakis et al., Astronomy and Astrophysics 326 (1997) 924. Freely avail-

able online at adsabs.harvard.edu.

3Gu´ena et al., arxiv.org/abs/1205.4235

248

Chapter 9 Circular Motion

Discussion

questions

A-D

Discussion question E.

Discussion questions

In the game of crack the whip, a line of people stand holding hands,
A
and then they start sweeping out a circle. One person is at the center,
and rotates without changing location. At the opposite end is the person
who is running the fastest, in a wide circle. In this game, someone always
ends up losing their grip and ﬂying off. Suppose the person on the end
loses her grip. What path does she follow as she goes ﬂying off? Draw an
overhead view. (Assume she is going so fast that she is really just trying
to put one foot in front of the other fast enough to keep from falling; she
is not able to get any signiﬁcant horizontal force between her feet and the
ground.)

Suppose the person on the outside is still holding on, but feels that
B
she may loose her grip at any moment. What force or forces are acting
on her, and in what directions are they? (We are not interested in the
vertical forces, which are the earth’s gravitational force pulling down, and
the ground’s normal force pushing up.) Make a table in the format shown
in section 5.3.

Suppose the person on the outside is still holding on, but feels that
C
she may loose her grip at any moment. What is wrong with the following
analysis of the situation? “The person whose hand she’s holding exerts
an inward force on her, and because of Newton’s third law, there’s an
equal and opposite force acting outward. That outward force is the one
she feels throwing her outward, and the outward force is what might make
her go ﬂying off, if it’s strong enough.”

If the only force felt by the person on the outside is an inward force,

D
why doesn’t she go straight in?

In the amusement park ride shown in the ﬁgure, the cylinder spins
E
faster and faster until the customer can pick her feet up off the ﬂoor with-
out falling. In the old Coney Island version of the ride, the ﬂoor actually
dropped out like a trap door, showing the ocean below. (There is also a
version in which the whole thing tilts up diagonally, but we’re discussing
the version that stays ﬂat.) If there is no outward force acting on her, why
does she stick to the wall? Analyze all the forces on her.

What is an example of circular motion where the inward force is a
F
normal force? What is an example of circular motion where the inward
force is friction? What is an example of circular motion where the inward
force is the sum of more than one force?

Does the acceleration vector always change continuously in circular

G
motion? The velocity vector?

Section 9.1 Conceptual framework

249

i / The law of sines.

j / Deriving |a|
uniform circular motion.

=

|v|2/r

9.2 Uniform circular motion

In this section I derive a simple and very useful equation for
the magnitude of the acceleration of an object undergoing constant
acceleration. The law of sines is involved, so I’ve recapped it in
ﬁgure i.

The derivation is brief, but the method requires some explana-
tion and justiﬁcation. The idea is to calculate a ∆v vector describing
the change in the velocity vector as the object passes through an
angle θ. We then calculate the acceleration, a = ∆v/∆t. The as-
tute reader will recall, however, that this equation is only valid for
motion with constant acceleration. Although the magnitude of the
acceleration is constant for uniform circular motion, the acceleration
vector changes its direction, so it is not a constant vector, and the
equation a = ∆v/∆t does not apply. The justiﬁcation for using it
is that we will then examine its behavior when we make the time
interval very short, which means making the angle θ very small. For
smaller and smaller time intervals, the ∆v/∆t expression becomes
a better and better approximation, so that the ﬁnal result of the
derivation is exact.

In ﬁgure j/1, the object sweeps out an angle θ. Its direction of
motion also twists around by an angle θ, from the vertical dashed
line to the tilted one. Figure j/2 shows the initial and ﬁnal velocity
vectors, which have equal magnitude, but directions diﬀering by θ.
In j/3, I’ve reassembled the vectors in the proper positions for vector
subtraction. They form an isosceles triangle with interior angles θ,
η, and η. (Eta, η, is my favorite Greek letter.) The law of sines
gives

|∆v|
sin θ

=

|v|
sin η

.

This tells us the magnitude of ∆v, which is one of the two ingredients
we need for calculating the magnitude of a = ∆v/∆t. The other
ingredient is ∆t. The time required for the object to move through
the angle θ is

∆t =

length of arc
|v|

.

Now if we measure our angles in radians we can use the deﬁnition of
radian measure, which is (angle) = (length of arc)/(radius), giving
∆t = θr/|v|. Combining this with the ﬁrst expression involving
|∆v| gives

for

|a| = |∆v|/∆t
|v|2
r

=

·

sin θ
θ

·

1
sin η

.

When θ becomes very small, the small-angle approximation sin θ ≈ θ
applies, and also η becomes close to 90◦, so sin η ≈ 1, and we have

250

Chapter 9 Circular Motion

an equation for |a|:

|a| =

|v|2
r

.

[uniform circular motion]

Force required to turn on a bike
example 4
(cid:46) A bicyclist is making a turn along an arc of a circle with radius
20 m, at a speed of 5 m/s.
If the combined mass of the cyclist
plus the bike is 60 kg, how great a static friction force must the
road be able to exert on the tires?

(cid:46) Taking the magnitudes of both sides of Newton’s second law
gives

|F| = |ma|
= m|a|.

Substituting |a| = |v|2/r gives

|F| = m|v|2/r
≈ 80 N

(rounded off to one sig ﬁg).

example 5
Don’t hug the center line on a curve!
(cid:46) You’re driving on a mountain road with a steep drop on your
right. When making a left turn, is it safer to hug the center line or
to stay closer to the outside of the road?

(cid:46) You want whichever choice involves the least acceleration, be-
cause that will require the least force and entail the least risk of
exceeding the maximum force of static friction. Assuming the
curve is an arc of a circle and your speed is constant, your car
is performing uniform circular motion, with |a| = |v|2/r . The de-
pendence on the square of the speed shows that driving slowly
is the main safety measure you can take, but for any given speed
you also want to have the largest possible value of r . Even though
your instinct is to keep away from that scary precipice, you are ac-
tually less likely to skid if you keep toward the outside, because
then you are describing a larger circle.

Acceleration related to radius and period of rotation
example 6
(cid:46) How can the equation for the acceleration in uniform circular
motion be rewritten in terms of the radius of the circle and the
period, T , of the motion, i.e., the time required to go around once?

(cid:46) The period can be related to the speed as follows:

|v| =

circumference
T
= 2πr /T .

Substituting into the equation |a| = |v|2/r gives

|a| =

4π2r
T 2 .

k / Example 7.

Section 9.2 Uniform circular motion

251

A clothes dryer
example 7
(cid:46) My clothes dryer has a drum with an inside radius of 35 cm, and
it spins at 48 revolutions per minute. What is the acceleration of
the clothes inside?

(cid:46) We can solve this by ﬁnding the period and plugging in to the
result of the previous example. If it makes 48 revolutions in one
minute, then the period is 1/48 of a minute, or 1.25 s. To get an
acceleration in mks units, we must convert the radius to 0.35 m.
Plugging in, the result is 8.8 m/s2.

More about clothes dryers!
example 8
(cid:46) In a discussion question in the previous section, we made the
assumption that the clothes remain against the inside of the drum
as they go over the top. In light of the previous example, is this a
correct assumption?

(cid:46) No. We know that there must be some minimum speed at which
the motor can run that will result in the clothes just barely stay-
ing against the inside of the drum as they go over the top. If the
clothes dryer ran at just this minimum speed, then there would be
no normal force on the clothes at the top: they would be on the
verge of losing contact. The only force acting on them at the top
would be the force of gravity, which would give them an acceler-
ation of g = 9.8 m/s2. The actual dryer must be running slower
than this minimum speed, because it produces an acceleration of
only 8.8 m/s2. My theory is that this is done intentionally, to make
the clothes mix and tumble.

16 For safety, mountain climbers often wear a climbing harness 
and tie in to other climbers on a rope team or to anchors such as 
pitons or snow anchors. When using anchors, the climber usually 
wants to tie in to more than one, both for extra strength and for 
redundancy in case one fails. The figure shows such an arrangement, 
with the climber hanging from a pair of anchors forming a symmetric 
“Y” at an angle 0. The metal piece at the center is called a carabiner. 
The usual advice is to make @ < 90°; for large values of 0, the stress 
placed on the anchors can be many times greater than the actual 
load L, so that two anchors are actually less safe than one. 

(a) Find the force S at each anchor in terms of L and 0. v 
(b) Verify that your answer makes sense in the case of 0 = 0. 

(c) Interpret your answer in the case of 6 = 180°. 

(d) What is the smallest value of 0 for which S equals or exceeds 
L, so that for larger angles a failure of at least one anchor is more 
likely than it would have been with a single anchor? v 


17 (a) The person with mass m hangs from the rope, hauling the 
box of mass M up a slope inclined at an angle 6. There is friction 
between the box and the slope, described by the usual coefficients 
of friction. The pulley, however, is frictionless. Find the magnitude 
of the box’s acceleration. Vv 
(b) Show that the units of your answer make sense. 

(c) Check the physical behavior of your answer in the special cases 
of M = 0 and 6 = —90°. 


236 Chapter 8 Vectors and Motion 


18 Complete example 6 on p. 226 by expressing the remaining 
nine x and y components of the forces in terms of the five magnitudes 
and the small, positive angle 0 = 9° by which the crack overhangs. 


v 


19 Problem 16 discussed a possible correct way of setting up 
a redundant anchor for mountaineering. The figure for this prob- 
lem shows an incorrect way of doing it, by arranging the rope in 
a triangle (which we'll take to be isoceles). One of the bad things 
about the triangular arrangement is that it requires more force from 
the anchors, making them more likely to fail. (a) Using the same 
notation as in problem 16, find S in terms of L and 6. Vv to climber 
(b) Verify that your answer makes sense in the case of 6 = 0, and 





compare with the correct setup. Problem 19. 
20 A telephone wire of mass m is strung between two poles, 
making an angle @ with the horizontal at each end. (a) Find the 
tension at the center. Vv 


(b) Which is greater, the tension at the center or at the ends? 


21 The figure shows an arcade game called skee ball that is 
similar to bowling. The player rolls the ball down a horizontal alley. 
The ball then rides up a curved lip and is launched at an initial 
speed u, at an angle a@ above horizontal. Suppose we want the ball Problem 20. 
to go into a hole that is at horizontal distance @ and height h, as 

shown in the figure. 











(a) Find the initial speed u that is required, in terms of the other 
variables and g. Vv 
(b) Check that your answer to part a has units that make sense. 
(c) Check that your answer to part a depends on g in a way that 
makes sense. This means that you should first determine on physical 
grounds whether increasing g should increase u, or decrease it. Then 
see whether your answer to part a has this mathematical behavior. 
(d) Do the same for the dependence on h. 

(e) Interpret your equation in the case where a = 90°. 

(f) Interpret your equation in the case where tana = h/é. 

(g) Find wu numerically if h = 70 cm, ¢ = 60 cm, and a= 65°. V 


Problem 21. 





Problems 237 





Problem 23. 





Problem 24. 


Problem 25. 


22 A plane flies toward a city directly north and a distance D 
away. The wind speed is u, and the plane’s speed with respect to 
the wind is v. 

(a) If the wind is blowing from the west (towards the east), what 
direction should the plane head (what angle west of north)? v 
(b) How long does it take the plane to get to the city? Vv 
(c) Check that your answer to part b has units that make sense. 
(d) Comment on the behavior of your answer in the case where 
U= Uv. [problem by B. Shotwell] 


23 A force F is applied to a box of mass M at an angle 0 below 
the horizontal (see figure). The coefficient of static friction between 
the box and the floor is ys, and the coefficient of kinetic friction 
between the two surfaces is Ux. 

(a) What is the magnitude of the normal force on the box from the 
floor? Vv 
(b) What is the minimum value of F' to get the box to start moving 
from rest? v 
(c) What is the value of F' so that the box will move with constant 
velocity (assuming it is already moving)? Vv 
(d) If 6 is greater than some critical angle 0.it, it is impossible to 
have the scenario described in part c. What is Ocrit? 

V [problem by B. Shotwell] 


24 (a) A mass M is at rest on a fixed, frictionless ramp inclined 
at angle 6 with respect to the horizontal. The mass is connected 
to the force probe, as shown. What is the reading on the force 


probe? Vv 
(b) Check that your answer to part a makes sense in the special 
cases 6 = 0 and 6 = 90°. [problem by B. Shotwell] 


25 The figure shows a rock climber wedged into a dihedral or 
“open book” consisting of two vertical walls of rock at an angle 6 rel- 
ative to one another. This position can be maintained without any 
ledges or holds, simply by pressing the feet against the walls. The 
left hand is being used just for a little bit of balance. (a) Find the 
minimum coefficient of friction between the rubber climbing shoes 
and the rock. (b) Interpret the behavior of your expression at ex- 
treme values of 6. (c) Steven Won has done tabletop experiments 
using climbing shoes on the rough back side of a granite slab from 
a kitchen countertop, and has estimated yw, = 1.17. Find the corre- 
sponding maximum value of 0. > Solution, p. 556 


26 You throw a rock horizontally from the edge of the roof of 
a building of height h with speed vg. What is the (positive) angle 
between the final velocity vector and the horizontal when the rock 
hits the ground? V [problem by B. Shotwell] 


238 Chapter 8 Vectors and Motion 


27 The figure shows a block acted on by two external forces, c pal tS) 
each of magnitude F’. One of the forces is horizontal, but the other —_> 

is applied at a downward angle @. Gravity is negligible compared to 

these forces. The block rests on a surface with friction described by 

a coefficient of friction ws. (a) Find the minimum value of js that Problem 27. 

is required if the block is to remain at rest. v 

(b) Show that this expression has the correct limit as 0 approaches 

Zero. 


J 


Problems 239 


Exercise 8: Vectors and motion 


Each diagram on page ?? shows the motion of an object in an x — y plane. Each dot is one 
location of the object at one moment in time. The time interval from one dot to the next is 
always the same, so you can think of the vector that connects one dot to the next as a v vector, 
and subtract to find Av vectors. 


1. Suppose the object in diagram 1 is moving from the top left to the bottom right. Deduce 
whatever you can about the force acting on it. Does the force always have the same magnitude? 
The same direction? 


Invent a physical situation that this diagram could represent. 


What if you reinterpret the diagram by reversing the object’s direction of motion? Redo the 
construction of one of the Av vectors and see what happens. 


2. What can you deduce about the force that is acting in diagram 2? 
Invent a physical situation that diagram 2 could represent. 
3. What can you deduce about the force that is acting in diagram 3? 


Invent a physical situation. 
