A low-tech way to build a partial Dyson sphere is to place a ring of habitats in circular orbit around the Sun. 
To completely surround the Sun, you could add rings orbiting it around different axes at slightly different distances, to avoid collisions. 
To avoid the nuisance that these fast-moving rings couldn’t be connected to one another, complicating transportation and communication, one could instead build a monolithic stationary Dyson sphere where the Sun’s inward gravitational pull is balanced by the outward pressure from the Sun’s radiation— an idea pioneered by Robert L. Forward and by Colin McInnes. 
The sphere can be built by gradually adding more “statites”: stationary satellites that counteract the Sun’s gravity with radiation pressure rather than centrifugal forces. 
Both of these forces drop off with the square of the distance to the Sun, which means that if they can be balanced at one distance from the Sun, they’ll conveniently be balanced at any other distance as well, allowing freedom to park anywhere in our Solar System. 
Statites need to be extremely lightweight sheets, weighing only 0.77 grams per square meter, which is about 100 times less than paper, but this is unlikely to be a showstopper
If the Dyson sphere is built to reflect rather than absorb most of the sunlight, then the total intensity of light bouncing around within it will be dramatically increased, further boosting the radiation pressure and the amount of mass that can be supported in the sphere. 
Many other stars have a thousandfold and even a millionfold greater luminosity than our Sun, and are therefore able to support correspondingly heavier stationary Dyson spheres. 
For today’s humans, life on or in a Dyson sphere would at best be disorienting and at worst impossible, but that need not stop future biological or nonbiological life forms from thriving there. 
A problem with using black hole evaporation as a power source is that, unless the black hole is much smaller than an atom in size, it’s an excruciatingly slow process that takes longer than the present age of our Universe and radiates less energy than a candle. 
The power produced decreases with the square of the size of the hole, and the physicists Louis Crane and Shawn Westmoreland have therefore proposed using a black hole about a thousand times smaller than a proton, weighing about as much as the largest-ever seagoing ship. 
Their main motivation was to use the black hole engine to power a starship (a topic to which we return below), so they were more concerned with portability than efficiency and proposed feeding the black hole with laser light, causing no energy-to-matter conversion at all. 
Even if you could feed it with matter instead of radiation, guaranteeing high efficiency appears difficult: to make protons enter such a black hole a thousandth their size, they would have to be fired at the hole with a machine as powerful as the Large Hadron Collider, augmenting their energy mc2 with at least a thousand times more kinetic (motion) energy. 
Since at least 10% of that kinetic energy would be lost to gravitons when the black hole evaporates, we’d therefore be putting more energy into the black hole than we’d be able to extract and put to work, ending up with negative efficiency. 
Fortunately, there are other ways of using black holes as power plants that don’t involve quantum gravity or other poorly understood physics. For example, many existing black holes spin very fast, with their event horizons whirling around near the speed of light, and this rotation energy can be extracted. 
The event horizon of a black hole is the region from which not even light can escape, because the gravitational pull is too powerful. 
If you toss an object into the ergosphere, it will therefore pick up speed rotating around the hole. Unfortunately, it will soon get eaten up by the black hole, forever disappearing through the event horizon, so this does you no good if you’re trying to extract energy. However, Roger Penrose discovered that if you launch the object at a clever angle and make it split into two pieces as figure 6.4 illustrates, then you can arrange for only one piece to get eaten while the other escapes the black hole with more energy than you started with. In other words, you’ve successfully converted some of the rotational energy of the black hole into useful energy that you can put to work. By repeating this process many times, you can milk the black hole of all its rotational energy so that it stops spinning and its ergosphere disappears. 
Another interesting strategy is to extract energy not from the black hole itself, but from matter falling into it. Nature has already found a way of doing this all on its own: the quasar. As gas swirls even closer to a black hole, forming a pizza-shaped disk whose innermost parts gradually get gobbled up, it gets extremely hot and gives off copious amounts of radiation. As gas falls downward toward the hole, it speeds up, converting its gravitational potential energy into motion energy, just as a skydiver does. The motion gets progressively messier as complicated turbulence converts the coordinated motion of the gas blob into random motion on ever-smaller scales, until individual atoms begin colliding with each other at high speeds—having such random motion is precisely what it means to be hot, and these violent collisions convert motion energy into radiation. By building a Dyson sphere around the entire black hole, at a safe distance, this radiation energy can be captured and put to use. The faster the black hole spins, the more efficient this process gets, with a maximally spinning black hole delivering energy at a whopping 42% efficiency. *4 For black holes weighing about as much as a star, most of the energy comes out as X-rays, whereas for the supermassive kind found in the centers of galaxies, much of it emerges somewhere in the range of infrared, visible and ultraviolet light. 
There is another known way to convert matter into energy that doesn’t involve black holes at all: the sphaleron process. It can destroy quarks and turn them into leptons: electrons, their heavier cousins the muon and tau particles, neutrinos or their antiparticles. 4 As illustrated in figure 6.5, the standard model of particle physics predicts that nine quarks with appropriate flavor and spin can come together and transform into three leptons through an intermediate state called a sphaleron. Because the input weighs more than the output, the mass difference gets converted into energy according to Einstein’s E = mc2 formula. 
Future intelligent life might therefore be able to build what I’ll call a sphalerizer: an energy generator acting like a diesel engine on steroids. A traditional diesel engine compresses a mixture of air and diesel oil until the temperature gets high enough for it to spontaneously ignite and burn, after which the hot mixture re-expands and does useful work in the process, say pushing a piston. The carbon dioxide and other combustion gases weigh about 0.00000005% less than what was in the piston initially, and this mass difference turns into the heat energy driving the engine. A sphalerizer would compress ordinary matter to a couple of quadrillion degrees, and then let it re-expand and cool once the sphalerons had done their thing. *6 We already know the result of this experiment, because our early Universe performed it for us about 13.8 billion years ago, when it was that hot: almost 100% of the matter gets converted into energy, with less than a billionth of the particles left over being the stuff that ordinary matter is made of: quarks and electrons. So it’s just like a diesel engine, except over a billion times more efficient! Another advantage is that you don’t need to be finicky about what to fuel it with—it works with anything made of quarks, meaning any normal matter at all. 
Because of these high-temperature processes, our baby Universe produced over a trillion times more radiation (photons and neutrinos) than matter (quarks and electrons that later clumped into atoms). During the 13.8 billion years since then, a great segregation took place, where atoms became concentrated into galaxies, stars and planets, while most photons stayed in intergalactic space, forming the cosmic microwave background radiation that has been used to make baby pictures of our Universe. Any advanced life form living in a galaxy or other matter concentration can therefore turn most of its available matter back into energy, rebooting the matter percentage down to the same tiny value that emerged from our early Universe by briefly re-creating those hot dense conditions inside a sphalerizer. 
From a physics perspective, everything that future life may want to create—from habitats and machines to new life forms—is simply elementary particles arranged in some particular way. Just as a blue whale is rearranged krill and krill is rearranged plankton, our entire Solar System is simply hydrogen rearranged during 13.8 billion years of cosmic evolution: gravity rearranged hydrogen into stars which rearranged the hydrogen into heavier atoms, after which gravity rearranged such atoms into our planet where chemical and biological processes rearranged them into life. 
Future life that has reached its technological limit can perform such particle rearrangements more rapidly and efficiently, by first using its computing power to figure out the most efficient method and then using its available energy to power the matter rearrangement process. We saw how matter can be converted into both computers and energy, so it’s in a sense the only fundamental resource needed. *7 Once future life has bumped up against the physical limits on what it can do with its matter, there is only one way left for it to do more: by getting more matter. And the only way it can do this is by expanding into our Universe. Spaceward ho! 
The first challenge is that our Universe is expanding, which means that almost all galaxies are flying away from us, so settling distant galaxies amounts to a game of catch-up. The second challenge is that this cosmic expansion is accelerating, due to the mysterious dark energy that makes up about 70% of our 
Universe. To understand how this causes trouble, imagine that you enter a train platform and see your train slowly accelerating away from you, but with a door left invitingly open. If you’re fast and foolhardy, can you catch the train? Since it will eventually go faster than you can run, the answer clearly depends on how far away from you the train is initially: if it’s beyond a certain critical distance, you’ll never catch up with it. We face the same situation trying to catch those distant galaxies that are accelerating away from us: even if we could travel at the speed of light, all galaxies beyond about 17 billion light-years remain forever out of reach—and that’s over 98% of the galaxies in our Universe. 
But hold on: didn’t Einstein’s special relativity theory say that nothing can travel faster than light? So how can galaxies outrace something traveling at the speed of light? The answer is that special relativity is superseded by Einstein’s general relativity theory, where the speed limit is more liberal: nothing can travel faster than the speed of light through space, but space is free to expand as fast as it wants. Einstein also gave us a nice way of visualizing these speed limits by viewing time as the fourth dimension in spacetime (see figure 6.7, where I’ve kept things three-dimensional by omitting one of the three space dimensions). If space weren’t expanding, light rays would form slanted 45-degree lines through spacetime, so that the regions we can see and reach from here and now are cones. Whereas our past light cone would be truncated by our Big Bang 13.8 billion years ago, our future light cone would expand forever, giving us access to an unlimited cosmic endowment. In contrast, the middle panel of the figure shows that an expanding universe with dark energy (which appears to be the Universe we inhabit) deforms our light cones into a champagne-glass shape, forever limiting the number of galaxies we can settle to about 10 billion. 
If this limit makes you feel cosmic claustrophobia, let me cheer you up with a possible loophole: my calculation assumes that dark energy remains constant over time, consistent with what the latest measurements suggest. However, we still have no clue what dark energy really is, which leaves a glimmer of hope that dark energy will eventually decay away (much like the similar dark-energy-like substance postulated to explain cosmic inflation), and if this happens, the acceleration will give way to deceleration, potentially enabling future life forms to keep settling new galaxies for as long as they last. 
Another popular idea is to build a rocket that need not carry its own fuel. For example, interstellar space isn’t a perfect vacuum, but contains the occasional hydrogen ion (a lone proton: a hydrogen atom that’s lost its electron). In 1960, this gave physicist Robert Bussard the idea behind what’s now known as a Bussard ramjet: to scoop up such ions en route and use them as rocket fuel in an onboard fusion reactor. Although recent work has cast doubts on whether this can be made to work in practice, there’s another carry-no-fuel idea that does appear feasible for a high-tech spacefaring civilization: laser sailing. 
The possibility of superintelligence completely transforms this picture, making it much more promising for those with intergalactic wanderlust. Removing the need to transport bulky human life-support systems and adding AI-invented technology, intergalactic settlement suddenly appears rather straightforward. Forward’s laser sailing becomes much cheaper when the spacecraft need merely be large enough to contain a “seed probe”: a robot capable of landing on an asteroid or planet in the target solar system and building up a new civilization from scratch. It doesn’t even have to carry the instructions with it: all it has to do is build a receiving antenna large enough to pick up more detailed blueprints and instructions transmitted from its mother civilization at the speed of light. Once done, it uses its newly constructed lasers to send out new seed probes to continue settling the galaxy one solar system at a time. Even the vast dark expanses of space between galaxies tend to contain a significant number of intergalactic stars (rejects once ejected from their home galaxies) that can be used as way stations, thus enabling an island-hopping strategy for intergalactic laser sailing. 
If dark energy continues to accelerate distant galaxies away from one another, as the latest experimental data suggests, then this will pose a major nuisance to the future of life. It means that even if a future civilization manages to settle a million galaxies, dark energy will over the course of tens of billions of years fragment this cosmic empire into thousands of different regions unable to communicate with one another. If future life does nothing to prevent this fragmentation, then the largest remaining bastions of life will be clusters containing about a thousand galaxies, whose combined gravity is strong enough to overpower the dark energy trying to separate them. 
If a superintelligent civilization wants to stay connected, this would give it a strong incentive to do large-scale cosmic engineering. How much matter will it have time to move into its largest supercluster before dark energy puts it forever out of reach? One method for moving a star large distances is to nudge a third star into a binary system where two stars are stably orbiting each other. Just as with romantic relationships, the introduction of a third partner can destabilize things and lead to one of the three being violently ejected—in the stellar case, at great speed. If some of the three partners are black holes, such a volatile threesome can be used to fling mass fast enough to fly far outside the host galaxy. Unfortunately, this three-body technique, applied either to stars, black holes or galaxies, doesn’t appear able to move more than a tiny fraction of a civilization’s mass the large distances required to outsmart dark energy.
If, despite its best attempts at cosmic engineering, a future civilization concludes that parts of it are doomed to drift out of contact forever, it might simply let them go and wish them well. However, if it has ambitious computing goals that involve seeking the answers to certain very difficult questions, it might instead resort to a slash-and-burn strategy: it could convert the outlying galaxies into massive computers that transform their matter and energy into computation at a frenzied pace, in the hope that before dark energy pushes their burnt-out remnants from view, they could transmit the long-sought answers back to the mother cluster. This slash-and-burn strategy would be particularly appropriate for regions so distant that they can only be reached by the “cosmic spam” method, much to the chagrin of the preexisting inhabitants. Back home in the mother region, the civilization could instead aim for maximum conservation and efficiency to last as long as possible. 
After exploring how long future life can last, let’s explore how long it might want to last. Although you might find it natural to want to live as long as possible, Freeman Dyson also gave a more quantitative argument for this desire: the cost of computation drops when you compute slowly, so you’ll ultimately get more done if you slow things down as much as possible. Freeman even calculated that if our Universe keeps expanding and cooling forever, an infinite amount of computation might be possible. 
Slow doesn’t necessarily mean boring: if future life lives in a simulated world, its subjectively experienced flow of time need not have anything to do with the glacial pace at which the simulation is being run in the outside world, so the prospects of infinite computation could translate into subjective immortality for simulated life forms. Cosmologist Frank Tipler has built on this idea to speculate that you could also achieve subjective immortality in the final moments before a Big Crunch by speeding up the computations toward infinity as the temperature and density skyrocketed. 
So if life engulfs our cosmos, what form will it choose: simple and fast, or complex and slow? I predict that it will make the same choice as Earth life has made: both! The denizens of Earth’s biosphere span a staggering range of sizes, from gargantuan two-hundred-ton blue whales down to the petite 10-16 kg bacterium Pelagibacter, believed to account for more biomass than all the world’s fish combined. Moreover, organisms that are large, complex and slow often mitigate their sluggishness by containing smaller modules that are simple and fast. For example, your blink reflex is extremely fast precisely because it’s implemented by a small and simple circuit that doesn’t involve most of your brain: if that hard-to-swat fly accidentally heads toward your eye, you’ll blink within a tenth of a second, long before the relevant information has had time to spread throughout your brain and make you consciously aware of what happened. By organizing its information processing into a hierarchy of modules, our biosphere manages to both have the cake and eat it, attaining both speed and complexity. We humans already use this same hierarchical strategy to optimize parallel computing. 
What, if any, of this future information processing will be conscious in the sense of involving a subjective experience is a controversial and fascinating topic which we’ll explore in chapter 8. If consciousness requires the different parts of the system to be able to communicate with one another, then the thoughts of larger systems are by necessity slower. Whereas you or a future Earth-sized supercomputer can have many thoughts per second, a galaxy-sized mind could have only one thought every hundred thousand years, and a cosmic mind a billion light-years in size would only have time to have about ten thoughts in total before dark energy fragmented it into disconnected parts. On the other hand, these few precious thoughts and accompanying experiences might be quite deep! 
However, if superintelligence develops technology that can readily rearrange elementary particles into any form of matter whatsoever, then it will eliminate most of the incentive for long-distance trade. Why bother shipping silver between distant solar systems when it’s simpler and quicker to transmute copper into silver by rearranging its particles? Why bother shipping high-tech machinery between galaxies when both the know-how and the raw materials (any matter will do) exist in both places? My guess is that in a cosmos teeming with superintelligence, almost the only commodity worth shipping long distances will be information. The only exception might be matter to be used for cosmic engineering projects—for example, to counteract the aforementioned destructive tendency of dark energy to tear civilizations apart. As opposed to traditional human trade, this matter can be shipped in any convenient bulk form whatsoever, perhaps even as an energy beam, since the receiving superintelligence can rapidly rearrange it into whatever objects it wants. 
If the distance between neighboring space-settling civilizations is much larger than dark energy lets them expand, then they’ll never come into contact with each other or even find out about each other’s existence, so they’ll feel as if they’re alone in the cosmos. If our cosmos is more fecund so that neighbors are closer together, however, some civilizations will eventually overlap. What happens in these overlap regions? Will there be cooperation, competition or war? 
After all, information is very different from the resources that humans usually fight over, in that you can simultaneously give it away and keep it. 
Some expanding civilizations might have goals that are essentially immutable, such as those of a fundamentalist cult or a spreading virus. However, it’s also plausible that some advanced civilizations are more like open-minded humans— willing to adjust their goals when presented with sufficiently compelling arguments. If two of them meet, there will be a clash not of weapons but of ideas, where the most persuasive one prevails and has its goals spread at the speed of light through the region controlled by the other civilization. Assimilating your neighbors is a faster expansion strategy than settlement, since your sphere of influence can spread at the speed with which ideas move (the speed of light using telecommunication), whereas physical settlement inevitably progresses slower than the speed of light. This assimilation will not be forced such as that infamously employed by the Borg in Star Trek, but voluntary based on the persuasive superiority of ideas, leaving the assimilated better off. 
We’ve seen that the future cosmos can contain rapidly expanding bubbles of two kinds: expanding civilizations and those death bubbles that expand at light speed and make space uninhabitable by destroying all our elementary particles. An ambitious civilization can thus encounter three kinds of regions: uninhabited ones, life bubbles and death bubbles. If it fears uncooperative rival civilizations, it has a strong incentive to launch a rapid “land grab” and settle the uninhabited regions before the rivals do. However, it has the same expansionist incentive even if there are no other civilizations, simply to acquire resources before dark energy makes them unreachable. We just saw how bumping into another expanding civilization can be either better or worse than bumping into uninhabited space, depending on how cooperative and open-minded this neighbor is. However, it’s better to bump into any expansionist civilization (even one trying to convert your civilization into paper clips) than a death bubble, which will continue expanding at the speed of light regardless of whether you try to fight it or reason with it. 
I give a detailed justification of this argument in my book Our Mathematical Universe, so I won’t rehash it here, but the basic reason for why we’re clueless about this neighbor distance is that we’re in turn clueless about the probability of intelligent life arising in a given place. As the American astronomer Frank Drake pointed out, this probability can be calculated by multiplying together the probability of there being a habitable environment there (say an appropriate planet), the probability that life will form there and the probability that this life will evolve to become intelligent. When I was a grad student, we had no clue about any of these three probabilities. After the past two decades’ dramatic discoveries of planets orbiting other stars, it now seems likely that habitable planets are abundant, with billions in our own Galaxy alone. The probability of evolving life and then intelligence, however, remains extremely uncertain: some experts think that one or both are rather inevitable and occur on most habitable planets, while others think that one or both are extremely rare because of one or more evolutionary bottlenecks that require a wild stroke of luck to pass through. 
This is broad enough to include all above-mentioned definitions, since understanding, self-awareness, problem solving, learning, etc. are all examples of complex goals that one might have. It’s also broad enough to subsume the Oxford Dictionary definition—“the ability to acquire and apply knowledge and skills”—since one can have as a goal to apply knowledge and skills. 
Because there are many possible goals, there are many possible types of intelligence. By our definition, it therefore makes no sense to quantify intelligence of humans, non-human animals or machines by a single number such as an IQ. *1 What’s more intelligent: a computer program that can only play chess or one that can only play Go? There’s no sensible answer to this, since they’re good at different things that can’t be directly compared. 
The DQN AI system of Google DeepMind can accomplish a slightly broader range of goals: it can play dozens of different vintage Atari computer games at human level or better. In contrast, human intelligence is thus far uniquely broad, able to master a dazzling panoply of skills. A healthy child given enough training time can get fairly good not only at any game, but also at any language, sport or vocation. Comparing the intelligence of humans and machines today, we humans win hands-down on breadth, while machines outperform us in a small but growing number of narrow domains, as illustrated in figure 2.1. The holy grail of AI research is to build “general AI” (better known as artificial general intelligence, AGI) that is maximally broad: able to accomplish virtually any goal, including learning. We’ll explore this in detail in chapter 4. The term “AGI” was popularized by the AI researchers Shane Legg, Mark Gubrud and Ben Goertzel to more specifically mean human-level artificial general intelligence: the ability to accomplish any goal at least as well as humans. 1 I’ll stick with their definition, so unless I explicitly qualify the acronym (by writing “superhuman AGI,” for example), I’ll use “AGI” as shorthand for “human-level AGI.”*2 
Although the word “intelligence” tends to have positive connotations, it’s important to note that we’re using it in a completely value-neutral way: as ability to accomplish complex goals regardless of whether these goals are considered good or bad. 
It’s natural for us to rate the difficulty of tasks relative to how hard it is for us humans to perform them, as in figure 2.1. But this can give a misleading picture of how hard they are for computers. It feels much harder to multiply 314,159 by 271,828 than to recognize a friend in a photo, yet computers creamed us at arithmetic long before I was born, while human-level image recognition has only recently become possible. This fact that low-level sensorimotor tasks seem easy despite requiring enormous computational resources is known as Moravec’s paradox, and is explained by the fact that our brain makes such tasks feel easy by dedicating massive amounts of customized hardware to them—more than a quarter of our brains, in fact. 
During the decades since he wrote those passages, the sea level has kept rising relentlessly, as he predicted, like global warming on steroids, and some of his foothills (including chess) have long since been submerged. What comes next and what we should do about it is the topic of the rest of this book. 
As the sea level keeps rising, it may one day reach a tipping point, triggering dramatic change. This critical sea level is the one corresponding to machines becoming able to perform AI design. Before this tipping point is reached, the sea-level rise is caused by humans improving machines; afterward, the rise can be driven by machines improving machines, potentially much faster than humans could have done, rapidly submerging all land. This is the fascinating and controversial idea of the singularity, which we’ll have fun exploring in chapter 4. 
Computer pioneer Alan Turing famously proved that if a computer can perform a certain bare minimum set of operations, then, given enough time and memory, it can be programmed to do anything that any other computer can do. Machines exceeding this critical threshold are called universal computers (aka Turing-universal computers); all of today’s smartphones and laptops are universal in this sense. Analogously, I like to think of the critical intelligence threshold required for AI design as the threshold for universal intelligence: given enough time and resources, it can make itself able to accomplish any goal as well as any other intelligent entity. For example, if it decides that it wants better social skills, forecasting skills or AI-design skills, it can acquire them.
If we say that an atlas contains information about the world, we mean that there’s a relation between the state of the book (in particular, the positions of certain molecules that give the letters and images their colors) and the state of the world (for example, the locations of continents). If the continents were in different places, then those molecules would be in different places as well. We humans use a panoply of different devices for storing information, from books and brains to hard drives, and they all share this property: that their state can be related to (and therefore inform us about) the state of other things that we care about. 
What fundamental physical property do they all have in common that makes them useful as memory devices, i.e., devices for storing information? The answer is that they all can be in many different long-lived states—long-lived enough to encode the information until it’s needed. As a simple example, suppose you place a ball on a hilly surface that has sixteen different valleys, as in figure 2.3. Once the ball has rolled down and come to rest, it will be in one of sixteen places, so you can use its position as a way of remembering any number between 1 and 16. 
This memory device is rather robust, because even if it gets a bit jiggled and disturbed by outside forces, the ball is likely to stay in the same valley that you put it in, so you can still tell which number is being stored. The reason that this memory is so stable is that lifting the ball out of its valley requires more energy than random disturbances are likely to provide. This same idea can provide stable memories much more generally than for a movable ball: the energy of a complicated physical system can depend on all sorts of mechanical, chemical, electrical and magnetic properties, and as long as it takes energy to change the system away from the state you want it to remember, this state will be stable. This is why solids have many long-lived states, whereas liquids and gases don’t: if you engrave someone’s name on a gold ring, the information will still be there years later because reshaping the gold requires significant energy, but if you engrave it in the surface of a pond, it will be lost within a second as the water surface effortlessly changes its shape.
We can therefore think of it as encoding a binary digit (abbreviated “bit”), i.e., a zero or a one. The information stored by any more complicated memory device can equivalently be stored in multiple bits: for example, taken together, the four bits shown in figure 2.3 can be in 2 × 2 × 2 × 2 = 16 different states 0000, 0001, 0010, 0011,…, 1111, so they collectively have exactly the same memory capacity as the more complicated 16-state system. We can therefore think of bits as atoms of information—the smallest indivisible chunk of information that can’t be further subdivided, which can combine to make up any information. For example, I just typed the word “word,” and my laptop represented it in its memory as the 4-number sequence 119 111 114 100, storing each of those numbers as 8 bits (it represents each lowercase letter by a number that’s 96 plus its order in the alphabet). As soon as I hit the w key on my keyboard, my laptop displayed a visual image of a w on my screen, and this image is also represented by bits: 32 bits specify the color of each of the screen’s millions of pixels. 
Since two-state systems are easy to manufacture and work with, most modern computers store their information as bits, but these bits are embodied in a wide variety of ways. On a DVD, each bit corresponds to whether there is or isn’t a microscopic pit at a given point on the plastic surface. On a hard drive, each bit corresponds to a point on the surface being magnetized in one of two ways. In my laptop’s working memory, each bit corresponds to the positions of certain electrons, determining whether a device called a micro-capacitor is charged. Some kinds of bits are convenient to transport as well, even at the speed of light: for example, in an optical fiber transmitting your email, each bit corresponds to a laser beam being strong or weak at a given time. 
Engineers prefer to encode bits into systems that aren’t only stable and easy to read from (as a gold ring), but also easy to write to: altering the state of your hard drive requires much less energy than engraving gold. They also prefer systems that are convenient to work with and cheap to mass-produce. But other than that, they simply don’t care about how the bits are represented as physical objects—and nor do you most of the time, because it simply doesn’t matter!
In other words, information can take on a life of its own, independent of its physical substrate! Indeed, it’s usually only this substrateindependent aspect of information that we’re interested in: if your friend calls you up to discuss that document you sent, she’s probably not calling to talk about voltages or molecules. This is our first hint of how something as intangible as intelligence can be embodied in tangible physical stuff, and we’ll soon see how this idea of substrate independence is much deeper, including not only information but also computation and learning. 
Because of this substrate independence, clever engineers have been able to repeatedly replace the memory devices inside our computers with dramatically better ones, based on new technologies, without requiring any changes whatsoever to our software. The result has been spectacular, as illustrated in figure 2.4: over the past six decades, computer memory has gotten half as expensive roughly every couple of years. Hard drives have gotten over 100 million times cheaper, and the faster memories useful for computation rather than mere storage have become a whopping 10 trillion times cheaper. If you could get such a “99.99999999999% off” discount on all your shopping, you could buy all real estate in New York City for about 10 cents and all the gold that’s ever been mined for around a dollar. 
For many of us, the spectacular improvements in memory technology come with personal stories. I fondly remember working in a candy store back in high school to pay for a computer sporting 16 kilobytes of memory, and when I made and sold a word processor for it with my high school classmate Magnus Bodin, we were forced to write it all in ultra-compact machine code to leave enough memory for the words that it was supposed to process. After getting used to floppy drives storing 70kB, I became awestruck by the smaller 3.5-inch floppies that could store a whopping 1.44MB and hold a whole book, and then my firstever hard drive storing 10MB—which might just barely fit a single one of today’s song downloads. These memories from my adolescence felt almost unreal the other day, when I spent about $100 on a hard drive with 300,000 times more capacity.
Comparing these numbers with the machine memories shows that the world’s best computers can now out-remember any biological system—at a cost that’s rapidly dropping and was a few thousand dollars in 2016. 
The memory in your brain works very differently from computer memory, not only in terms of how it’s built, but also in terms of how it’s used. Whereas you retrieve memories from a computer or hard drive by specifying where it’s stored, you retrieve memories from your brain by specifying something about what is stored. Each group of bits in your computer’s memory has a numerical address, and to retrieve a piece of information, the computer specifies at what address to look, just as if I tell you “Go to my bookshelf, take the fifth book from the right on the top shelf, and tell me what it says on page 314.” In contrast, you retrieve information from your brain similarly to how you retrieve it from a search engine: you specify a piece of the information or something related to it, and it pops up. If I tell you “to be or not,” or if I google it, chances are that it will trigger “To be, or not to be, that is the question.” Indeed, it will probably work even if I use another part of the quote or mess things up somewhat. Such memory systems are called auto-associative, since they recall by association rather than by address. 
In a famous 1982 paper, the physicist John Hopfield showed how a network of interconnected neurons could function as an auto-associative memory. I find the basic idea very beautiful, and it works for any physical system with multiple stable states. For example, consider a ball on a surface with two valleys, like the one-bit system in figure 2.3, and let’s shape the surface so that the x-coordinates 
of the two minima where the ball can come to rest are x = √2 ≈ 1.41421 and x = π ≈ 3.14159, respectively. If you remember only that π is close to 3, you simply 
put the ball at x = 3 and watch it reveal a more exact π-value as it rolls down to the nearest minimum. Although it sounds deceptively simple, this idea of a function is incredibly general. Some functions are rather trivial, such as the one called NOT that inputs a single bit and outputs the reverse, thus turning zero into one and vice versa. The functions we learn about in school typically correspond to buttons on a pocket calculator, inputting one or more numbers and outputting a single number —for example, the function x2 simply inputs a number and outputs it multiplied by itself. Other functions can be extremely complicated. For instance, if you’re in possession of a function that would input bits representing an arbitrary chess position and output bits representing the best possible next move, you can use it to win the World Computer Chess Championship. If you’re in possession of a function that inputs all the world’s financial data and outputs the best stocks to buy, you’ll soon be extremely rich. Many AI researchers dedicate their careers to figuring out how to implement certain functions. 
In other words, if you can implement highly complex functions, then you can build an intelligent machine that’s able to accomplish highly complex goals. This brings our question of how matter can be intelligent into sharper focus: in particular, how can a clump of seemingly dumb matter compute a complicated function? 
Rather than just remain immobile as a gold ring or other static memory device, it must exhibit complex dynamics so that its future state depends in some complicated (and hopefully controllable/programmable) way on the present state. Its atom arrangement must be less ordered than a rigid solid where nothing interesting changes, but more ordered than a liquid or gas. Specifically, we want the system to have the property that if we put it in a state that encodes the input information, let it evolve according to the laws of physics for some amount of time, and then interpret the resulting final state as the output information, then the output is the desired function of the input. 
As a first example of this idea, let’s explore how we can build a very simple (but also very important) function called a NAND gate *3 out of plain old dumb matter. This function inputs two bits and outputs one bit: it outputs 0 if both inputs are 1; in all other cases, it outputs 1. If we connect two switches in series with a battery and an electromagnet, then the electromagnet will only be on if the first switch and the second switch are closed (“on”). Let’s place a third switch under the electromagnet, as illustrated in figure 2.6, such that the magnet will pull it open whenever it’s powered on. If we interpret the first two switches as the input bits and the third one as the output bit (with 0 = switch open, and 1 = switch closed), then we have ourselves a NAND gate: the third switch is open only if the first two are closed. There are many other ways of building NAND gates that are more practical—for example, using transistors as illustrated in figure 2.6. In today’s computers, NAND gates are typically built from microscopic transistors and other components that can be automatically etched onto silicon wafers. 
There’s a remarkable theorem in computer science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates. *4 So if you can build enough NAND gates, you can build a device computing anything! In case you’d like a taste of how this works, I’ve illustrated in figure 2.7 how to multiply numbers using nothing but NAND gates. 
As a first example of this idea, let’s explore how we can build a very simple (but also very important) function called a NAND gate *3 out of plain old dumb matter. This function inputs two bits and outputs one bit: it outputs 0 if both inputs are 1; in all other cases, it outputs 1. If we connect two switches in series with a battery and an electromagnet, then the electromagnet will only be on if the first switch and the second switch are closed (“on”). Let’s place a third switch under the electromagnet, as illustrated in figure 2.6, such that the magnet will pull it open whenever it’s powered on. If we interpret the first two switches as the input bits and the third one as the output bit (with 0 = switch open, and 1 = switch closed), then we have ourselves a NAND gate: the third switch is open only if the first two are closed. There are many other ways of building NAND gates that are more practical—for example, using transistors as illustrated in figure 2.6. In today’s computers, NAND gates are typically built from microscopic transistors and other components that can be automatically etched onto silicon wafers. 
There’s a remarkable theorem in computer science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates. *4 So if you can build enough NAND gates, you can build a device computing anything! In case you’d like a taste of how this works, I’ve illustrated in figure 2.7 how to multiply numbers using nothing but NAND gates. 
You’d also have no way of knowing what type of transistors the microprocessor was using. 
I first came to appreciate this crucial idea of substrate independence because there are many beautiful examples of it in physics. Waves, for instance: they have properties such as speed, wavelength and frequency, and we physicists can study the equations they obey without even needing to know what particular substance they’re waves in. When you hear something, you’re detecting sound waves caused by molecules bouncing around in the mixture of gases that we call air, and we can calculate all sorts of interesting things about these waves—how their intensity fades as the square of the distance, such as how they bend when they pass through open doors and how they bounce off of walls and cause echoes —without knowing what air is made of. In fact, we don’t even need to know that it’s made of molecules: we can ignore all details about oxygen, nitrogen, carbon dioxide, etc., because the only property of the wave’s substrate that matters and enters into the famous wave equation is a single number that we can measure: the wave speed, which in this case is about 300 meters per second. Indeed, this wave equation that I taught my MIT students about in a course last spring was first discovered and put to great use long before physicists had even established that atoms and molecules existed! 
This wave example illustrates three important points. First, substrate independence doesn’t mean that a substrate is unnecessary, but that most of its details don’t matter. You obviously can’t have sound waves in a gas if there’s no gas, but any gas whatsoever will suffice. Similarly, you obviously can’t have computation without matter, but any matter will do as long as it can be arranged into NAND gates, connected neurons or some other building block enabling universal computation. Second, the substrate-independent phenomenon takes on a life of its own, independent of its substrate. A wave can travel across a lake, even though none of its water molecules do—they mostly bob up and down, like fans doing “the wave” in a sports stadium. Third, it’s often only the substrateindependent aspect that we’re interested in: a surfer usually cares more about the position and height of a wave than about its detailed molecular composition. 
We’ve now arrived at an answer to our opening question about how tangible physical stuff can give rise to something that feels as intangible, abstract and ethereal as intelligence: it feels so non-physical because it’s substrateindependent, taking on a life of its own that doesn’t depend on or reflect the physical details. In short, computation is a pattern in the spacetime arrangement of particles, and it’s not the particles but the pattern that really matters! Matter doesn’t matter. 
In other words, the hardware is the matter and the software is the pattern. This substrate independence of computation implies that AI is possible: intelligence doesn’t require flesh, blood or carbon atoms. 
Because of this substrate independence, shrewd engineers have been able to repeatedly replace the technologies inside our computers with dramatically better ones, without changing the software. The results have been every bit as spectacular as those for memory devices. As illustrated in figure 2.8, computation keeps getting half as expensive roughly every couple of years, and this trend has now persisted for over a century, cutting the computer cost a whopping million million million (1018) times since my grandmothers were born. If everything got a million million million times cheaper, then a hundredth of a cent would enable you to buy all goods and services produced on Earth this year. This dramatic drop in costs is of course a key reason why computation is everywhere these days, having spread from the building-sized computing facilities of yesteryear into our homes, cars and pockets—and even turning up in unexpected places such as sneakers. 
All examples of persistent doubling that I know of in nature have the same fundamental cause, and this technological one is no exception: each step creates the next. For example, you yourself underwent exponential growth right after your conception: each of your cells divided and gave rise to two cells roughly daily, causing your total number of cells to increase day by day as 1, 2, 4, 8, 16 and so on. According to the most popular scientific theory of our cosmic origins, known as inflation, our baby Universe once grew exponentially just like you did, repeatedly doubling its size at regular intervals until a speck much smaller and lighter than an atom had grown more massive than all the galaxies we’ve ever seen with our telescopes. Again, the cause was a process whereby each doubling step caused the next. This is how technology progresses as well: once 
technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. 
Something that occurs just as regularly as the doubling of our technological power is the appearance of claims that the doubling is ending. Yes, Moore’s law will of course end, meaning that there’s a physical limit to how small transistors can be made. But some people mistakenly assume that Moore’s law is synonymous with the persistent doubling of our technological power. Contrariwise, Ray Kurzweil points out that Moore’s law involves not the first but the fifth technological paradigm to bring exponential growth in computing, as illustrated in figure 2.8: whenever one technology stopped improving, we replaced it with an even better one. When we could no longer keep shrinking our vacuum tubes, we replaced them with transistors and then integrated circuits, where electrons move around in two dimensions. When this technology reaches its limits, there are many other alternatives we can try—for example, using three-dimensional circuits and using something other than electrons to do our bidding. 
Nobody knows for sure what the next blockbuster computational substrate will be, but we do know that we’re nowhere near the limits imposed by the laws of physics. My MIT colleague Seth Lloyd has worked out what this fundamental limit is, and as we’ll explore in greater detail in chapter 6, this limit is a whopping 33 orders of magnitude (1033 times) beyond today’s state of the art for how much computing a clump of matter can do. So even if we keep doubling the power of our computers every couple of years, it will take over two centuries until we reach that final frontier. 
Although all universal computers are capable of the same computations, some are more efficient than others. For example, a computation requiring millions of multiplications doesn’t require millions of separate multiplication modules built from separate transistors as in figure 2.6: it needs only one such module, since it can use it many times in succession with appropriate inputs. In this spirit of efficiency, most modern computers use a paradigm where computations are split into multiple time steps, during which information is shuffled back and forth between memory modules and computation modules. 
Today’s computers often gain additional speed by parallel processing, which cleverly undoes some of this reuse of modules: if a computation can be split into parts that can be done in parallel (because the input of one part doesn’t require the output of another), then the parts can be computed simultaneously by different parts of the hardware. 
The ultimate parallel computer is a quantum computer. Quantum computing pioneer David Deutsch controversially argues that “quantum computers share information with huge numbers of versions of themselves throughout the multiverse,” and can get answers faster here in our Universe by in a sense getting help from these other versions. 4 We don’t yet know whether a commercially competitive quantum computer can be built during the coming decades, because it depends both on whether quantum physics works as we think it does and on our ability to overcome daunting technical challenges, but companies and governments around the world are betting tens of millions of dollars annually on the possibility. Although quantum computers cannot speed up run-of-the-mill computations, clever algorithms have been developed that may dramatically speed up specific types of calculations, such as cracking cryptosystems and training neural networks. A quantum computer could also efficiently simulate the behavior of quantum-mechanical systems, including atoms, molecules and new materials, replacing measurements in chemistry labs in the same way that simulations on traditional computers have replaced measurements in wind tunnels. 
Although a pocket calculator can crush me in an arithmetic contest, it will never improve its speed or accuracy, no matter how much it practices. It doesn’t learn: for example, every time I press its square-root button, it computes exactly the same function in exactly the same way. Similarly, the first computer program that ever beat me at chess never learned from its mistakes, but merely implemented a function that its clever programmer had designed to compute a good next move. In contrast, when Magnus Carlsen lost his first game of chess at age five, he began a learning process that made him the World Chess Champion eighteen years later. 
The ability to learn is arguably the most fascinating aspect of general intelligence. We’ve already seen how a seemingly dumb clump of matter can remember and compute, but how can it learn? We’ve seen that finding the answer to a difficult question corresponds to computing a function, and that appropriately arranged matter can calculate any computable function. When we humans first created pocket calculators and chess programs, we did the arranging. For matter to learn, it must instead rearrange itself to get better and better at computing the desired function—simply by obeying the laws of physics. 
To demystify the learning process, let’s first consider how a very simple physical system can learn the digits of π and other numbers. Above we saw how a surface with many valleys (see figure 2.3) can be used as a memory device: for 
example, if the bottom of one of the valleys is at position x = π ≈ 3.14159 and 
there are no other valleys nearby, then you can put a ball at x = 3 and watch the system compute the missing decimals by letting the ball roll down to the bottom. Now, suppose that the surface is made of soft clay and starts out completely flat, as a blank slate. If some math enthusiasts repeatedly place the ball at the locations of each of their favorite numbers, then gravity will gradually create valleys at these locations, after which the clay surface can be used to recall these stored memories. 
Neural networks have now transformed both biological and artificial intelligence, and have recently started dominating the AI subfield known as machine learning (the study of algorithms that improve through experience). Before delving deeper into how such networks can learn, let’s first understand how they can compute. A neural network is simply a group of interconnected neurons that are able to influence each other’s behavior. Your brain contains about as many neurons as there are stars in our Galaxy: in the ballpark of a hundred billion. On average, each of these neurons is connected to about a thousand others via junctions called synapses, and it’s the strengths of these roughly hundred trillion synapse connections that encode most of the information in your brain. 
We can schematically draw a neural network as a collection of dots representing neurons connected by lines representing synapses (see figure 2.9). Real-world neurons are very complicated electrochemical devices looking nothing like this schematic illustration: they involve different parts with names such as axons and dendrites, there are many different kinds of neurons that operate in a wide variety of ways, and the exact details of how and when electrical activity in one neuron affects other neurons is still the subject of active study. However, AI researchers have shown that neural networks can still attain human-level performance on many remarkably complex tasks even if one ignores all these complexities and replaces real biological neurons with extremely simple simulated ones that are all identical and obey very simple rules. The currently most popular model for such an artificial neural network represents the state of each neuron by a single number and the strength of each synapse by a single number. 
The success of these simple artificial neural networks is yet another example of substrate independence: neural networks have great computational power seemingly independent of the low-level nitty-gritty details of their construction. Indeed, George Cybenko, Kurt Hornik, Maxwell Stinchcombe and Halbert White proved something remarkable in 1989: such simple neural networks are universal in the sense that they can compute any function arbitrarily accurately, by simply adjusting those synapse strength numbers accordingly. In other words, evolution probably didn’t make our biological neurons so complicated because it was necessary, but because it was more efficient—and because evolution, as opposed to human engineers, doesn’t reward designs that are simple and easy to understand. 
When I first learned about this, I was mystified by how something so simple could compute something arbitrarily complicated. 
Although you can prove that you can compute anything in theory with an arbitrarily large neural network, the proof doesn’t say anything about whether you can do so in practice, with a network of reasonable size. In fact, the more I thought about it, the more puzzled I became that neural networks worked so well. 
For example, suppose that we wish to classify megapixel grayscale images into two categories, say cats or dogs. If each of the million pixels can take one of, say, 256 values, then there are 2561000000 possible images, and for each one, we wish to compute the probability that it depicts a cat. This means that an arbitrary function that inputs a picture and outputs a probability is defined by a list of 2561000000 probabilities, that is, way more numbers than there are atoms in our Universe (about 1078). Yet neural networks with merely thousands or millions of parameters somehow manage to perform such classification tasks quite well. How can successful neural networks be “cheap,” in the sense of requiring so few parameters? After all, you can prove that a neural network small enough to fit inside our Universe will epically fail to approximate almost all functions, succeeding merely on a ridiculously tiny fraction of all computational tasks that you might assign to it. 
I’ve had lots of fun puzzling over this and related mysteries with my student Henry Lin. One of the things I feel most grateful for in life is the opportunity to collaborate with amazing students, and Henry is one of them. When he first walked into my office to ask whether I was interested in working with him, I thought to myself that it would be more appropriate for me to ask whether he was interested in working with me: this modest, friendly and bright-eyed kid from Shreveport, Louisiana, had already written eight scientific papers, won a Forbes 30-Under-30 award, and given a TED talk with over a million views— and he was only twenty! A year later, we wrote a paper together with a surprising 
conclusion: the question of why neural networks work so well can’t be answered with mathematics alone, because part of the answer lies in physics. We found that the class of functions that the laws of physics throw at us and make us interested in computing is also a remarkably tiny class because, for reasons that we still don’t fully understand, the laws of physics are remarkably simple. Moreover, the tiny fraction of functions that neural networks can compute is very similar to the tiny fraction that physics makes us interested in! We also extended previous work showing that deep-learning neural networks (they’re called “deep” if they contain many layers) are much more efficient than shallow ones for many of these functions of interest. 
This helps explain not only why neural networks are now all the rage among AI researchers, but also why we evolved neural networks in our brains: if we evolved brains to predict the future, then it makes sense that we’d evolve a computational architecture that’s good at precisely those computational problems that matter in the physical world. 
Now that we’ve explored how neural networks work and compute, let’s return to the question of how they can learn. Specifically, how can a neural network get better at computing by updating its synapses? 
In his seminal 1949 book, The Organization of Behavior: A Neuropsychological Theory, the Canadian psychologist Donald Hebb argued that if two nearby neurons were frequently active (“firing”) at the same time, their synaptic coupling would strengthen so that they learned to help trigger each other—an idea captured by the popular slogan “Fire together, wire together.” Although the details of how actual brains learn are still far from understood, and research has shown that the answers are in many cases much more complicated, it’s also been shown that even this simple learning rule (known as Hebbian learning) allows neural networks to learn interesting things. John Hopfield showed that Hebbian learning allowed his oversimplified artificial neural network to store lots of complex memories by simply being exposed to them repeatedly. Such exposure to information to learn from is usually called “training” when referring to artificial neural networks (or to animals or people being taught skills), although “studying,” “education” or “experience” might be just as apt. 
As if by magic, this simple rule can make the neural network learn remarkably complex computations if training is performed with large amounts of data. We don’t yet know precisely what learning rules our brains use, but whatever the answer may be, there’s no indication that they violate the laws of physics. 
Just as most digital computers gain efficiency by splitting their work into multiple steps and reusing computational modules many times, so do many artificial and biological neural networks. Brains have parts that are what computer scientists call recurrent rather than feedforward neural networks, where information can flow in multiple directions rather than just one way, so that the current output can become input to what happens next. The network of logic gates in the microprocessor of a laptop is also recurrent in this sense: it keeps reusing its past information, and lets new information input from a keyboard, trackpad, camera, etc., affect its ongoing computation, which in turn determines information output to, say, a screen, loudspeaker, printer or wireless network. Analogously, the network of neurons in your brain is recurrent, letting information input from your eyes, ears and other senses affect its ongoing computation, which in turn determines information output to your muscles. 
The history of learning is at least as long as the history of life itself, since every self-reproducing organism performs interesting copying and processing of information—behavior that has somehow been learned. During the era of Life 1.0, however, organisms didn’t learn during their lifetime: their rules for processing information and reacting were determined by their inherited DNA, so the only learning occurred slowly at the species level, through Darwinian evolution across generations. 
As we all know, the explosive improvements in computer memory and computational power (figure 2.4 and figure 2.8) have translated into spectacular progress in artificial intelligence—but it took a long time until machine learning came of age. When IBM’s Deep Blue computer overpowered chess champion Garry Kasparov in 1997, its major advantages lay in memory and computation, not in learning. Its computational intelligence had been created by a team of humans, and the key reason that Deep Blue could outplay its creators was its ability to compute faster and thereby analyze more potential positions. When IBM’s Watson computer dethroned the human world champion in the quiz show Jeopardy!, it too relied less on learning than on custom-programmed skills and superior memory and speed. The same can be said of most early breakthroughs in robotics, from legged locomotion to self-driving cars and self-landing rockets. 
In contrast, the driving force behind many of the most recent AI breakthroughs has been machine learning. Consider figure 2.11, for example. It’s easy for you to tell what it’s a photo of, but to program a function that inputs nothing but the colors of all the pixels of an image and outputs an accurate caption such as “A group of young people playing a game of frisbee” had eluded all the world’s AI researchers for decades. Yet a team at Google led by Ilya Sutskever did precisely that in 2014. Input a different set of pixel colors, and it replies “A herd of elephants walking across a dry grass field,” again correctly. How did they do it? Deep Blue–style, by programming handcrafted algorithms for detecting frisbees, faces and the like? No, by creating a relatively simple neural network with no knowledge whatsoever about the physical world or its contents, and then letting it learn by exposing it to massive amounts of data. AI visionary Jeff Hawkins wrote in 2004 that “no computer can…see as well as a mouse,” but those days are now long gone. 
Just as we don’t fully understand how our children learn, we still don’t fully understand how such neural networks learn, and why they occasionally fail. But what’s clear is that they’re already highly useful and are triggering a surge of investments in deep learning. Deep learning has now transformed many aspects of computer vision, from handwriting transcription to real-time video analysis for self-driving cars. It has similarly revolutionized the ability of computers to transform spoken language into text and translate it into other languages, even in real time—which is why we can now talk to personal digital assistants such as Siri, Google Now and Cortana. Those annoying CAPTCHA puzzles, where we need to convince a website that we’re human, are getting ever more difficult in order to keep ahead of what machine-learning technology can do. In 2015, Google DeepMind released an AI system using deep learning that was able to master dozens of computer games like a kid would—with no instructions whatsoever—except that it soon learned to play better than any human. In 2016, the same company built AlphaGo, a Go-playing computer system that used deep learning to evaluate the strength of different board positions and defeated the world’s strongest Go champion. 
To learn our goals, an AI must figure out not what we do, but why we do it. We humans accomplish this so effortlessly that it’s easy to forget how hard the task is for a computer, and how easy it is to misunderstand. If you ask a future self-driving car to take you to the airport as fast as possible and it takes you literally, you’ll get there chased by helicopters and covered in vomit. If you exclaim, “That’s not what I wanted!,” it can justifiably answer, “That’s what you asked for.” The same theme recurs in many famous stories. In the ancient Greek legend, King Midas asked that everything he touched turn to gold, but was disappointed when this prevented him from eating and even more so when he inadvertently turned his daughter to gold. In the stories where a genie grants three wishes, there are many variants for the first two wishes, but the third wish is almost always the same: “Please undo the first two wishes, because that’s not what I really wanted.” 
All these examples show that to figure out what people really want, you can’t merely go by what they say. You also need a detailed model of the world, including the many shared preferences that we tend to leave unstated because we consider them obvious, such as that we don’t like vomiting or eating gold. Once we have such a world model, we can often figure out what people want even if they don’t tell us, simply by observing their goal-oriented behavior. Indeed, children of hypocrites usually learn more from what they see their parents do than from what they hear them say. 
AI researchers are currently trying hard to enable machines to infer goals from behavior, and this will be useful also long before any superintelligence comes on the scene. For example, a retired man may appreciate it if his eldercare robot can figure out what he values simply by observing him, so that he’s spared the hassle of having to explain everything with words or computer programming. One challenge involves finding a good way to encode arbitrary systems of goals and ethical principles into a computer, and another challenge is making machines that can figure out which particular system best matches the behavior they observe. 
If this one example were all the AI knew about firefighters, fires and babies, it would indeed be impossible to know which explanation was correct. However, a key idea underlying inverse reinforcement learning is that we make decisions all the time, and that every decision we make reveals something about our goals. The hope is therefore that by observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences. 4 
In the inverse reinforcement-learning approach, a core idea is that the AI is trying to maximize not the goal-satisfaction of itself, but that of its human owner. 
It therefore has an incentive to be cautious when it’s unclear about what its owner wants, and to do its best to find out. 
It should also be fine with its owner switching it off, since that would imply that it had misunderstood what its owner really wanted. 
Even if an AI can be built to learn what your goals are, this doesn’t mean that it will necessarily adopt them. Consider your least favorite politicians: you know what they want, but that’s not what you want, and even though they try hard, they’ve failed to persuade you to adopt their goals. 
We have many strategies for imbuing our children with our goals—some more successful than others, as I’ve learned from raising two teenage boys. When those to be persuaded are computers rather than people, the challenge is known as the value-loading problem, and it’s even harder than the moral education of children. Consider an AI system whose intelligence is gradually being improved from subhuman to superhuman, first by us tinkering with it and then through recursive self-improvement like Prometheus. At first, it’s much less powerful than you, so it can’t prevent you from shutting it down and replacing those parts of its software and data that encode its goals—but this won’t help, because it’s still too dumb to fully understand your goals, which requires human-level intelligence to comprehend. 
The reason that value loading can be harder with machines than with people is that their intelligence growth can be much faster: whereas children can spend many years in that magic persuadable window where their intelligence is comparable to that of their parents, an AI might, like Prometheus, blow through this window in a matter of days or hours. 
Some researchers are pursuing an alternative approach to making machines adopt our goals, which goes by the buzzword corrigibility. The hope is that one can give a primitive AI a goal system such that it simply doesn’t care if you occasionally shut it down and alter its goals. If this proves possible, then you can safely let your AI get superintelligent, power it off, install your goals, try it out for a while and, whenever you’re unhappy with the results, just power it down and make more goal tweaks. 
But even if you build an AI that will both learn and adopt your goals, you still haven’t finished solving the goal-alignment problem: what if your AI’s goals evolve as it gets smarter? How are you going to guarantee that it retains your goals no matter how much recursive self-improvement it undergoes? Let’s explore an interesting argument for why goal retention is guaranteed automatically, and then see if we can poke holes in it. 
Although we can’t predict in detail what will happen after an intelligence explosion—which is why Vernor Vinge called it a “singularity”—the physicist and AI researcher Steve Omohundro argued in a seminal 2008 essay that we can nonetheless predict certain aspects of the superintelligent AI’s behavior almost independently of whatever ultimate goals it may have. 5 This argument was reviewed and further developed in Nick Bostrom’s book Superintelligence. We humans tend to prefer some particle arrangements over others; for example, we prefer our hometown arranged as it is over having its particles rearranged by a hydrogen bomb explosion. So suppose we try to define a goodness function that associates a number with every possible arrangement of the particles in our Universe, quantifying how “good” we think this arrangement is, and then give a superintelligent AI the goal of maximizing this function. This may sound like a reasonable approach, since describing goal-oriented behavior as function maximization is popular in other areas of science: for example, economists often model people as trying to maximize what they call a “utility function,” and many AI designers train their intelligent agents to maximize what they call a “reward function.” When we’re taking about the ultimate goals for our cosmos, however, this approach poses a computational nightmare, since it would need to define a goodness value for every one of more than a googolplex possible arrangements of the elementary particles in our Universe, where a googolplex is 1 followed by 10100 zeroes—more zeroes than there are particles in our Universe. How would we define this goodness function to the AI? 
As we’ve explored above, the only reason that we humans have any preferences at all may be that we’re the solution to an evolutionary optimization problem. Thus all normative words in our human language, such as “delicious,” “fragrant,” “beautiful,” “comfortable,” “interesting,” “sexy,” “meaningful,” “happy” and “good,” trace their origin to this evolutionary optimization: there is therefore no guarantee that a superintelligent AI would find them rigorously definable. Even if the AI learned to accurately predict the preferences of some representative human, it wouldn’t be able to compute the goodness function for most particle arrangements: the vast majority of possible particle arrangements correspond to strange cosmic scenarios with no stars, planets or people whatsoever, with which humans have no experience, so who is to say how “good” they are? 
There are of course some functions of the cosmic particle arrangement that can be rigorously defined, and we even know of physical systems that evolve to maximize some of them. For example, we’ve already discussed how many systems evolve to maximize their entropy, which in the absence of gravity eventually leads to heat death, where everything is boringly uniform and unchanging. So entropy is hardly something we would want our AI to call “goodness” and strive to maximize. Here are a few examples of other quantities that one could strive to maximize and that may be rigorously definable in terms of particle arrangements: 
The fraction of all the matter in our Universe that’s in the form of a particular organism, say humans or E. coli (inspired by evolutionary inclusive-fitness maximization) 
The ability of an AI to predict the future, which AI researcher Marcus Hutter argues is a good measure of its intelligence 
What AI researchers Alex Wissner-Gross and Cameron Freer term causal entropy (a proxy for future opportunities), which they argue is the hallmark of intelligence 
The computational capacity of our Universe 
The algorithmic complexity of our Universe (how many bits are needed to describe it) 
The amount of consciousness in our Universe (see next chapter) 
However, when one starts with a physics perspective, where our cosmos consists of elementary particles in motion, it’s hard to see how one rather than another interpretation of “goodness” would naturally stand out as special. We have yet to identify any final goal for our Universe that appears both definable and desirable
This means that to wisely decide what to do about AI development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy. To program a self-driving car, we need to solve the trolley problem of whom to hit during an accident. To program a friendly AI, we need to capture the meaning of life. What’s “meaning”? What’s “life”? What’s the ultimate ethical imperative? In other words, how should we strive to shape the future of our Universe? If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us. This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation! 
Although thinkers have pondered the mystery of consciousness for thousands of years, the rise of AI adds a sudden urgency, in particular to the question of predicting which intelligent entities have subjective experiences. As we saw in chapter 3, the question of whether intelligent machines should be granted some form of rights depends crucially on whether they’re conscious and can suffer or feel joy. As we discussed in chapter 7, it becomes hopeless to formulate utilitarian ethics based on maximizing positive experiences without knowing which intelligent entities are capable of having them. As mentioned in chapter 5, some people might prefer their robots to be unconscious to avoid feeling slaveowner guilt. On the other hand, they may desire the opposite if they upload their minds to break free from biological limitations: after all, what’s the point of uploading yourself into a robot that talks and acts like you if it’s a mere unconscious zombie, by which I mean that being the uploaded you doesn’t feel like anything? Isn’t this equivalent to committing suicide from your subjective point of view, even though your friends may not realize that your subjective experience has died? 
For the long-term cosmic future of life (chapter 6), understanding what’s conscious and what’s not becomes pivotal: if technology enables intelligent life to flourish throughout our Universe for billions of years, how can we be sure that this life is conscious and able to appreciate what’s happening?
So what precisely is it that we don't understand about consciousness? Few have thought harder about this question than David Chalmers, a famous Australian philosopher rarely seen without a playful smile and a black leather jacket— which my wife liked so much that she gave me a similar one for Christmas. He followed his heart into philosophy despite making the finals at the International Mathematics Olympiad — and despite the fact that his only B grade in college, shattering his otherwise straight As, was for an introductory philosophy course. Indeed, he seems utterly undeterred by put-downs or controversy, and I've been astonished by his ability to politely listen to uninformed and misguided criticism of his own work without even feeling the need to respond. 
As David has emphasized, there are really two separate mysteries of the mind. First, there's the mystery of how a brain processes information, which David calls the " easy " problems. For example, how does a brain attend to, interpret and respond to sensory input? How can it report on its internal state using language? Although these questions are actually extremely difficult, they're by our definitions not mysteries of consciousness, but mysteries of intelligence: they ask how a brain remembers, computes and learns. Moreover, we saw in the first part of the book how AI researchers have started to make serious progress on solving many of these “ easy problems ” with machines — from playing Go to driving cars, analyzing images and processing natural language. 
Then there's the separate mystery of why you have a subjective experience, which David calls the hard problem. When you're driving, you're experiencing colors, sounds, emotions, and a feeling of self. But why are you experiencing anything at all? Does a self-driving car experience anything at all? If you're racing against a self- driving car, you're both inputting information from sensors, processing it and outputting motor commands. But subjectively experiencing driving is something logically separate — is it optional, and if so, what causes it? 
What I like about this physics perspective is that it transforms the hard problem that we as humans have struggled with for millennia into a more focused version that’s easier to tackle with the methods of science. Instead of starting with a hard problem of why an arrangement of particles can feel conscious, let’s start with a hard fact that some arrangements of particles do feel conscious while others don’t. For example, you know that the particles that make up your brain are in a conscious arrangement right now, but not when you’re in deep dreamless sleep. 
This physics perspective leads to three separate hard questions about consciousness, as shown in figure 8.1. First of all, what properties of the particle 
arrangement make the difference? Specifically, what physical properties distinguish conscious and unconscious systems? If we can answer that, then we can figure out which AI systems are conscious. In the more immediate future, it can also help emergency-room doctors determine which unresponsive patients are conscious. 
Second, how do physical properties determine what the experience is like? Specifically, what determines qualia, basic building blocks of consciousness such as the redness of a rose, the sound of a cymbal, the smell of a steak, the taste of a tangerine or the pain of a pinprick. When people tell me that consciousness research is a hopeless waste of time, the main argument they give is that it’s “unscientific” and always will be. But is that really true? The influential Austro-British philosopher Karl Popper popularized the now widely accepted adage “If it’s not falsifiable, it’s not scientific.” In other words, science is all about testing theories against observations: if a theory can’t be tested even in principle, then it’s logically impossible to ever falsify it, which by Popper’s definition means that it’s unscientific. 
So could there be a scientific theory that answers any of the three consciousness questions in figure 8.1? Please let me try to persuade you that the answer is a resounding YES!, at least for the pretty hard problem: “What physical properties distinguish conscious and unconscious systems?” Suppose that someone has a theory that, given any physical system, answers the question of whether the system is conscious with “yes,” “no” or “unsure.” Let’s hook your brain up to a device that measures some of the information processing in different parts of your brain, and let’s feed this information into a computer program that uses the consciousness theory to predict which parts of that information are conscious, and presents you with its predictions in real time on a screen, as in figure 8.2. First you think of an apple. The screen informs you that there’s information about an apple in your brain which you’re aware of, but that there’s also information in your brainstem about your pulse that you’re unaware of. Would you be impressed? Although the first two predictions of the theory were correct, you decide to do some more rigorous testing. You think about your mother and the computer informs you that there’s information in your brain about your mother but that you’re unaware of this. The theory made an incorrect prediction, which means that it’s ruled out and goes in the garbage dump of scientific history together with Aristotelian mechanics, the luminiferous aether, geocentric cosmology and countless other failed ideas. Here’s the key point: Although the theory was wrong, it was scientific! Had it not been scientific, you wouldn’t have been able to test it and rule it out. 
On the other hand, if the theory refuses to make any predictions, merely replying “unsure” whenever queried, then it’s untestable and hence unscientific. This might happen because it’s applicable only in some situations, because the required computations are too hard to carry out in practice or because the brain sensors are no good. Today’s most popular scientific theories tend to be somewhere in the middle, giving testable answers to some but not all of our questions. For example, our core theory of physics will refuse to answer questions about systems that are simultaneously extremely small (requiring quantum mechanics) and extremely heavy (requiring general relativity), because we haven’t yet figured out which mathematical equations to use in this case. This core theory will also refuse to predict the exact masses of all possible atoms —in this case, we think we have the necessary equations, but we haven’t managed to accurately compute their solutions. The more dangerously a theory lives by sticking its neck out and making testable predictions, the more useful it is, and the more seriously we take it if it survives all our attempts to kill it.
In summary, any theory predicting which physical systems are conscious (the pretty hard problem) is scientific, as long as it can predict which of your brain processes are conscious. However, the testability issue becomes less clear for the higher-up questions in figure 8.1. What would it mean for a theory to predict how you subjectively experience the color red? And if a theory purports to explain why there is such a thing as consciousness in the first place, then how do you test it experimentally? Just because these questions are hard doesn’t mean that we should avoid them, and we’ll indeed return to them below. But when confronted with several related unanswered questions, I think it’s wise to tackle the easiest one first. For this reason, my consciousness research at MIT is focused squarely on the base of the pyramid in figure 8.1. I recently discussed this strategy with my fellow physicist Piet Hut from Princeton, who joked that trying to build the top of the pyramid before the base would be like worrying about the interpretation of quantum mechanics before discovering the Schrödinger equation, the mathematical foundation that lets us predict the outcomes of our experiments. 
When discussing what’s beyond science, it’s important to remember that the answer depends on time! Four centuries ago, Galileo Galilei was so impressed by math-based physics theories that he described nature as “a book written in the language of mathematics.” If he threw a grape and a hazelnut, he could accurately predict the shapes of their trajectories and when they would hit the ground. Yet he had no clue why one was green and the other brown, or why one was soft and the other hard—these aspects of the world were beyond the reach of science at the time. But not forever! When James Clerk Maxwell discovered his eponymous equations in 1861, it became clear that light and colors could also be understood mathematically. We now know that the aforementioned Schrödinger equation, discovered in 1925, can be used to predict all properties of matter, including what’s soft or hard. While theoretical progress has enabled ever more scientific predictions, technological progress has enabled ever more experimental tests: almost everything we now study with telescopes, microscopes or particle colliders was once beyond science. In other words, the purview of science has expanded dramatically since Galileo’s days, from a tiny fraction of all phenomena to a large percentage, including subatomic particles, black holes and our cosmic origins 13.8 billion years ago. This raises the question: What’s left? 
If you multiply 32 by 17 in your head, you’re conscious of many of the inner workings of your computation. But suppose I instead show you a portrait of Albert Einstein and tell you to say the name of its subject. As we saw in chapter 2, this too is a computational task: your brain is evaluating a function whose input is information from your eyes about a large number of pixel colors and whose output is information to muscles controlling your mouth and vocal cords. Computer scientists call this task “image classification” followed by “speech synthesis.” Although this computation is way more complicated than your multiplication task, you can do it much faster, seemingly without effort, and without being conscious of the details of how you do it. Your subjective experience consists merely of looking at the picture, experiencing a feeling of recognition and hearing yourself say “Einstein.” 
Psychologists have long known that you can unconsciously perform a wide range of other tasks and behaviors as well, from blink reflexes to breathing, reaching, grabbing and keeping your balance. Typically, you’re consciously aware of what you did, but not how you did it. On the other hand, behaviors that involve unfamiliar situations, self-control, complicated logical rules, abstract reasoning or manipulation of language tend to be conscious. They’re known as behavioral correlates of consciousness, and they’re closely linked to the effortful, slow and controlled way of thinking that psychologists call “System 2.”5 
It’s also known that you can convert many routines from conscious to unconscious through extensive practice, for example walking, swimming, bicycling, driving, typing, shaving, shoe tying, computer-gaming and piano playing. 6 Indeed, it’s well known that experts do their specialties best when they’re in a state of “flow,” aware only of what’s happening at a higher level, and unconscious of the low-level details of how they’re doing it. For example, try reading the next sentence while being consciously aware of every single letter, as when you first learned to read. Can you feel how much slower it is, compared to when you’re merely conscious of the text at the level of words or ideas? 
Clever experiments and analyses have suggested that consciousness is limited not merely to certain behaviors, but also to certain parts of the brain. Which are the prime suspects? Many of the first clues came from patients with brain lesions: localized brain damage caused by accidents, strokes, tumors or infections. But this was often inconclusive. For example, does the fact that lesions in the back of the brain can cause blindness mean that this is the site of visual consciousness, or does it merely mean that visual information passes through there en route to wherever it will later become conscious, just as it first passes through the eyes? 
Although lesions and medical interventions haven’t pinpointed the locations of conscious experiences, they’ve helped narrow down the options. For example, I know that although I experience pain in my hand as actually occurring there, the pain experience must occur elsewhere, because a surgeon once switched off my hand pain without doing anything to my hand: he merely anesthetized nerves in my shoulder. Moreover, some amputees experience phantom pain that feels as though it’s in their nonexistent hand. As another example, I once noticed that when I looked only with my right eye, part of my visual field was missing—a doctor determined that my retina was coming loose and reattached it. In contrast, patients with certain brain lesions experience hemineglect, where they too miss information from half their visual field, but aren’t even aware that it’s missing— for example, failing to notice and eat the food on the left half of their plate. It’s as if consciousness about half of their world has disappeared. But are those damaged brain areas supposed to generate the spatial experience, or were they merely feeding spatial information to the sites of consciousness, just as my retina did? 
The pioneering U.S.-Canadian neurosurgeon Wilder Penfield found in the 1930s that his neurosurgery patients reported different parts of their body being touched when he electrically stimulated specific brain areas in what’s now called the somatosensory cortex (figure 8.3). 9 He also found that they involuntarily moved different parts of their body when he stimulated brain areas in what’s now called the motor cortex. But does that mean that information processing in these brain areas corresponds to consciousness of touch and motion? 
Although we’re still nowhere near being able to measure every single firing of all of your roughly hundred billion neurons, brain-reading technology is advancing rapidly, involving techniques with intimidating names such as fMRI, EEG, MEG, ECoG, ePhys and fluorescent voltage sensing. fMRI, which stands for functional magnetic resonance imaging, measures the magnetic properties of hydrogen nuclei to make a 3-D map of your brain roughly every second, with millimeter resolution. EEG (electroencephalography) and MEG (magnetoencephalography) measure the electric and magnetic field outside your head to map your brain thousands of times per second, but with poor resolution, unable to distinguish features smaller than a few centimeters. If you’re squeamish, you’ll appreciate that these three techniques are all noninvasive. If you don’t mind opening up your skull, you have additional options. ECoG (electrocorticography) involves placing say a hundred wires on the surface of your brain, while ePhys (electrophysiology) involves inserting microwires, which are sometimes thinner than a human hair, deep into the brain to record voltages from as many as a thousand simultaneous locations. Many epileptic patients spend days in the hospital while ECoG is used to figure out what part of their brain is triggering seizures and should be resected, and kindly agree to let neuroscientists perform consciousness experiments on them in the meantime. Finally, fluorescent voltage sensing involves genetically manipulating neurons to emit flashes of light when firing, enabling their activity to be measured with a microscope. Out of all the techniques, it has the potential to rapidly monitor the largest number of neurons, at least in animals with transparent brains—such as the C. elegans worm with its 302 neurons and the larval zebrafish with its about 100,000. 
Although Francis Crick warned Christof Koch about studying consciousness, Christof refused to give up and and eventually won Francis over. In 1990, they wrote a seminal paper about what they called “neural correlates of consciousness” (NCCs), asking which specific brain processes corresponded to conscious experiences. For thousands of years, thinkers had had access to the information processing in their brains only via their subjective experience and 
behavior. Crick and Koch pointed out that brain-reading technology was suddenly providing independent access to this information, allowing scientific study of which information processing corresponded to what conscious experience. Sure enough, technology-driven measurements have by now turned the quest for NCCs into quite a mainstream part of neuroscience, one whose thousands of publications extend into even the most prestigious journals. 10 
What are the conclusions so far? To get a flavor for NCC detective work, let’s first ask whether your retina is conscious, or whether it’s merely a zombie system that records visual information, processes it and sends it on to a system downstream in your brain where your subjective visual experience occurs. In the left panel of figure 8.4, which square is darker: the one labeled A or B? A, right? No, they’re in fact identically colored, which you can verify by looking at them through small holes between your fingers. This proves that your visual experience can’t reside entirely in your retina, since if it did, they’d look the same. 
Now look at the right panel of figure 8.4. Do you see two women or a vase? If you look long enough, you’ll subjectively experience both in succession, even though the information reaching your retina remains the same. By measuring what happens in your brain during the two situations, one can tease apart what makes the difference—and it’s not the retina, which behaves identically in both cases. 
The death blow to the conscious-retina hypothesis comes from a technique called “continuous flash suppression” pioneered by Christof Koch, Stanislas Dehaene and collaborators: it’s been discovered that if you make one of your eyes watch a complicated sequence of rapidly changing patterns, then this will distract your visual system to such an extent that you’ll be completely unaware of a still image shown to the other eye. 11 In summary, you can have a visual image in your retina without experiencing it, and you can (while dreaming) experience an image without it being on your retina. This proves that your two retinas don’t host your visual consciousness any more than a video camera does, even though they perform complicated computations involving over a hundred million neurons. 
NCC researchers also use continuous flash suppression, unstable visual/auditory illusions and other tricks to pinpoint which of your brain regions are responsible for each of your conscious experiences. The basic strategy is to compare what your neurons are doing in two situations where essentially everything (including your sensory input) is the same—except your conscious experience. The parts of your brain that are measured to behave differently are then identified as NCCs. 
Such NCC research has proven that none of your consciousness resides in your gut, even though that’s the location of your enteric nervous system with its whopping half-billion neurons that compute how to optimally digest your food; feelings such as hunger and nausea are instead produced in your brain. Similarly, none of your consciousness appears to reside in the brainstem, the bottom part of the brain that connects to the spinal cord and controls breathing, heart rate and blood pressure. More shockingly, your consciousness doesn’t appear to extend to your cerebellum (figure 8.3), which contains about two-thirds of all your neurons: patients whose cerebellum is destroyed experience slurred speech and clumsy motion reminiscent of a drunkard, but remain fully conscious. 
So far, we’ve looked at experimental clues regarding what types of information processing are conscious and where consciousness occurs. But when does it occur? When I was a kid, I used to think that we become conscious of events as they happen, with absolutely no time lag or delay. Although that’s still how it subjectively feels to me, it clearly can’t be correct, since it takes time for my brain to process the information that enters via my sensory organs. NCC researchers have carefully measured how long, and Christof Koch’s summary is that it takes about a quarter of a second from when light enters your eye from a complex object until you consciously perceive seeing it as what it is. 13 This means that if you’re driving down a highway at fifty-five miles per hour and suddenly see a squirrel a few meters in front of you, it’s too late for you to do anything about it, because you’ve already run over it! 
In summary, your consiousness lives in the past, with Christof Koch estimating that it lags behind the outside world by about a quarter second. Intriguingly, you can often react to things faster than you can become conscious of them, which proves that the information processing in charge of your most rapid reactions must be unconscious. For example, if a foreign object approaches your eye, your blink reflex can close your eyelid within a mere tenth of a second. It’s as if one of your brain systems receives ominous information from the visual system, computes that your eye is in danger of getting struck, emails your eye muscles instructions to blink and simultaneously emails the conscious part of your brain saying “Hey, we’re going to blink.” By the time this email has been read and included into your conscious experience, the blink has already happened. 
Indeed, the system that reads that email is continually bombarded with messages from all over your body, some more delayed than others. It takes longer for nerve signals to reach your brain from your fingers than from your face because of distance, and it takes longer for you to analyze images than sounds because it’s more complicated—which is why Olympic races are started with a bang rather than with a visual cue. Yet if you touch your nose, you consciously experience the sensation on your nose and fingertip as simultaneous, and if you clap your hands, you see, hear and feel the clap at exactly the same time. 14 This means that your full conscious experience of an event isn’t created 
until the last slowpoke email reports have trickled in and been analyzed. 
A famous family of NCC experiments pioneered by physiologist Benjamin Libet has shown that the sort of actions you can perform unconsciously aren’t limited to rapid responses such as blinks and ping-pong smashes, but also include certain decisions that you might attribute to free will—brain measurements can sometimes predict your decision before you become conscious of having made it. To appreciate why, let’s compare theories of consciousness with theories of gravity. Scientists started taking Newton’s theory of gravity seriously because they got more out of it than they put into it: simple equations that fit on a napkin could accurately predict the outcome of every gravity experiment ever conducted. They therefore also took seriously its predictions far beyond the domain where it had been tested, and these bold extrapolations turned out to work even for the motions of galaxies in clusters millions of light-years across. However, the predictions were off by a tiny amount for the motion of Mercury around the Sun. Scientists then started taking seriously Einstein’s improved theory of gravity, general relativity, because it was arguably even more elegant and economical, and correctly predicted even what Newton’s theory got wrong. They consequently took seriously also its predictions far beyond the domain where it had been tested, for phenomena as exotic as black holes, gravitational waves in the very fabric of spacetime, and the expansion of our Universe from a hot fiery origin—all of which were subsequently confirmed by experiment. 
Analogously, if a mathematical theory of consciousness whose equations fit on a napkin could successfully predict the outcomes of all experiments we perform on brains, then we’d start taking seriously not merely the theory itself, but also its predictions for consciousness beyond brains—for example, in machines. 
Although some theories of consciousness date back to antiquity, most modern ones are grounded in neuropsychology and neuroscience, attempting to explain and predict consciousness in terms of neural events occurring in the brain. 16 Although these theories have made some successful predictions for neural correlates of consciousness, they neither can nor aspire to make predictions about machine consciousness. To make the leap from brains to machines, we need to generalize from NCCs to PCCs: physical correlates of consciousness, defined as the patterns of moving particles that are conscious. Because if a theory can correctly predict what’s conscious and what’s not by referring only to physical building blocks such as elementary particles and force fields, then it can make predictions not merely for brains, but also for any other arrangements of matter, including future AI systems. So let’s take a physics perspective: What particle arrangements are conscious? 
But this really raises another question: How can something as complex as consciousness be made of something as simple as particles? I think it’s because it’s a phenomenon that has properties above and beyond those of its particles. In physics, we call such phenomena “emergent.”17 Let’s understand this by looking at an emergent phenomenon that’s simpler than consciousness: wetness. 
A drop of water is wet, but an ice crystal and a cloud of steam aren’t, even though they’re made of identical water molecules. Why? Because the property of wetness depends only on the arrangement of the molecules. It makes absolutely no sense to say that a single water molecule is wet, because the phenomenon of wetness emerges only when there are many molecules, arranged in the pattern we call liquid. So solids, liquids and gases are all emergent phenomena: they’re more than the sum of their parts, because they have properties above and beyond the properties of their particles. They have properties that their particles lack. 
Now just like solids, liquids and gases, I think consciousness is an emergent phenomenon, with properties above and beyond those of its particles. For example, entering deep sleep extinguishes consciousness, by merely rearranging the particles. In the same way, my consciousness would disappear if I froze to death, which would rearrange my particles in a more unfortunate way. 
When you put lots of particles together to make anything from water to a 
brain, new phenomena with observable properties emerge. We physicists love studying these emergent properties, which can often be identified by a small set of numbers that you can go out and measure—quantities such as how viscous the substance is, how compressible it is and so on. For example, if a substance is so viscous that it’s rigid, we call it a solid, otherwise we call it a fluid. And if a fluid isn’t compressible, we call it a liquid, otherwise we call it a gas or a plasma, depending on how well it conducts electricity. 
I first met Giulio at a 2014 physics conference in Puerto Rico to which I’d invited him and Christof Koch, and he struck me as the ultimate renaissance man who’d have blended right in with Galileo and Leonardo da Vinci. His quiet demeanor couldn’t hide his incredible knowledge of art, literature and philosophy, and his culinary reputation preceded him: a cosmopolitan TV journalist had recently told me how Giulio had, in just a few minutes, whipped up the most delicious salad he’d tasted in his life. I soon realized that behind his soft-spoken demeanor was a fearless intellect who’d follow the evidence wherever it took him, regardless of the preconceptions and taboos of the establishment. Just as Galileo had pursued his mathematical theory of motion despite establishment pressure not to challenge geocentrism, Giulio had developed the most mathematically precise consciousness theory to date, integrated information theory (IIT). 
I’d been arguing for decades that consciousness is the way information feels when being processed in certain complex ways. 18 IIT agrees with this and replaces my vague phrase “certain complex ways” by a precise definition: the information processing needs to be integrated, that is, Φ needs to be large. Giulio’s argument for this is as powerful as it is simple.
the conscious system needs to be integrated into a unified whole, because if it instead consisted of two independent parts, then they’d feel like two separate conscious entities rather than one. In other words, if a conscious part of a brain or computer can’t communicate with the rest, then the rest can’t be part of its subjective experience. 
Giulio and his collaborators have measured a simplified version of Φ by using EEG to measure the brain’s response to magnetic stimulation. Their “consciousness detector” works really well: it determined that patients were conscious when they were awake or dreaming, but unconscious when they were anesthetized or in deep sleep. It even discovered consciousness in two patients suffering from “locked-in” syndrome, who couldn’t move or communicate in any normal way. 19 So this is emerging as a promising technology for doctors in the future to figure out whether certain patients are conscious or not.
IIT is defined only for discrete systems that can be in a finite number of states, for example bits in a computer memory or oversimplified neurons that can be either on or off. This unfortunately means that IIT isn’t defined for most traditional physical systems, which can change continuously—for example, the position of a particle or the strength of a magnetic field can take any of an infinite number of values. 20 If you try to apply the IIT formula to such systems, you’ll typically get the unhelpful result that Φ is infinite. Quantum-mechanical systems can be discrete, but the original IIT isn’t defined for quantum systems. So how can we anchor IIT and other information-based consciousness theories on a solid physical foundation? 
We can do this by building on what we learned in chapter 2 about how clumps of matter can have emergent properties that are related to information. We saw that for something to be usable as a memory device that can store information, it needs to have many long-lived states. We also saw that being computronium, a substance that can do computations, in addition requires complex dynamics: the laws of physics need to make it change in ways that are complicated enough to be able to implement arbitrary information processing. Finally, we saw how a neural network, for example, is a powerful substrate for learning because, simply by obeying the laws of physics, it can rearrange itself to get better and better at implementing desired computations. Now we’re asking an additional question: What makes a blob of matter able to have a subjective experience? In other words, under what conditions will a blob of matter be able to do these four things? 
But how can consciousness feel so non-physical if it’s in fact a physical phenomenon? How can it feel so independent of its physical substrate? I think it’s because it is rather independent of its physical substrate, the stuff in which it is a pattern! We encountered many beautiful examples of substrate-independent patterns in chapter 2, including waves, memories and computations. We saw how they weren’t merely more than their parts (emergent), but rather independent of their parts, taking on a life of their own. For example, we saw how a future simulated mind or computer-game character would have no way of knowing whether it ran on Windows, Mac OS, an Android phone or some other operating system, because it would be substrate-independent. Nor could it tell whether the logic gates of its computer were made of transistors, optical circuits or other hardware. Or what the fundamental laws of physics are—they could be anything as long as they allow the construction of universal computers. 
In summary, I think that consciousness is a physical phenomenon that feels non-physical because it’s like waves and computations: it has properties independent of its specific physical substrate. This follows logically from the consciousness-as-information idea. This leads to a radical idea that I really like: If consciousness is the way that information feels when it’s processed in certain ways, then it must be substrate-independent; it’s only the structure of the information processing that matters, not the structure of the matter doing the information processing. In other words, consciousness is substrate-independent twice over! 
As I said, I think that consciousness is the way information feels when being processed in certain ways. This means that to be conscious, a system needs to be able to store and process information, implying the first two principles. Note that the memory doesn’t need to last long: I recommend watching this touching video of Clive Wearing, who appears perfectly conscious even though his memories last less than a minute. 21 I think that a conscious system also needs to be fairly independent from the rest of the world, because otherwise it wouldn’t subjectively feel that it had any independent existence whatsoever. Finally, I think that the conscious system needs to be integrated into a unified whole, as Giulio Tononi argued, because if it consisted of two independent parts, then they would feel like two separate conscious entities, rather than one. The first three principles imply autonomy: that the system is able to retain and process information without much outside interference, hence determining its own future. All four principles together mean that a system is autonomous but its parts aren’t. 
If these four principles are correct, then we have our work cut out for us: we need to look for mathematically rigorous theories that embody them and test them experimentally. We also need to determine whether additional principles are needed. Regardless of whether IIT is correct or not, researchers should try to develop competing theories and test all available theories with ever better experiments. 
We’ve already discussed the perennial controversy about whether consciousness research is unscientific nonsense and a pointless waste of time. In addition, there are recent controversies at the cutting edge of consciousness research—let’s explore the ones that I find most enlightening. 
Giulio Tononi’s IIT has lately drawn not merely praise but also criticism, some of which has been scathing. Scott Aaronson recently had this to say on his blog: “In my opinion, the fact that Integrated Information Theory is wrong— demonstrably wrong, for reasons that go to its core—puts it in something like the top 2% of all mathematical theories of consciousness ever proposed. Almost all competing theories of consciousness, it seems to me, have been so vague, fluffy and malleable that they can only aspire to wrongness.”22 To the credit of both Scott and Giulio, they never came to blows when I watched them debate IIT at a recent New York University workshop, and they politely listened to each other’s arguments. Aaronson showed that certain simple networks of logic gates had extremely high integrated information (Φ) and argued that since they clearly weren’t conscious, IIT was wrong. Giulio countered that if they were built, they would be conscious, and that Scott’s assumption to the contrary was anthropocentrically biased, much as if a slaughterhouse owner claimed that animals couldn’t be conscious just because they couldn’t talk and were very different from humans. My analysis, with which they both agreed, was that they were at odds about whether integration was merely a necessary condition for consciousness (which Scott was OK with) or also a sufficient condition (which Giulio claimed). The latter is clearly a stronger and more contentious claim, which I hope we can soon test experimentally.
This claim has been challenged by both David Chalmers and AI professor Murray Shanahan by imagining what would happen if you instead gradually replaced the neural circuits in your brain by hypothetical digital hardware perfectly simulating them. 25 Although your behavior would be unaffected by the replacement since the simulation is by assumption perfect, your experience would change from conscious initially to unconscious at the end, according to Giulio. But how would it feel in between, as ever more got replaced? When the parts of your brain responsible for your conscious experience of the upper half of your visual field were replaced, would you notice that part of your visual scenery was suddenly missing, but that you mysteriously knew what was there nonetheless, as reported by patients with “blindsight”?26 This would be deeply troubling, because if you can consciously experience any difference, then you can also tell your friends about it when asked—yet by assumption, your behavior can’t change. The only logical possibility compatible with the assumptions is that at exactly the same instance that any one thing disappears from your consciousness, your mind is mysteriously altered so as either to make you lie and deny that your experience changed, or to forget that things had been different. 
On the other hand, Murray Shanahan admits that the same gradualreplacement critique can be leveled at any theory claiming that you can act conscious without being conscious, so you might be tempted to conclude that acting and being conscious are one and the same, and that externally observable behavior is therefore all that matters. But then you’d have fallen into the trap of predicting that you’re unconscious while dreaming, even though you know better. 
Imagine using future technology to build a direct communication link between two human brains, and gradually increasing the capacity of this link until communication is as efficient between the brains as it is within them. Would there come a moment when the two individual consciousnesses suddenly disappear and get replaced by a single unified one as IIT predicts, or would the transition be gradual so that the individual consciousnesses coexisted in some form even as a joint experience began to emerge? 
Another fascinating controversy is whether experiments underestimate how much we’re conscious of. We saw earlier that although we feel we’re visually conscious of vast amounts of information involving colors, shapes, objects and seemingly everything that’s in front of us, experiments have shown that we can only remember and report a dismally small fraction of this. 27 Some researchers have tried to resolve this discrepancy by asking whether we may sometimes have “consciousness without access,” that is, subjective experience of things that are too complex to fit into our working memory for later use. 28 For example, when you experience inattentional blindness by being too distracted to notice an object in plain sight, this doesn’t imply that you had no conscious visual experience of it, merely that it wasn’t stored in your working memory. 29 Should it count as forgetfulness rather than blindness? Other researchers reject this idea that people can’t be trusted about what they say they experienced, and warn of its implications. Murray Shanahan imagines a clinical trial where patients report complete pain relief thanks to a new wonder drug, which nonetheless gets rejected by a government panel: “The patients only think they are not in pain. Thanks to neuroscience, we know better.”30 On the other hand, there have been cases where patients who accidentally awoke during surgery were given a drug to make them forget the ordeal. Should we trust their subsequent report that they experienced no pain?
If some future AI system is conscious, then what will it subjectively experience? This is the essence of the “even harder problem” of consciousness, and forces us up to the second level of difficulty depicted in figure 8.1. Not only do we currently lack a theory that answers this question, but we’re not even sure whether it’s logically possible to fully answer it. After all, what could a satisfactory answer sound like? How would you explain to a person born blind what the color red looks like? 
Fortunately, our current inability to give a complete answer doesn’t prevent us from giving partial answers. Intelligent aliens studying the human sensory system would probably infer that colors are qualia that feel associated with each point on a two-dimensional surface (our visual field), while sounds don’t feel as spatially localized, and pains are qualia that feel associated with different parts of our body. From discovering that our retinas have three types of light-sensitive cone cells, they could infer that we experience three primary colors and that all other color qualia result from combining them. By measuring how long it takes neurons to transmit information across the brain, they could conclude that we experience no more than about ten conscious thoughts or perceptions per second, and that when we watch movies on our TV at twenty-four frames per second, we experience this not as a sequence of still images, but as continuous motion. From measuring how fast adrenaline is released into our bloodstream and how long it remains before being broken down, they could predict that we feel bursts of anger starting within seconds and lasting for minutes. 
Applying similar physics-based arguments, we can make some educated guesses about certain aspects of how an artificial consciousness may feel. First of all, the space of possible AI experiences is huge compared to what we humans can experience. We have one class of qualia for each of our senses, but AIs can have vastly more types of sensors and internal representations of information, so we must avoid the pitfall of assuming that being an AI necessarily feels similar to being a person. 
We’d therefore expect an Earthsized “Gaia” AI to have only about ten conscious experiences per second, like a human, and a galaxy-sized AI could have only one global thought every 100,000 years or so—so no more than about a hundred experiences during the entire history of our Universe thus far! This would give large AIs a seemingly irresistible incentive to delegate computations to the smallest subsystems capable of handling them, to speed things up, much like our conscious mind has delegated the blink reflex to a small, fast and unconscious subsystem. Although we saw above that the conscious information processing in our brains appears to be merely the tip of an otherwise unconscious iceberg, we should expect the situation to be even more extreme for large future AIs: if they have a single consciousness, then it’s likely to be unaware of almost all the information processing taking place within it. Moreover, although the conscious experiences that it enjoys may be extremely complex, they’re also snail-paced compared to the rapid activities of its smaller parts. 
This really brings to a head the aforementioned controversy about whether parts of a conscious entity can be conscious too. IIT predicts not, which means that if a future astronomically large AI is conscious, then almost all its information processing is unconscious. This would mean that if a civilization of smaller AIs improves its communication abilities to the point that a single conscious hive mind emerges, their much faster individual consciousnesses are suddenly extinguished. If the IIT prediction is wrong, on the other hand, the hive mind can coexist with the panoply of smaller conscious minds. Indeed, one could even imagine a nested hierarchy of consciousnesses at all levels from microscopic to cosmic. 
IIT explains this by saying that raw sensory information in System 0 is stored in grid-like brain structures with very high integration, while System 2 has high integration because of feedback loops, where all the information you’re aware of right now can affect your future brain states. On the other hand, it was precisely the conscious-grid prediction that triggered Scott Aaronson’s aforementioned IITcritique. In summary, if a theory solving the pretty hard problem of consciousness can one day pass a rigorous battery of experimental tests so that we start taking its predictions seriously, then it will also greatly narrow down the options for the even harder problem of what future conscious AIs may experience. 
Some aspects of our subjective experience clearly trace back to our evolutionary origins, for example our emotional desires related to selfpreservation (eating, drinking, avoiding getting killed) and reproduction. This means that it should be possible to create AI that never experiences qualia such as hunger, thirst, fear or sexual desire. As we saw in the last chapter, if a highly intelligent AI is programmed to have virtually any sufficiently ambitious goal, it’s likely to strive for self-preservation in order to be able to accomplish that goal. If they’re part of a society of AIs, however, they might lack our strong human fear of death: as long as they’ve backed themselves up, all they stand to lose are the memories they’ve accumulated since their most recent backup, as long as they’re confident that their backed-up software will be used. In addition, the ability to readily copy information and software between AIs would probably reduce the strong sense of individuality that’s so characteristic of our human consciousness: there would be less of a distinction between you and me if we could easily share and copy all our memories and abilities, so a group of nearby AIs may feel more like a single organism with a hive mind. 
