A low-tech way to build a partial Dyson sphere is to place a ring of habitats in circular orbit around the Sun. 
To completely surround the Sun, you could add rings orbiting it around different axes at slightly different distances, to avoid collisions. 
To avoid the nuisance that these fast-moving rings couldn’t be connected to one another, complicating transportation and communication, one could instead build a monolithic stationary Dyson sphere where the Sun’s inward gravitational pull is balanced by the outward pressure from the Sun’s radiation— an idea pioneered by Robert L. Forward and by Colin McInnes. 
software. They never learn to swim toward sugar; instead, that algorithm was hard-coded into their DNA from the start. There was of course a learning process of sorts, but it didn’t take place during the lifetime of that particular bacterium. Rather, it occurred during the preceding evolution of that species of bacteria, through a slow trial-and-error process spanning many generations, where natural selection favored those random DNA mutations that improved sugar consumption. Some of these mutations helped by improving the design of flagella and other hardware, while other mutations improved the bacterial information-processing system that implements the sugar-finding algorithm and other software. 
Such bacteria are an example of what I’ll call “Life 1.0”: life where both the hardware and software are evolved rather than designed. You and I, on the other hand, are examples of “Life 2.0”: life whose hardware is evolved, but whose software is largely designed. By your software, I mean all the algorithms and knowledge that you use to process the information from your senses and decide what to do—everything from the ability to recognize your friends when you see them to your ability to walk, read, write, calculate, sing and tell jokes. 
You weren’t able to perform any of those tasks when you were born, so all this software got programmed into your brain later through the process we call learning. Whereas your childhood curriculum is largely designed by your family and teachers, who decide what you should learn, you gradually gain more power to design your own software. Perhaps your school allows you to select a foreign language: Do you want to install a software module into your brain that enables you to speak French, or one that enables you to speak Spanish? Do you want to learn to play tennis or chess? Do you want to study to become a chef, a lawyer or a pharmacist? Do you want to learn more about artificial intelligence (AI) and the future of life by reading a book about it? 
This ability of Life 2.0 to design its software enables it to be much smarter than Life 1.0. DNA that I was born with. Your synapses store all your knowledge and skills as roughly 100 terabytes’ worth of information, while your DNA stores merely about a gigabyte, barely enough to store a single movie download. So it’s physically impossible for an infant to be born speaking perfect English and ready to ace her college entrance exams: there’s no way the information could have been preloaded into her brain, since the main information module she got from her parents (her DNA) lacks sufficient information-storage capacity. 
The ability to design its software enables Life 2.0 to be not only smarter than Life 1.0, but also more flexible. If the environment changes, 1.0 can only adapt by slowly evolving over many generations. Life 2.0, on the other hand, can adapt almost instantly, via a software update. For example, bacteria frequently encountering antibiotics may evolve drug resistance over many generations, but an individual bacterium won’t change its behavior at all; in contrast, a girl learning that she has a peanut allergy will immediately change her behavior to start avoiding peanuts. This flexibility gives Life 2.0 an even greater edge at the population level: even though the information in our human DNA hasn’t evolved dramatically over the past fifty thousand years, the information collectively stored in our brains, books and computers has exploded. By installing a software module enabling us to communicate through sophisticated spoken language, we ensured that the most useful information stored in one person’s brain could get copied to other brains, potentially surviving even after the original brain died. By installing a software module enabling us to read and write, we became able to store and share vastly more information than people could memorize. By developing brain software capable of producing technology (i.e., by studying science and engineering), we enabled much of the world’s information to be accessed by many of the world’s humans with just a few clicks. 
When I was a kid, I imagined that billionaires exuded pomposity and arrogance. When I first met Larry Page at Google in 2008, he totally shattered these stereotypes. Casually dressed in jeans and a remarkably ordinary-looking shirt, he would have blended right in at an MIT picnic. His thoughtful soft-spoken style and his friendly smile made me feel relaxed rather than intimidated talking with him. On July 18, 2015, we ran into each other at a party in Napa Valley thrown by Elon Musk and his then wife, Talulah, and got into a conversation about the scatological interests of our kids. I recommended the profound literary classic The Day My Butt Went Psycho, by Andy Griffiths, and Larry ordered it on the spot. I struggled to remind myself that he might go down in history as the most influential human ever to have lived: my guess is that if superintelligent digital life engulfs our Universe in my lifetime, it will be because of Larry’s decisions. 
This question is wonderfully controversial, with the world’s leading AI researchers disagreeing passionately not only in their forecasts, but also in their emotional reactions, which range from confident optimism to serious concern. They don’t even have consensus on short-term questions about AI’s economic, legal and military impact, and their disagreements grow when we expand the time horizon and ask about artificial general intelligence (AGI)—especially about AGI reaching human level and beyond, enabling Life 3.0. General intelligence can accomplish virtually any goal, including learning, in contrast to, say, the narrow intelligence of a chess-playing program. 
Interestingly, the controversy about Life 3.0 centers around not one but two separate questions: when and what? When (if ever) will it happen, and what will it mean for humanity? The way I see it, there are three distinct schools of thought that all need to be taken seriously, because they each include a number of world-leading experts. As illustrated in figure 1.2, I think of them as digital utopians, techno-skeptics and members of the beneficial-AI movement, respectively. Please let me introduce you to some of their most eloquent champions. 
This flexibility has enabled Life 2.0 to dominate Earth. Freed from its genetic shackles, humanity’s combined knowledge has kept growing at an accelerating pace as each breakthrough enabled the next: language, writing, the printing press, modern science, computers, the internet, etc. This ever-faster cultural evolution of our shared software has emerged as the dominant force shaping our human future, rendering our glacially slow biological evolution almost irrelevant. 
The sphere can be built by gradually adding more “statites”: stationary satellites that counteract the Sun’s gravity with radiation pressure rather than centrifugal forces. Wouldn’t it be tempting to escape the perils of technology without succumbing to stagnant totalitarianism? Let’s explore a scenario where this was accomplished by reverting to primitive technology, inspired by the Amish. After the Omegas took over the world as in the opening of the book, a massive global propaganda campaign was launched that romanticized the simple farming life of 1,500 years ago. Earth’s population was reduced to about 100 million people by an engineered pandemic blamed on terrorists. The pandemic was secretly targeted to ensure that nobody who knew anything about science or technology survived. With the excuse of eliminating the infection hazard of large concentrations of people, Prometheus-controlled robots emptied and razed all cities. Survivors were given large tracts of (suddenly available) land and educated in sustainable farming, fishing and hunting practices using only early medieval technology. In the meantime, armies of robots systematically removed all traces of modern technology (including cities, factories, power lines and paved roads), and thwarted all human attempts to document or re-create any such technology. Once the technology was globally forgotten, robots helped dismantle other robots until there were almost none left. The very last robots were deliberately vaporized together with Prometheus itself in a large thermonuclear explosion. There was no longer any need to ban modern technology, since it was all gone. As a result, humanity bought itself over a millennium of additional time without worries about either AI or totalitarianism. 
The first one regards the timeline from figure 1.2: how long will it take until machines greatly supersede human-level AGI? Here, a common misconception is that we know the answer with great certainty. 
One popular myth is that we know we’ll get superhuman AGI this century. In fact, history is full of technological over-hyping. Where are those fusion power plants and flying cars we were promised we’d have by now? AI too has been repeatedly over-hyped in the past, even by some of the founders of the field: for example, John McCarthy (who coined the term “artificial intelligence”), Marvin Minsky, Nathaniel Rochester and Claude Shannon wrote this overly optimistic forecast about what could be accomplished during two months with stone-age computers: “We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College…An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.” 
On the other hand, a popular counter-myth is that we know we won’t get superhuman AGI this century. Researchers have made a wide range of estimates for how far we are from superhuman AGI, but we certainly can’t say with great confidence that the probability is zero this century, given the dismal track record of such techno-skeptic predictions. For example, Ernest Rutherford, arguably the greatest nuclear physicist of his time, said in 1933—less than twenty-four hours before Leo Szilard’s invention of the nuclear chain reaction—that nuclear energy was “moonshine,” and in 1956 Astronomer Royal Richard Woolley called talk about space travel “utter bilge.” The most extreme form of this myth is that superhuman AGI will never arrive because it’s physically impossible. However, physicists know that a brain consists of quarks and electrons arranged to act as a powerful computer, and that there’s no law of physics preventing us from building even more intelligent quark blobs. 
Another common misconception is that the only people harboring concerns about AI and advocating AI-safety research are Luddites who don’t know much about AI. When Stuart Russell mentioned this during his Puerto Rico talk, the audience laughed loudly. A related misconception is that supporting AI-safety research is hugely controversial. In fact, to support a modest investment in AIsafety research, people don’t need to be convinced that risks are high, merely non-negligible, just as a modest investment in home insurance is justified by a non-negligible probability of the home burning down. 
My personal analysis is that the media have made the AI-safety debate seem more controversial than it really is. After all, fear sells, and articles using out-ofcontext quotes to proclaim imminent doom can generate more clicks than nuanced and balanced ones. As a result, two people who only know about each other’s positions from media quotes are likely to think they disagree more than they really do. For example, a techno-skeptic whose only knowledge about Bill Gates’ position comes from a British tabloid may mistakenly think he believes superintelligence to be imminent. Similarly, someone in the beneficial-AI movement who knows nothing about Andrew Ng’s position except his abovementioned quote about overpopulation on Mars may mistakenly think he doesn’t care about AI safety. In fact, I personally know that he does—the crux is simply that because his timeline estimates are longer, he naturally tends to prioritize short-term AI challenges over long-term ones. 
Reversion has to a lesser extent happened before: for example, some of the technologies that were in widespread use during the Roman Empire were largely forgotten for about a millennium before making a comeback during the Renaissance. Isaac Asimov’s Foundation trilogy centers around the “Seldon Plan” to shorten a reversion period from 30,000 years to 1,000 years. With clever planning, it may be possible to do the opposite and lengthen rather than shorten a reversion period, for example by erasing all knowledge of agriculture. However, unfortunately for reversion enthusiasts, it’s unlikely that this scenario can be extended indefinitely without humanity either going high-tech or going extinct. Counting on people’s resembling today’s biological humans 100 million years from now would be naive, given that we haven’t existed as a species for more. After contemplating problems that future technology might cause, it’s important to also consider problems that lack of that technology can cause. In this spirit, let us explore scenarios where superintelligence is never created because humanity eliminates itself by other means. 
How might we accomplish that? The simplest strategy is “just wait.” Although we’ll see in the next chapter how we can solve such problems as asteroid impacts and boiling oceans, these solutions all require technology that we haven’t yet developed, so unless our technology advances far beyond its present level, Mother Nature will drive us extinct long before another billion years have passed. As the famous economist John Maynard Keynes said: “In the long run we are all dead.” 
Unfortunately, there are also ways in which we might self-destruct much sooner, through collective stupidity. Why would our species commit collective suicide, also known as omnicide, if virtually nobody wants it? With our present level of intelligence and emotional maturity, we humans have a knack for miscalculations, misunderstandings and incompetence, and as a result, our history is full of accidents, wars and other calamities that, in hindsight, essentially nobody wanted. Economists and mathematicians have developed elegant game-theory explanations for how people can be incentivized to actions that ultimately cause a catastrophic outcome for everyone. Nuclear War: A Case Study in Human Recklessness 
You might think that the greater the stakes, the more careful we’d be, but a closer examination of the greatest risk that our current technology permits, namely a global thermonuclear war, isn’t reassuring. We’ve had to rely on luck to weather an embarrassingly long list of near misses caused by all sorts of things: computer malfunction, power failure, faulty intelligence, navigation error, bomber crash, satellite explosion and so on. 7 In fact, if it weren’t for heroic acts of certain individuals—for example, Vasili Arkhipov and Stanislav Petrov— we might already have had a global nuclear war. Given our track record, I think it’s highly unlikely that the annual probability of accidental nuclear war is as low as one in a thousand if we keep up our present behavior, in which case the 
probability that we’ll have one within 10,000 years exceeds 1− 0.99910000 ≈ 
99.995%. 
To fully appreciate our human recklessness, we must realize that we started the nuclear gamble even before carefully studying the risks. First, radiation risks had been underestimated, and over $2 billion in compensation has been paid out to victims of radiation exposure from uranium handling and nuclear tests in the United States alone. 
Second, it was eventually discovered that hydrogen bombs deliberately detonated hundreds of kilometers above Earth would create a powerful electromagnetic pulse (EMP) that might disable the electric grid and electronic devices over vast areas (figure 5.2), leaving infrastructure paralyzed, roads clogged with disabled vehicles and conditions for nuclear-aftermath survival less than ideal. For example, the U.S. EMP Commission reported that “the water infrastructure is a vast machine, powered partly by gravity but mostly by electricity,” and that denial of water can cause death in three to four days. Third, the potential of nuclear winter wasn’t realized until four decades in, after we’d deployed 63,000 hydrogen bombs—oops! Regardless of whose cities burned, massive amounts of smoke reaching the upper troposphere might spread around the globe, blocking out enough sunlight to transform summers into winters, much like when an asteroid or supervolcano caused a mass extinction in the past. When the alarm was sounded by both U.S. and Soviet scientists in the 1980s, this contributed to the decision of Ronald Reagan and Mikhail Gorbachev to start slashing stockpiles. 10 Unfortunately, more accurate calculations have painted an even gloomier picture: figure 5.3 shows cooling by about 20° Celsius (36° Fahrenheit) in much of the core farming regions of the United States, Europe, Russia and China (and by 35°C in some parts of Russia) for the first two summers, and about half that even a full decade later
Both of these forces drop off with the square of the distance to the Sun, which means that if they can be balanced at one distance from the Sun, they’ll conveniently be balanced at any other distance as well, allowing freedom to park anywhere in our Solar System. 
Statites need to be extremely lightweight sheets, weighing only 0.77 grams per square meter, which is about 100 times less than paper, but this is unlikely to be a showstopper. I rolled my eyes when seeing this headline in the Daily Mail:3 “Stephen Hawking Warns That Rise of Robots May Be Disastrous for Mankind.” I’ve lost count of how many similar articles I’ve seen. Typically, they’re accompanied by an evil-looking robot carrying a weapon, and suggest that we should worry about robots rising up and killing us because they’ve become conscious and/or evil. On a lighter note, such articles are actually rather impressive, because they succinctly summarize the scenario that my AI colleagues don’t worry about. That scenario combines as many as three separate misconceptions: concern about consciousness, evil and robots, respectively. 
If you drive down the road, you have a subjective experience of colors, sounds, etc. But does a self-driving car have a subjective experience? Does it feel like anything at all to be a self-driving car, or is it like an unconscious zombie without any subjective experience? Although this mystery of consciousness is interesting in its own right, and we’ll devote chapter 8 to it, it’s irrelevant to AI risk. If you get struck by a driverless car, it makes no difference to you whether it subjectively feels conscious. In the same way, what will affect us humans is what superintelligent AI does, not how it subjectively feels. 
The fear of machines turning evil is another red herring. The real worry isn’t malevolence, but competence. A superintelligent AI is by definition very good at attaining its goals, whatever they may be, so we need to ensure that its goals are aligned with ours. You’re probably not an ant hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. The beneficial-AI movement wants to avoid placing humanity in the position of those ants. I sympathize with Rodney Brooks and other robotics pioneers who feel unfairly demonized by scaremongering tabloids, because some journalists seem obsessively fixated on robots and adorn many of their articles with evil-looking metal monsters with shiny red eyes. In fact, the main concern of the beneficialAI movement isn’t with robots but with intelligence itself: specifically, intelligence whose goals are misaligned with ours. To cause us trouble, such misaligned intelligence needs no robotic body, merely an internet connection— we’ll explore in chapter 4 how this may enable outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders and developing weapons we cannot even understand. Even if building robots were physically impossible, a super-intelligent and super-wealthy AI could easily pay or manipulate myriad humans to unwittingly do its bidding, as in William Gibson’s science fiction novel Neuromancer. In the rest of this book, you and I will explore together the future of life with AI. Let’s navigate this rich and multifaceted topic in an organized way by first exploring the full story of life conceptually and chronologically, and then exploring goals, meaning and what actions to take to create the future we want. 
In chapter 2, we explore the foundations of intelligence and how seemingly dumb matter can be rearranged to remember, compute and learn. As we proceed into the future, our story branches out into many scenarios defined by the answers to certain key questions. Figure 1.6 summarizes key questions we’ll encounter as we march forward in time, to potentially ever more advanced AI. 
Right now, we face the choice of whether to start an AI arms race, and questions about how to make tomorrow’s AI systems bug-free and robust. If AI’s economic impact keeps growing, we also have to decide how to modernize our laws and what career advice to give kids so that they can avoid soon-to-beautomated jobs. We explore such short-term questions in chapter 3. 
If AI progress continues to human levels, then we also need to ask ourselves how to ensure that it’s beneficial, and whether we can or should create a leisure society that flourishes without jobs. This also raises the question of whether an intelligence explosion or slow-but-steady growth can propel AGI far beyond human levels. We explore a wide range of such scenarios in chapter 4 and investigate the spectrum of possibilities for the aftermath in chapter 5, ranging from arguably dystopic to arguably utopic. Who’s in charge—humans, AI or cyborgs? Are humans treated well or badly? Are we replaced and, if so, do we perceive our replacements as conquerors or worthy descendants? I’m very curious about which of the chapter 5 scenarios you personally prefer! I’ve set up a website, http://AgeOfAi.org, where you can share your views and join the conversation. 
Finally, we forge billions of years into the future in chapter 6 where we can, ironically, draw stronger conclusions than in the previous chapters, as the ultimate limits of life in our cosmos are set not by intelligence but by the laws of physics. 
The robot misconception is related to the myth that machines can’t control humans. Intelligence enables control: humans control tigers not because we’re stronger, but because we’re smarter. This means that if we cede our position as smartest on our planet, it’s possible that we might also cede control. 
Figure 1.5 summarizes all of these common misconceptions, so that we can dispense with them once and for all and focus our discussions with friends and colleagues on the many legitimate controversies—which, as we’ll see, there’s no shortage of! 
The consciousness misconception is related to the myth that machines can’t have goals. Machines can obviously have goals in the narrow sense of exhibiting goal-oriented behavior: the behavior of a heat-seeking missile is most economically explained as a goal to hit a target. If you feel threatened by a machine whose goals are misaligned with yours, then it’s precisely its goals in this narrow sense that trouble you, not whether the machine is conscious and experiences a sense of purpose. If that heat-seeking missile were chasing you, you probably wouldn’t exclaim “I’m not worried, because machines can’t have goals!” 
If the Dyson sphere is built to reflect rather than absorb most of the sunlight, then the total intensity of light bouncing around within it will be dramatically increased, further boosting the radiation pressure and the amount of mass that can be supported in the sphere. 
Many other stars have a thousandfold and even a millionfold greater luminosity than our Sun, and are therefore able to support correspondingly heavier stationary Dyson spheres. 
For today’s humans, life on or in a Dyson sphere would at best be disorienting and at worst impossible, but that need not stop future biological or nonbiological life forms from thriving there. 
A problem with using black hole evaporation as a power source is that, unless the black hole is much smaller than an atom in size, it’s an excruciatingly slow process that takes longer than the present age of our Universe and radiates less energy than a candle. 
The power produced decreases with the square of the size of the hole, and the physicists Louis Crane and Shawn Westmoreland have therefore proposed using a black hole about a thousand times smaller than a proton, weighing about as much as the largest-ever seagoing ship. 
Their main motivation was to use the black hole engine to power a starship (a topic to which we return below), so they were more concerned with portability than efficiency and proposed feeding the black hole with laser light, causing no energy-to-matter conversion at all. 
Even if you could feed it with matter instead of radiation, guaranteeing high efficiency appears difficult: to make protons enter such a black hole a thousandth their size, they would have to be fired at the hole with a machine as powerful as the Large Hadron Collider, augmenting their energy mc2 with at least a thousand times more kinetic (motion) energy. 
Since at least 10% of that kinetic energy would be lost to gravitons when the black hole evaporates, we’d therefore be putting more energy into the black hole than we’d be able to extract and put to work, ending up with negative efficiency. 
Fortunately, there are other ways of using black holes as power plants that don’t involve quantum gravity or other poorly understood physics. For example, many existing black holes spin very fast, with their event horizons whirling around near the speed of light, and this rotation energy can be extracted. 
The event horizon of a black hole is the region from which not even light can escape, because the gravitational pull is too powerful. 
If you toss an object into the ergosphere, it will therefore pick up speed rotating around the hole. Unfortunately, it will soon get eaten up by the black hole, forever disappearing through the event horizon, so this does you no good if you’re trying to extract energy. However, Roger Penrose discovered that if you launch the object at a clever angle and make it split into two pieces as figure 6.4 illustrates, then you can arrange for only one piece to get eaten while the other escapes the black hole with more energy than you started with. In other words, you’ve successfully converted some of the rotational energy of the black hole into useful energy that you can put to work. By repeating this process many times, you can milk the black hole of all its rotational energy so that it stops spinning and its ergosphere disappears. 
Another interesting strategy is to extract energy not from the black hole itself, but from matter falling into it. Nature has already found a way of doing this all on its own: the quasar. As gas swirls even closer to a black hole, forming a pizza-shaped disk whose innermost parts gradually get gobbled up, it gets extremely hot and gives off copious amounts of radiation. As gas falls downward toward the hole, it speeds up, converting its gravitational potential energy into motion energy, just as a skydiver does. The motion gets progressively messier as complicated turbulence converts the coordinated motion of the gas blob into random motion on ever-smaller scales, until individual atoms begin colliding with each other at high speeds—having such random motion is precisely what it means to be hot, and these violent collisions convert motion energy into radiation. By building a Dyson sphere around the entire black hole, at a safe distance, this radiation energy can be captured and put to use. The faster the black hole spins, the more efficient this process gets, with a maximally spinning black hole delivering energy at a whopping 42% efficiency. *4 For black holes weighing about as much as a star, most of the energy comes out as X-rays, whereas for the supermassive kind found in the centers of galaxies, much of it emerges somewhere in the range of infrared, visible and ultraviolet light. 
There is another known way to convert matter into energy that doesn’t involve black holes at all: the sphaleron process. It can destroy quarks and turn them into leptons: electrons, their heavier cousins the muon and tau particles, neutrinos or their antiparticles. 4 As illustrated in figure 6.5, the standard model of particle physics predicts that nine quarks with appropriate flavor and spin can come together and transform into three leptons through an intermediate state called a sphaleron. Because the input weighs more than the output, the mass difference gets converted into energy according to Einstein’s E = mc2 formula. 
Future intelligent life might therefore be able to build what I’ll call a sphalerizer: an energy generator acting like a diesel engine on steroids. A traditional diesel engine compresses a mixture of air and diesel oil until the temperature gets high enough for it to spontaneously ignite and burn, after which the hot mixture re-expands and does useful work in the process, say pushing a piston. The carbon dioxide and other combustion gases weigh about 0.00000005% less than what was in the piston initially, and this mass difference turns into the heat energy driving the engine. A sphalerizer would compress ordinary matter to a couple of quadrillion degrees, and then let it re-expand and cool once the sphalerons had done their thing. *6 We already know the result of this experiment, because our early Universe performed it for us about 13.8 billion years ago, when it was that hot: almost 100% of the matter gets converted into energy, with less than a billionth of the particles left over being the stuff that ordinary matter is made of: quarks and electrons. So it’s just like a diesel engine, except over a billion times more efficient! Another advantage is that you don’t need to be finicky about what to fuel it with—it works with anything made of quarks, meaning any normal matter at all. 
Because of these high-temperature processes, our baby Universe produced over a trillion times more radiation (photons and neutrinos) than matter (quarks and electrons that later clumped into atoms). During the 13.8 billion years since then, a great segregation took place, where atoms became concentrated into galaxies, stars and planets, while most photons stayed in intergalactic space, forming the cosmic microwave background radiation that has been used to make baby pictures of our Universe. Any advanced life form living in a galaxy or other matter concentration can therefore turn most of its available matter back into energy, rebooting the matter percentage down to the same tiny value that emerged from our early Universe by briefly re-creating those hot dense conditions inside a sphalerizer. 
From a physics perspective, everything that future life may want to create—from habitats and machines to new life forms—is simply elementary particles arranged in some particular way. Just as a blue whale is rearranged krill and krill is rearranged plankton, our entire Solar System is simply hydrogen rearranged during 13.8 billion years of cosmic evolution: gravity rearranged hydrogen into stars which rearranged the hydrogen into heavier atoms, after which gravity rearranged such atoms into our planet where chemical and biological processes rearranged them into life. 
Future life that has reached its technological limit can perform such particle rearrangements more rapidly and efficiently, by first using its computing power to figure out the most efficient method and then using its available energy to power the matter rearrangement process. We saw how matter can be converted into both computers and energy, so it’s in a sense the only fundamental resource needed. *7 Once future life has bumped up against the physical limits on what it can do with its matter, there is only one way left for it to do more: by getting more matter. And the only way it can do this is by expanding into our Universe. Spaceward ho! 
The first challenge is that our Universe is expanding, which means that almost all galaxies are flying away from us, so settling distant galaxies amounts to a game of catch-up. The second challenge is that this cosmic expansion is accelerating, due to the mysterious dark energy that makes up about 70% of our 
Universe. To understand how this causes trouble, imagine that you enter a train platform and see your train slowly accelerating away from you, but with a door left invitingly open. If you’re fast and foolhardy, can you catch the train? Since it will eventually go faster than you can run, the answer clearly depends on how far away from you the train is initially: if it’s beyond a certain critical distance, you’ll never catch up with it. We face the same situation trying to catch those distant galaxies that are accelerating away from us: even if we could travel at the speed of light, all galaxies beyond about 17 billion light-years remain forever out of reach—and that’s over 98% of the galaxies in our Universe. 
But hold on: didn’t Einstein’s special relativity theory say that nothing can travel faster than light? So how can galaxies outrace something traveling at the speed of light? The answer is that special relativity is superseded by Einstein’s general relativity theory, where the speed limit is more liberal: nothing can travel faster than the speed of light through space, but space is free to expand as fast as it wants. Einstein also gave us a nice way of visualizing these speed limits by viewing time as the fourth dimension in spacetime (see figure 6.7, where I’ve kept things three-dimensional by omitting one of the three space dimensions). If space weren’t expanding, light rays would form slanted 45-degree lines through spacetime, so that the regions we can see and reach from here and now are cones. Whereas our past light cone would be truncated by our Big Bang 13.8 billion years ago, our future light cone would expand forever, giving us access to an unlimited cosmic endowment. In contrast, the middle panel of the figure shows that an expanding universe with dark energy (which appears to be the Universe we inhabit) deforms our light cones into a champagne-glass shape, forever limiting the number of galaxies we can settle to about 10 billion. 
If this limit makes you feel cosmic claustrophobia, let me cheer you up with a possible loophole: my calculation assumes that dark energy remains constant over time, consistent with what the latest measurements suggest. However, we still have no clue what dark energy really is, which leaves a glimmer of hope that dark energy will eventually decay away (much like the similar dark-energy-like substance postulated to explain cosmic inflation), and if this happens, the acceleration will give way to deceleration, potentially enabling future life forms to keep settling new galaxies for as long as they last. 
Another popular idea is to build a rocket that need not carry its own fuel. For example, interstellar space isn’t a perfect vacuum, but contains the occasional hydrogen ion (a lone proton: a hydrogen atom that’s lost its electron). In 1960, this gave physicist Robert Bussard the idea behind what’s now known as a Bussard ramjet: to scoop up such ions en route and use them as rocket fuel in an onboard fusion reactor. Although recent work has cast doubts on whether this can be made to work in practice, there’s another carry-no-fuel idea that does appear feasible for a high-tech spacefaring civilization: laser sailing. 
The possibility of superintelligence completely transforms this picture, making it much more promising for those with intergalactic wanderlust. Removing the need to transport bulky human life-support systems and adding AI-invented technology, intergalactic settlement suddenly appears rather straightforward. Forward’s laser sailing becomes much cheaper when the spacecraft need merely be large enough to contain a “seed probe”: a robot capable of landing on an asteroid or planet in the target solar system and building up a new civilization from scratch. It doesn’t even have to carry the instructions with it: all it has to do is build a receiving antenna large enough to pick up more detailed blueprints and instructions transmitted from its mother civilization at the speed of light. Once done, it uses its newly constructed lasers to send out new seed probes to continue settling the galaxy one solar system at a time. Even the vast dark expanses of space between galaxies tend to contain a significant number of intergalactic stars (rejects once ejected from their home galaxies) that can be used as way stations, thus enabling an island-hopping strategy for intergalactic laser sailing. 
If dark energy continues to accelerate distant galaxies away from one another, as the latest experimental data suggests, then this will pose a major nuisance to the future of life. It means that even if a future civilization manages to settle a million galaxies, dark energy will over the course of tens of billions of years fragment this cosmic empire into thousands of different regions unable to communicate with one another. If future life does nothing to prevent this fragmentation, then the largest remaining bastions of life will be clusters containing about a thousand galaxies, whose combined gravity is strong enough to overpower the dark energy trying to separate them. 
If a superintelligent civilization wants to stay connected, this would give it a strong incentive to do large-scale cosmic engineering. How much matter will it have time to move into its largest supercluster before dark energy puts it forever out of reach? One method for moving a star large distances is to nudge a third star into a binary system where two stars are stably orbiting each other. Just as with romantic relationships, the introduction of a third partner can destabilize things and lead to one of the three being violently ejected—in the stellar case, at great speed. If some of the three partners are black holes, such a volatile threesome can be used to fling mass fast enough to fly far outside the host galaxy. Unfortunately, this three-body technique, applied either to stars, black holes or galaxies, doesn’t appear able to move more than a tiny fraction of a civilization’s mass the large distances required to outsmart dark energy.
If, despite its best attempts at cosmic engineering, a future civilization concludes that parts of it are doomed to drift out of contact forever, it might simply let them go and wish them well. However, if it has ambitious computing goals that involve seeking the answers to certain very difficult questions, it might instead resort to a slash-and-burn strategy: it could convert the outlying galaxies into massive computers that transform their matter and energy into computation at a frenzied pace, in the hope that before dark energy pushes their burnt-out remnants from view, they could transmit the long-sought answers back to the mother cluster. This slash-and-burn strategy would be particularly appropriate for regions so distant that they can only be reached by the “cosmic spam” method, much to the chagrin of the preexisting inhabitants. Back home in the mother region, the civilization could instead aim for maximum conservation and efficiency to last as long as possible. 
After exploring how long future life can last, let’s explore how long it might want to last. Although you might find it natural to want to live as long as possible, Freeman Dyson also gave a more quantitative argument for this desire: the cost of computation drops when you compute slowly, so you’ll ultimately get more done if you slow things down as much as possible. Freeman even calculated that if our Universe keeps expanding and cooling forever, an infinite amount of computation might be possible. 
Slow doesn’t necessarily mean boring: if future life lives in a simulated world, its subjectively experienced flow of time need not have anything to do with the glacial pace at which the simulation is being run in the outside world, so the prospects of infinite computation could translate into subjective immortality for simulated life forms. Cosmologist Frank Tipler has built on this idea to speculate that you could also achieve subjective immortality in the final moments before a Big Crunch by speeding up the computations toward infinity as the temperature and density skyrocketed. 
So if life engulfs our cosmos, what form will it choose: simple and fast, or complex and slow? I predict that it will make the same choice as Earth life has made: both! The denizens of Earth’s biosphere span a staggering range of sizes, from gargantuan two-hundred-ton blue whales down to the petite 10-16 kg bacterium Pelagibacter, believed to account for more biomass than all the world’s fish combined. Moreover, organisms that are large, complex and slow often mitigate their sluggishness by containing smaller modules that are simple and fast. For example, your blink reflex is extremely fast precisely because it’s implemented by a small and simple circuit that doesn’t involve most of your brain: if that hard-to-swat fly accidentally heads toward your eye, you’ll blink within a tenth of a second, long before the relevant information has had time to spread throughout your brain and make you consciously aware of what happened. By organizing its information processing into a hierarchy of modules, our biosphere manages to both have the cake and eat it, attaining both speed and complexity. We humans already use this same hierarchical strategy to optimize parallel computing. 
What, if any, of this future information processing will be conscious in the sense of involving a subjective experience is a controversial and fascinating topic which we’ll explore in chapter 8. If consciousness requires the different parts of the system to be able to communicate with one another, then the thoughts of larger systems are by necessity slower. Whereas you or a future Earth-sized supercomputer can have many thoughts per second, a galaxy-sized mind could have only one thought every hundred thousand years, and a cosmic mind a billion light-years in size would only have time to have about ten thoughts in total before dark energy fragmented it into disconnected parts. On the other hand, these few precious thoughts and accompanying experiences might be quite deep! 
However, if superintelligence develops technology that can readily rearrange elementary particles into any form of matter whatsoever, then it will eliminate most of the incentive for long-distance trade. Why bother shipping silver between distant solar systems when it’s simpler and quicker to transmute copper into silver by rearranging its particles? Why bother shipping high-tech machinery between galaxies when both the know-how and the raw materials (any matter will do) exist in both places? My guess is that in a cosmos teeming with superintelligence, almost the only commodity worth shipping long distances will be information. The only exception might be matter to be used for cosmic engineering projects—for example, to counteract the aforementioned destructive tendency of dark energy to tear civilizations apart. As opposed to traditional human trade, this matter can be shipped in any convenient bulk form whatsoever, perhaps even as an energy beam, since the receiving superintelligence can rapidly rearrange it into whatever objects it wants. 
If the distance between neighboring space-settling civilizations is much larger than dark energy lets them expand, then they’ll never come into contact with each other or even find out about each other’s existence, so they’ll feel as if they’re alone in the cosmos. If our cosmos is more fecund so that neighbors are closer together, however, some civilizations will eventually overlap. What happens in these overlap regions? Will there be cooperation, competition or war? 
After all, information is very different from the resources that humans usually fight over, in that you can simultaneously give it away and keep it. 
Some expanding civilizations might have goals that are essentially immutable, such as those of a fundamentalist cult or a spreading virus. However, it’s also plausible that some advanced civilizations are more like open-minded humans— willing to adjust their goals when presented with sufficiently compelling arguments. If two of them meet, there will be a clash not of weapons but of ideas, where the most persuasive one prevails and has its goals spread at the speed of light through the region controlled by the other civilization. Assimilating your neighbors is a faster expansion strategy than settlement, since your sphere of influence can spread at the speed with which ideas move (the speed of light using telecommunication), whereas physical settlement inevitably progresses slower than the speed of light. This assimilation will not be forced such as that infamously employed by the Borg in Star Trek, but voluntary based on the persuasive superiority of ideas, leaving the assimilated better off. 
We’ve seen that the future cosmos can contain rapidly expanding bubbles of two kinds: expanding civilizations and those death bubbles that expand at light speed and make space uninhabitable by destroying all our elementary particles. An ambitious civilization can thus encounter three kinds of regions: uninhabited ones, life bubbles and death bubbles. If it fears uncooperative rival civilizations, it has a strong incentive to launch a rapid “land grab” and settle the uninhabited regions before the rivals do. However, it has the same expansionist incentive even if there are no other civilizations, simply to acquire resources before dark energy makes them unreachable. We just saw how bumping into another expanding civilization can be either better or worse than bumping into uninhabited space, depending on how cooperative and open-minded this neighbor is. However, it’s better to bump into any expansionist civilization (even one trying to convert your civilization into paper clips) than a death bubble, which will continue expanding at the speed of light regardless of whether you try to fight it or reason with it. 
I give a detailed justification of this argument in my book Our Mathematical Universe, so I won’t rehash it here, but the basic reason for why we’re clueless about this neighbor distance is that we’re in turn clueless about the probability of intelligent life arising in a given place. As the American astronomer Frank Drake pointed out, this probability can be calculated by multiplying together the probability of there being a habitable environment there (say an appropriate planet), the probability that life will form there and the probability that this life will evolve to become intelligent. When I was a grad student, we had no clue about any of these three probabilities. After the past two decades’ dramatic discoveries of planets orbiting other stars, it now seems likely that habitable planets are abundant, with billions in our own Galaxy alone. The probability of evolving life and then intelligence, however, remains extremely uncertain: some experts think that one or both are rather inevitable and occur on most habitable planets, while others think that one or both are extremely rare because of one or more evolutionary bottlenecks that require a wild stroke of luck to pass through. 
This is broad enough to include all above-mentioned definitions, since understanding, self-awareness, problem solving, learning, etc. are all examples of complex goals that one might have. It’s also broad enough to subsume the Oxford Dictionary definition—“the ability to acquire and apply knowledge and skills”—since one can have as a goal to apply knowledge and skills. 
Because there are many possible goals, there are many possible types of intelligence. By our definition, it therefore makes no sense to quantify intelligence of humans, non-human animals or machines by a single number such as an IQ. *1 What’s more intelligent: a computer program that can only play chess or one that can only play Go? There’s no sensible answer to this, since they’re good at different things that can’t be directly compared. 
The DQN AI system of Google DeepMind can accomplish a slightly broader range of goals: it can play dozens of different vintage Atari computer games at human level or better. In contrast, human intelligence is thus far uniquely broad, able to master a dazzling panoply of skills. A healthy child given enough training time can get fairly good not only at any game, but also at any language, sport or vocation. Comparing the intelligence of humans and machines today, we humans win hands-down on breadth, while machines outperform us in a small but growing number of narrow domains, as illustrated in figure 2.1. The holy grail of AI research is to build “general AI” (better known as artificial general intelligence, AGI) that is maximally broad: able to accomplish virtually any goal, including learning. We’ll explore this in detail in chapter 4. The term “AGI” was popularized by the AI researchers Shane Legg, Mark Gubrud and Ben Goertzel to more specifically mean human-level artificial general intelligence: the ability to accomplish any goal at least as well as humans. 1 I’ll stick with their definition, so unless I explicitly qualify the acronym (by writing “superhuman AGI,” for example), I’ll use “AGI” as shorthand for “human-level AGI.”*2 
Although the word “intelligence” tends to have positive connotations, it’s important to note that we’re using it in a completely value-neutral way: as ability to accomplish complex goals regardless of whether these goals are considered good or bad. 
It’s natural for us to rate the difficulty of tasks relative to how hard it is for us humans to perform them, as in figure 2.1. But this can give a misleading picture of how hard they are for computers. It feels much harder to multiply 314,159 by 271,828 than to recognize a friend in a photo, yet computers creamed us at arithmetic long before I was born, while human-level image recognition has only recently become possible. This fact that low-level sensorimotor tasks seem easy despite requiring enormous computational resources is known as Moravec’s paradox, and is explained by the fact that our brain makes such tasks feel easy by dedicating massive amounts of customized hardware to them—more than a quarter of our brains, in fact. 
During the decades since he wrote those passages, the sea level has kept rising relentlessly, as he predicted, like global warming on steroids, and some of his foothills (including chess) have long since been submerged. What comes next and what we should do about it is the topic of the rest of this book. 
As the sea level keeps rising, it may one day reach a tipping point, triggering dramatic change. This critical sea level is the one corresponding to machines becoming able to perform AI design. Before this tipping point is reached, the sea-level rise is caused by humans improving machines; afterward, the rise can be driven by machines improving machines, potentially much faster than humans could have done, rapidly submerging all land. This is the fascinating and controversial idea of the singularity, which we’ll have fun exploring in chapter 4. 
Computer pioneer Alan Turing famously proved that if a computer can perform a certain bare minimum set of operations, then, given enough time and memory, it can be programmed to do anything that any other computer can do. Machines exceeding this critical threshold are called universal computers (aka Turing-universal computers); all of today’s smartphones and laptops are universal in this sense. Analogously, I like to think of the critical intelligence threshold required for AI design as the threshold for universal intelligence: given enough time and resources, it can make itself able to accomplish any goal as well as any other intelligent entity. For example, if it decides that it wants better social skills, forecasting skills or AI-design skills, it can acquire them.
If we say that an atlas contains information about the world, we mean that there’s a relation between the state of the book (in particular, the positions of certain molecules that give the letters and images their colors) and the state of the world (for example, the locations of continents). If the continents were in different places, then those molecules would be in different places as well. We humans use a panoply of different devices for storing information, from books and brains to hard drives, and they all share this property: that their state can be related to (and therefore inform us about) the state of other things that we care about. 
What fundamental physical property do they all have in common that makes them useful as memory devices, i.e., devices for storing information? The answer is that they all can be in many different long-lived states—long-lived enough to encode the information until it’s needed. As a simple example, suppose you place a ball on a hilly surface that has sixteen different valleys, as in figure 2.3. Once the ball has rolled down and come to rest, it will be in one of sixteen places, so you can use its position as a way of remembering any number between 1 and 16. 
This memory device is rather robust, because even if it gets a bit jiggled and disturbed by outside forces, the ball is likely to stay in the same valley that you put it in, so you can still tell which number is being stored. The reason that this memory is so stable is that lifting the ball out of its valley requires more energy than random disturbances are likely to provide. This same idea can provide stable memories much more generally than for a movable ball: the energy of a complicated physical system can depend on all sorts of mechanical, chemical, electrical and magnetic properties, and as long as it takes energy to change the system away from the state you want it to remember, this state will be stable. This is why solids have many long-lived states, whereas liquids and gases don’t: if you engrave someone’s name on a gold ring, the information will still be there years later because reshaping the gold requires significant energy, but if you engrave it in the surface of a pond, it will be lost within a second as the water surface effortlessly changes its shape.
We can therefore think of it as encoding a binary digit (abbreviated “bit”), i.e., a zero or a one. The information stored by any more complicated memory device can equivalently be stored in multiple bits: for example, taken together, the four bits shown in figure 2.3 can be in 2 × 2 × 2 × 2 = 16 different states 0000, 0001, 0010, 0011,…, 1111, so they collectively have exactly the same memory capacity as the more complicated 16-state system. We can therefore think of bits as atoms of information—the smallest indivisible chunk of information that can’t be further subdivided, which can combine to make up any information. For example, I just typed the word “word,” and my laptop represented it in its memory as the 4-number sequence 119 111 114 100, storing each of those numbers as 8 bits (it represents each lowercase letter by a number that’s 96 plus its order in the alphabet). As soon as I hit the w key on my keyboard, my laptop displayed a visual image of a w on my screen, and this image is also represented by bits: 32 bits specify the color of each of the screen’s millions of pixels. 
Since two-state systems are easy to manufacture and work with, most modern computers store their information as bits, but these bits are embodied in a wide variety of ways. On a DVD, each bit corresponds to whether there is or isn’t a microscopic pit at a given point on the plastic surface. On a hard drive, each bit corresponds to a point on the surface being magnetized in one of two ways. In my laptop’s working memory, each bit corresponds to the positions of certain electrons, determining whether a device called a micro-capacitor is charged. Some kinds of bits are convenient to transport as well, even at the speed of light: for example, in an optical fiber transmitting your email, each bit corresponds to a laser beam being strong or weak at a given time. 
Engineers prefer to encode bits into systems that aren’t only stable and easy to read from (as a gold ring), but also easy to write to: altering the state of your hard drive requires much less energy than engraving gold. They also prefer systems that are convenient to work with and cheap to mass-produce. But other than that, they simply don’t care about how the bits are represented as physical objects—and nor do you most of the time, because it simply doesn’t matter!
In other words, information can take on a life of its own, independent of its physical substrate! Indeed, it’s usually only this substrateindependent aspect of information that we’re interested in: if your friend calls you up to discuss that document you sent, she’s probably not calling to talk about voltages or molecules. This is our first hint of how something as intangible as intelligence can be embodied in tangible physical stuff, and we’ll soon see how this idea of substrate independence is much deeper, including not only information but also computation and learning. 
Because of this substrate independence, clever engineers have been able to repeatedly replace the memory devices inside our computers with dramatically better ones, based on new technologies, without requiring any changes whatsoever to our software. The result has been spectacular, as illustrated in figure 2.4: over the past six decades, computer memory has gotten half as expensive roughly every couple of years. Hard drives have gotten over 100 million times cheaper, and the faster memories useful for computation rather than mere storage have become a whopping 10 trillion times cheaper. If you could get such a “99.99999999999% off” discount on all your shopping, you could buy all real estate in New York City for about 10 cents and all the gold that’s ever been mined for around a dollar. 
For many of us, the spectacular improvements in memory technology come with personal stories. I fondly remember working in a candy store back in high school to pay for a computer sporting 16 kilobytes of memory, and when I made and sold a word processor for it with my high school classmate Magnus Bodin, we were forced to write it all in ultra-compact machine code to leave enough memory for the words that it was supposed to process. After getting used to floppy drives storing 70kB, I became awestruck by the smaller 3.5-inch floppies that could store a whopping 1.44MB and hold a whole book, and then my firstever hard drive storing 10MB—which might just barely fit a single one of today’s song downloads. These memories from my adolescence felt almost unreal the other day, when I spent about $100 on a hard drive with 300,000 times more capacity.
Comparing these numbers with the machine memories shows that the world’s best computers can now out-remember any biological system—at a cost that’s rapidly dropping and was a few thousand dollars in 2016. 
The memory in your brain works very differently from computer memory, not only in terms of how it’s built, but also in terms of how it’s used. Whereas you retrieve memories from a computer or hard drive by specifying where it’s stored, you retrieve memories from your brain by specifying something about what is stored. Each group of bits in your computer’s memory has a numerical address, and to retrieve a piece of information, the computer specifies at what address to look, just as if I tell you “Go to my bookshelf, take the fifth book from the right on the top shelf, and tell me what it says on page 314.” In contrast, you retrieve information from your brain similarly to how you retrieve it from a search engine: you specify a piece of the information or something related to it, and it pops up. If I tell you “to be or not,” or if I google it, chances are that it will trigger “To be, or not to be, that is the question.” Indeed, it will probably work even if I use another part of the quote or mess things up somewhat. Such memory systems are called auto-associative, since they recall by association rather than by address. 
In a famous 1982 paper, the physicist John Hopfield showed how a network of interconnected neurons could function as an auto-associative memory. I find the basic idea very beautiful, and it works for any physical system with multiple stable states. For example, consider a ball on a surface with two valleys, like the one-bit system in figure 2.3, and let’s shape the surface so that the x-coordinates 
of the two minima where the ball can come to rest are x = √2 ≈ 1.41421 and x = π ≈ 3.14159, respectively. If you remember only that π is close to 3, you simply 
put the ball at x = 3 and watch it reveal a more exact π-value as it rolls down to the nearest minimum. Although it sounds deceptively simple, this idea of a function is incredibly general. Some functions are rather trivial, such as the one called NOT that inputs a single bit and outputs the reverse, thus turning zero into one and vice versa. The functions we learn about in school typically correspond to buttons on a pocket calculator, inputting one or more numbers and outputting a single number —for example, the function x2 simply inputs a number and outputs it multiplied by itself. Other functions can be extremely complicated. For instance, if you’re in possession of a function that would input bits representing an arbitrary chess position and output bits representing the best possible next move, you can use it to win the World Computer Chess Championship. If you’re in possession of a function that inputs all the world’s financial data and outputs the best stocks to buy, you’ll soon be extremely rich. Many AI researchers dedicate their careers to figuring out how to implement certain functions. 
In other words, if you can implement highly complex functions, then you can build an intelligent machine that’s able to accomplish highly complex goals. This brings our question of how matter can be intelligent into sharper focus: in particular, how can a clump of seemingly dumb matter compute a complicated function? 
Rather than just remain immobile as a gold ring or other static memory device, it must exhibit complex dynamics so that its future state depends in some complicated (and hopefully controllable/programmable) way on the present state. Its atom arrangement must be less ordered than a rigid solid where nothing interesting changes, but more ordered than a liquid or gas. Specifically, we want the system to have the property that if we put it in a state that encodes the input information, let it evolve according to the laws of physics for some amount of time, and then interpret the resulting final state as the output information, then the output is the desired function of the input. 
As a first example of this idea, let’s explore how we can build a very simple (but also very important) function called a NAND gate *3 out of plain old dumb matter. This function inputs two bits and outputs one bit: it outputs 0 if both inputs are 1; in all other cases, it outputs 1. If we connect two switches in series with a battery and an electromagnet, then the electromagnet will only be on if the first switch and the second switch are closed (“on”). Let’s place a third switch under the electromagnet, as illustrated in figure 2.6, such that the magnet will pull it open whenever it’s powered on. If we interpret the first two switches as the input bits and the third one as the output bit (with 0 = switch open, and 1 = switch closed), then we have ourselves a NAND gate: the third switch is open only if the first two are closed. There are many other ways of building NAND gates that are more practical—for example, using transistors as illustrated in figure 2.6. In today’s computers, NAND gates are typically built from microscopic transistors and other components that can be automatically etched onto silicon wafers. 
There’s a remarkable theorem in computer science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates. *4 So if you can build enough NAND gates, you can build a device computing anything! In case you’d like a taste of how this works, I’ve illustrated in figure 2.7 how to multiply numbers using nothing but NAND gates. 
As a first example of this idea, let’s explore how we can build a very simple (but also very important) function called a NAND gate *3 out of plain old dumb matter. This function inputs two bits and outputs one bit: it outputs 0 if both inputs are 1; in all other cases, it outputs 1. If we connect two switches in series with a battery and an electromagnet, then the electromagnet will only be on if the first switch and the second switch are closed (“on”). Let’s place a third switch under the electromagnet, as illustrated in figure 2.6, such that the magnet will pull it open whenever it’s powered on. If we interpret the first two switches as the input bits and the third one as the output bit (with 0 = switch open, and 1 = switch closed), then we have ourselves a NAND gate: the third switch is open only if the first two are closed. There are many other ways of building NAND gates that are more practical—for example, using transistors as illustrated in figure 2.6. In today’s computers, NAND gates are typically built from microscopic transistors and other components that can be automatically etched onto silicon wafers. 
There’s a remarkable theorem in computer science that says that NAND gates are universal, meaning that you can implement any well-defined function simply by connecting together NAND gates. *4 So if you can build enough NAND gates, you can build a device computing anything! In case you’d like a taste of how this works, I’ve illustrated in figure 2.7 how to multiply numbers using nothing but NAND gates. 
You’d also have no way of knowing what type of transistors the microprocessor was using. 
I first came to appreciate this crucial idea of substrate independence because there are many beautiful examples of it in physics. Waves, for instance: they have properties such as speed, wavelength and frequency, and we physicists can study the equations they obey without even needing to know what particular substance they’re waves in. When you hear something, you’re detecting sound waves caused by molecules bouncing around in the mixture of gases that we call air, and we can calculate all sorts of interesting things about these waves—how their intensity fades as the square of the distance, such as how they bend when they pass through open doors and how they bounce off of walls and cause echoes —without knowing what air is made of. In fact, we don’t even need to know that it’s made of molecules: we can ignore all details about oxygen, nitrogen, carbon dioxide, etc., because the only property of the wave’s substrate that matters and enters into the famous wave equation is a single number that we can measure: the wave speed, which in this case is about 300 meters per second. Indeed, this wave equation that I taught my MIT students about in a course last spring was first discovered and put to great use long before physicists had even established that atoms and molecules existed! 
This wave example illustrates three important points. First, substrate independence doesn’t mean that a substrate is unnecessary, but that most of its details don’t matter. You obviously can’t have sound waves in a gas if there’s no gas, but any gas whatsoever will suffice. Similarly, you obviously can’t have computation without matter, but any matter will do as long as it can be arranged into NAND gates, connected neurons or some other building block enabling universal computation. Second, the substrate-independent phenomenon takes on a life of its own, independent of its substrate. A wave can travel across a lake, even though none of its water molecules do—they mostly bob up and down, like fans doing “the wave” in a sports stadium. Third, it’s often only the substrateindependent aspect that we’re interested in: a surfer usually cares more about the position and height of a wave than about its detailed molecular composition. 
We’ve now arrived at an answer to our opening question about how tangible physical stuff can give rise to something that feels as intangible, abstract and ethereal as intelligence: it feels so non-physical because it’s substrateindependent, taking on a life of its own that doesn’t depend on or reflect the physical details. In short, computation is a pattern in the spacetime arrangement of particles, and it’s not the particles but the pattern that really matters! Matter doesn’t matter. 
In other words, the hardware is the matter and the software is the pattern. This substrate independence of computation implies that AI is possible: intelligence doesn’t require flesh, blood or carbon atoms. 
Because of this substrate independence, shrewd engineers have been able to repeatedly replace the technologies inside our computers with dramatically better ones, without changing the software. The results have been every bit as spectacular as those for memory devices. As illustrated in figure 2.8, computation keeps getting half as expensive roughly every couple of years, and this trend has now persisted for over a century, cutting the computer cost a whopping million million million (1018) times since my grandmothers were born. If everything got a million million million times cheaper, then a hundredth of a cent would enable you to buy all goods and services produced on Earth this year. This dramatic drop in costs is of course a key reason why computation is everywhere these days, having spread from the building-sized computing facilities of yesteryear into our homes, cars and pockets—and even turning up in unexpected places such as sneakers. 
All examples of persistent doubling that I know of in nature have the same fundamental cause, and this technological one is no exception: each step creates the next. For example, you yourself underwent exponential growth right after your conception: each of your cells divided and gave rise to two cells roughly daily, causing your total number of cells to increase day by day as 1, 2, 4, 8, 16 and so on. According to the most popular scientific theory of our cosmic origins, known as inflation, our baby Universe once grew exponentially just like you did, repeatedly doubling its size at regular intervals until a speck much smaller and lighter than an atom had grown more massive than all the galaxies we’ve ever seen with our telescopes. Again, the cause was a process whereby each doubling step caused the next. This is how technology progresses as well: once 
technology gets twice as powerful, it can often be used to design and build technology that’s twice as powerful in turn, triggering repeated capability doubling in the spirit of Moore’s law. 
Something that occurs just as regularly as the doubling of our technological power is the appearance of claims that the doubling is ending. Yes, Moore’s law will of course end, meaning that there’s a physical limit to how small transistors can be made. But some people mistakenly assume that Moore’s law is synonymous with the persistent doubling of our technological power. Contrariwise, Ray Kurzweil points out that Moore’s law involves not the first but the fifth technological paradigm to bring exponential growth in computing, as illustrated in figure 2.8: whenever one technology stopped improving, we replaced it with an even better one. When we could no longer keep shrinking our vacuum tubes, we replaced them with transistors and then integrated circuits, where electrons move around in two dimensions. When this technology reaches its limits, there are many other alternatives we can try—for example, using three-dimensional circuits and using something other than electrons to do our bidding. 
Nobody knows for sure what the next blockbuster computational substrate will be, but we do know that we’re nowhere near the limits imposed by the laws of physics. My MIT colleague Seth Lloyd has worked out what this fundamental limit is, and as we’ll explore in greater detail in chapter 6, this limit is a whopping 33 orders of magnitude (1033 times) beyond today’s state of the art for how much computing a clump of matter can do. So even if we keep doubling the power of our computers every couple of years, it will take over two centuries until we reach that final frontier. 
Although all universal computers are capable of the same computations, some are more efficient than others. For example, a computation requiring millions of multiplications doesn’t require millions of separate multiplication modules built from separate transistors as in figure 2.6: it needs only one such module, since it can use it many times in succession with appropriate inputs. In this spirit of efficiency, most modern computers use a paradigm where computations are split into multiple time steps, during which information is shuffled back and forth between memory modules and computation modules. 
Today’s computers often gain additional speed by parallel processing, which cleverly undoes some of this reuse of modules: if a computation can be split into parts that can be done in parallel (because the input of one part doesn’t require the output of another), then the parts can be computed simultaneously by different parts of the hardware. 
The ultimate parallel computer is a quantum computer. Quantum computing pioneer David Deutsch controversially argues that “quantum computers share information with huge numbers of versions of themselves throughout the multiverse,” and can get answers faster here in our Universe by in a sense getting help from these other versions. 4 We don’t yet know whether a commercially competitive quantum computer can be built during the coming decades, because it depends both on whether quantum physics works as we think it does and on our ability to overcome daunting technical challenges, but companies and governments around the world are betting tens of millions of dollars annually on the possibility. Although quantum computers cannot speed up run-of-the-mill computations, clever algorithms have been developed that may dramatically speed up specific types of calculations, such as cracking cryptosystems and training neural networks. A quantum computer could also efficiently simulate the behavior of quantum-mechanical systems, including atoms, molecules and new materials, replacing measurements in chemistry labs in the same way that simulations on traditional computers have replaced measurements in wind tunnels. 
Although a pocket calculator can crush me in an arithmetic contest, it will never improve its speed or accuracy, no matter how much it practices. It doesn’t learn: for example, every time I press its square-root button, it computes exactly the same function in exactly the same way. Similarly, the first computer program that ever beat me at chess never learned from its mistakes, but merely implemented a function that its clever programmer had designed to compute a good next move. In contrast, when Magnus Carlsen lost his first game of chess at age five, he began a learning process that made him the World Chess Champion eighteen years later. 
The ability to learn is arguably the most fascinating aspect of general intelligence. We’ve already seen how a seemingly dumb clump of matter can remember and compute, but how can it learn? We’ve seen that finding the answer to a difficult question corresponds to computing a function, and that appropriately arranged matter can calculate any computable function. When we humans first created pocket calculators and chess programs, we did the arranging. For matter to learn, it must instead rearrange itself to get better and better at computing the desired function—simply by obeying the laws of physics. 
To demystify the learning process, let’s first consider how a very simple physical system can learn the digits of π and other numbers. Above we saw how a surface with many valleys (see figure 2.3) can be used as a memory device: for 
example, if the bottom of one of the valleys is at position x = π ≈ 3.14159 and 
there are no other valleys nearby, then you can put a ball at x = 3 and watch the system compute the missing decimals by letting the ball roll down to the bottom. Now, suppose that the surface is made of soft clay and starts out completely flat, as a blank slate. If some math enthusiasts repeatedly place the ball at the locations of each of their favorite numbers, then gravity will gradually create valleys at these locations, after which the clay surface can be used to recall these stored memories. 
Neural networks have now transformed both biological and artificial intelligence, and have recently started dominating the AI subfield known as machine learning (the study of algorithms that improve through experience). Before delving deeper into how such networks can learn, let’s first understand how they can compute. A neural network is simply a group of interconnected neurons that are able to influence each other’s behavior. Your brain contains about as many neurons as there are stars in our Galaxy: in the ballpark of a hundred billion. On average, each of these neurons is connected to about a thousand others via junctions called synapses, and it’s the strengths of these roughly hundred trillion synapse connections that encode most of the information in your brain. 
We can schematically draw a neural network as a collection of dots representing neurons connected by lines representing synapses (see figure 2.9). Real-world neurons are very complicated electrochemical devices looking nothing like this schematic illustration: they involve different parts with names such as axons and dendrites, there are many different kinds of neurons that operate in a wide variety of ways, and the exact details of how and when electrical activity in one neuron affects other neurons is still the subject of active study. However, AI researchers have shown that neural networks can still attain human-level performance on many remarkably complex tasks even if one ignores all these complexities and replaces real biological neurons with extremely simple simulated ones that are all identical and obey very simple rules. The currently most popular model for such an artificial neural network represents the state of each neuron by a single number and the strength of each synapse by a single number. 
The success of these simple artificial neural networks is yet another example of substrate independence: neural networks have great computational power seemingly independent of the low-level nitty-gritty details of their construction. Indeed, George Cybenko, Kurt Hornik, Maxwell Stinchcombe and Halbert White proved something remarkable in 1989: such simple neural networks are universal in the sense that they can compute any function arbitrarily accurately, by simply adjusting those synapse strength numbers accordingly. In other words, evolution probably didn’t make our biological neurons so complicated because it was necessary, but because it was more efficient—and because evolution, as opposed to human engineers, doesn’t reward designs that are simple and easy to understand. 
When I first learned about this, I was mystified by how something so simple could compute something arbitrarily complicated. 
Although you can prove that you can compute anything in theory with an arbitrarily large neural network, the proof doesn’t say anything about whether you can do so in practice, with a network of reasonable size. In fact, the more I thought about it, the more puzzled I became that neural networks worked so well. 
For example, suppose that we wish to classify megapixel grayscale images into two categories, say cats or dogs. If each of the million pixels can take one of, say, 256 values, then there are 2561000000 possible images, and for each one, we wish to compute the probability that it depicts a cat. This means that an arbitrary function that inputs a picture and outputs a probability is defined by a list of 2561000000 probabilities, that is, way more numbers than there are atoms in our Universe (about 1078). Yet neural networks with merely thousands or millions of parameters somehow manage to perform such classification tasks quite well. How can successful neural networks be “cheap,” in the sense of requiring so few parameters? After all, you can prove that a neural network small enough to fit inside our Universe will epically fail to approximate almost all functions, succeeding merely on a ridiculously tiny fraction of all computational tasks that you might assign to it. 
I’ve had lots of fun puzzling over this and related mysteries with my student Henry Lin. One of the things I feel most grateful for in life is the opportunity to collaborate with amazing students, and Henry is one of them. When he first walked into my office to ask whether I was interested in working with him, I thought to myself that it would be more appropriate for me to ask whether he was interested in working with me: this modest, friendly and bright-eyed kid from Shreveport, Louisiana, had already written eight scientific papers, won a Forbes 30-Under-30 award, and given a TED talk with over a million views— and he was only twenty! A year later, we wrote a paper together with a surprising 
conclusion: the question of why neural networks work so well can’t be answered with mathematics alone, because part of the answer lies in physics. We found that the class of functions that the laws of physics throw at us and make us interested in computing is also a remarkably tiny class because, for reasons that we still don’t fully understand, the laws of physics are remarkably simple. Moreover, the tiny fraction of functions that neural networks can compute is very similar to the tiny fraction that physics makes us interested in! We also extended previous work showing that deep-learning neural networks (they’re called “deep” if they contain many layers) are much more efficient than shallow ones for many of these functions of interest. 
This helps explain not only why neural networks are now all the rage among AI researchers, but also why we evolved neural networks in our brains: if we evolved brains to predict the future, then it makes sense that we’d evolve a computational architecture that’s good at precisely those computational problems that matter in the physical world. 
Now that we’ve explored how neural networks work and compute, let’s return to the question of how they can learn. Specifically, how can a neural network get better at computing by updating its synapses? 
In his seminal 1949 book, The Organization of Behavior: A Neuropsychological Theory, the Canadian psychologist Donald Hebb argued that if two nearby neurons were frequently active (“firing”) at the same time, their synaptic coupling would strengthen so that they learned to help trigger each other—an idea captured by the popular slogan “Fire together, wire together.” Although the details of how actual brains learn are still far from understood, and research has shown that the answers are in many cases much more complicated, it’s also been shown that even this simple learning rule (known as Hebbian learning) allows neural networks to learn interesting things. John Hopfield showed that Hebbian learning allowed his oversimplified artificial neural network to store lots of complex memories by simply being exposed to them repeatedly. Such exposure to information to learn from is usually called “training” when referring to artificial neural networks (or to animals or people being taught skills), although “studying,” “education” or “experience” might be just as apt. 
As if by magic, this simple rule can make the neural network learn remarkably complex computations if training is performed with large amounts of data. We don’t yet know precisely what learning rules our brains use, but whatever the answer may be, there’s no indication that they violate the laws of physics. 
Just as most digital computers gain efficiency by splitting their work into multiple steps and reusing computational modules many times, so do many artificial and biological neural networks. Brains have parts that are what computer scientists call recurrent rather than feedforward neural networks, where information can flow in multiple directions rather than just one way, so that the current output can become input to what happens next. The network of logic gates in the microprocessor of a laptop is also recurrent in this sense: it keeps reusing its past information, and lets new information input from a keyboard, trackpad, camera, etc., affect its ongoing computation, which in turn determines information output to, say, a screen, loudspeaker, printer or wireless network. Analogously, the network of neurons in your brain is recurrent, letting information input from your eyes, ears and other senses affect its ongoing computation, which in turn determines information output to your muscles. 
The history of learning is at least as long as the history of life itself, since every self-reproducing organism performs interesting copying and processing of information—behavior that has somehow been learned. During the era of Life 1.0, however, organisms didn’t learn during their lifetime: their rules for processing information and reacting were determined by their inherited DNA, so the only learning occurred slowly at the species level, through Darwinian evolution across generations. 
As we all know, the explosive improvements in computer memory and computational power (figure 2.4 and figure 2.8) have translated into spectacular progress in artificial intelligence—but it took a long time until machine learning came of age. When IBM’s Deep Blue computer overpowered chess champion Garry Kasparov in 1997, its major advantages lay in memory and computation, not in learning. Its computational intelligence had been created by a team of humans, and the key reason that Deep Blue could outplay its creators was its ability to compute faster and thereby analyze more potential positions. When IBM’s Watson computer dethroned the human world champion in the quiz show Jeopardy!, it too relied less on learning than on custom-programmed skills and superior memory and speed. The same can be said of most early breakthroughs in robotics, from legged locomotion to self-driving cars and self-landing rockets. 
In contrast, the driving force behind many of the most recent AI breakthroughs has been machine learning. Consider figure 2.11, for example. It’s easy for you to tell what it’s a photo of, but to program a function that inputs nothing but the colors of all the pixels of an image and outputs an accurate caption such as “A group of young people playing a game of frisbee” had eluded all the world’s AI researchers for decades. Yet a team at Google led by Ilya Sutskever did precisely that in 2014. Input a different set of pixel colors, and it replies “A herd of elephants walking across a dry grass field,” again correctly. How did they do it? Deep Blue–style, by programming handcrafted algorithms for detecting frisbees, faces and the like? No, by creating a relatively simple neural network with no knowledge whatsoever about the physical world or its contents, and then letting it learn by exposing it to massive amounts of data. AI visionary Jeff Hawkins wrote in 2004 that “no computer can…see as well as a mouse,” but those days are now long gone. 
Just as we don’t fully understand how our children learn, we still don’t fully understand how such neural networks learn, and why they occasionally fail. But what’s clear is that they’re already highly useful and are triggering a surge of investments in deep learning. Deep learning has now transformed many aspects of computer vision, from handwriting transcription to real-time video analysis for self-driving cars. It has similarly revolutionized the ability of computers to transform spoken language into text and translate it into other languages, even in real time—which is why we can now talk to personal digital assistants such as Siri, Google Now and Cortana. Those annoying CAPTCHA puzzles, where we need to convince a website that we’re human, are getting ever more difficult in order to keep ahead of what machine-learning technology can do. In 2015, Google DeepMind released an AI system using deep learning that was able to master dozens of computer games like a kid would—with no instructions whatsoever—except that it soon learned to play better than any human. In 2016, the same company built AlphaGo, a Go-playing computer system that used deep learning to evaluate the strength of different board positions and defeated the world’s strongest Go champion. 
To learn our goals, an AI must figure out not what we do, but why we do it. We humans accomplish this so effortlessly that it’s easy to forget how hard the task is for a computer, and how easy it is to misunderstand. If you ask a future self-driving car to take you to the airport as fast as possible and it takes you literally, you’ll get there chased by helicopters and covered in vomit. If you exclaim, “That’s not what I wanted!,” it can justifiably answer, “That’s what you asked for.” The same theme recurs in many famous stories. In the ancient Greek legend, King Midas asked that everything he touched turn to gold, but was disappointed when this prevented him from eating and even more so when he inadvertently turned his daughter to gold. In the stories where a genie grants three wishes, there are many variants for the first two wishes, but the third wish is almost always the same: “Please undo the first two wishes, because that’s not what I really wanted.” 
All these examples show that to figure out what people really want, you can’t merely go by what they say. You also need a detailed model of the world, including the many shared preferences that we tend to leave unstated because we consider them obvious, such as that we don’t like vomiting or eating gold. Once we have such a world model, we can often figure out what people want even if they don’t tell us, simply by observing their goal-oriented behavior. Indeed, children of hypocrites usually learn more from what they see their parents do than from what they hear them say. 
AI researchers are currently trying hard to enable machines to infer goals from behavior, and this will be useful also long before any superintelligence comes on the scene. For example, a retired man may appreciate it if his eldercare robot can figure out what he values simply by observing him, so that he’s spared the hassle of having to explain everything with words or computer programming. One challenge involves finding a good way to encode arbitrary systems of goals and ethical principles into a computer, and another challenge is making machines that can figure out which particular system best matches the behavior they observe. 
If this one example were all the AI knew about firefighters, fires and babies, it would indeed be impossible to know which explanation was correct. However, a key idea underlying inverse reinforcement learning is that we make decisions all the time, and that every decision we make reveals something about our goals. The hope is therefore that by observing lots of people in lots of situations (either for real or in movies and books), the AI can eventually build an accurate model of all our preferences. 4 
In the inverse reinforcement-learning approach, a core idea is that the AI is trying to maximize not the goal-satisfaction of itself, but that of its human owner. 
It therefore has an incentive to be cautious when it’s unclear about what its owner wants, and to do its best to find out. 
It should also be fine with its owner switching it off, since that would imply that it had misunderstood what its owner really wanted. 
Even if an AI can be built to learn what your goals are, this doesn’t mean that it will necessarily adopt them. Consider your least favorite politicians: you know what they want, but that’s not what you want, and even though they try hard, they’ve failed to persuade you to adopt their goals. 
We have many strategies for imbuing our children with our goals—some more successful than others, as I’ve learned from raising two teenage boys. When those to be persuaded are computers rather than people, the challenge is known as the value-loading problem, and it’s even harder than the moral education of children. Consider an AI system whose intelligence is gradually being improved from subhuman to superhuman, first by us tinkering with it and then through recursive self-improvement like Prometheus. At first, it’s much less powerful than you, so it can’t prevent you from shutting it down and replacing those parts of its software and data that encode its goals—but this won’t help, because it’s still too dumb to fully understand your goals, which requires human-level intelligence to comprehend. 
The reason that value loading can be harder with machines than with people is that their intelligence growth can be much faster: whereas children can spend many years in that magic persuadable window where their intelligence is comparable to that of their parents, an AI might, like Prometheus, blow through this window in a matter of days or hours. 
Some researchers are pursuing an alternative approach to making machines adopt our goals, which goes by the buzzword corrigibility. The hope is that one can give a primitive AI a goal system such that it simply doesn’t care if you occasionally shut it down and alter its goals. If this proves possible, then you can safely let your AI get superintelligent, power it off, install your goals, try it out for a while and, whenever you’re unhappy with the results, just power it down and make more goal tweaks. 
But even if you build an AI that will both learn and adopt your goals, you still haven’t finished solving the goal-alignment problem: what if your AI’s goals evolve as it gets smarter? How are you going to guarantee that it retains your goals no matter how much recursive self-improvement it undergoes? Let’s explore an interesting argument for why goal retention is guaranteed automatically, and then see if we can poke holes in it. 
Although we can’t predict in detail what will happen after an intelligence explosion—which is why Vernor Vinge called it a “singularity”—the physicist and AI researcher Steve Omohundro argued in a seminal 2008 essay that we can nonetheless predict certain aspects of the superintelligent AI’s behavior almost independently of whatever ultimate goals it may have. 5 This argument was reviewed and further developed in Nick Bostrom’s book Superintelligence. We humans tend to prefer some particle arrangements over others; for example, we prefer our hometown arranged as it is over having its particles rearranged by a hydrogen bomb explosion. So suppose we try to define a goodness function that associates a number with every possible arrangement of the particles in our Universe, quantifying how “good” we think this arrangement is, and then give a superintelligent AI the goal of maximizing this function. This may sound like a reasonable approach, since describing goal-oriented behavior as function maximization is popular in other areas of science: for example, economists often model people as trying to maximize what they call a “utility function,” and many AI designers train their intelligent agents to maximize what they call a “reward function.” When we’re taking about the ultimate goals for our cosmos, however, this approach poses a computational nightmare, since it would need to define a goodness value for every one of more than a googolplex possible arrangements of the elementary particles in our Universe, where a googolplex is 1 followed by 10100 zeroes—more zeroes than there are particles in our Universe. How would we define this goodness function to the AI? Let’s now explore a scenario where all these forms of suffering are absent because a single benevolent superintelligence runs the world and enforces strict rules designed to maximize its model of human happiness. This is one possible outcome of the first Omega scenario from the previous chapter, where they relinquish control to Prometheus after figuring out how to make it want a flourishing human society. 
Thanks to amazing technologies developed by the dictator AI, humanity is free from poverty, disease and other low-tech problems, and all humans enjoy a life of luxurious leisure. They have all their basic needs taken care of, while AIcontrolled machines produce all necessary goods and services. Crime is practically eliminated, because the dictator AI is essentially omniscient and efficiently punishes anyone disobeying the rules. Everybody wears the security bracelet from the last chapter (or a more convenient implanted version), capable of real-time surveillance, punishment, sedation and execution. Everybody knows that they live in an AI dictatorship with extreme surveillance and policing, but most people view this as a good thing. 
For example, after spending an intense week in the knowledge sector learning about the ultimate laws of physics that the AI has discovered, you might decide to cut loose in the hedonistic sector over the weekend and then relax for a few days at a beach resort in the wildlife sector. 
The AI enforces two tiers of rules: universal and local. Universal rules apply in all sectors, for example a ban on harming other people, making weapons or trying to create a rival superintelligence. Individual sectors have additional local rules on top of this, encoding certain moral values. The sector system therefore helps deal with values that don’t mesh. The largest number of local rules apply in the prison sector and some of the religious sectors, while there’s a Libertarian Sector whose denizens pride themselves on having no local rules whatsoever. All punishments, even local ones, are carried out by the AI, since a human punishing another human would violate the universal no-harm rule. If you violate a local rule, the AI gives you the choice (unless you’re in the prison sector) of accepting the prescribed punishment or banishment from that sector forever. For example, if two women get romantically involved in a sector where homosexuality is punished by a prison sentence (as it is in many countries today), the AI will let them choose between going to jail or permanently leaving that sector, never again meeting their old friends (unless they leave too). 
Regardless of what sector they’re born in, all children get a minimum basic education from the AI, which includes knowledge about humanity as a whole and the fact that they’re free to visit and move to other sectors if they so choose. 
The AI designed the large number of different sectors partly because it was created to value the human diversity that exists today. But each sector is a happier place than today’s technology would allow, because the AI has eliminated all traditional problems, including poverty and crime. For example, people in the hedonistic sector need not worry about sexually transmitted diseases (they’ve been eradicated), hangovers or addiction (the AI has developed perfect recreational drugs with no negative side effects). Although the benevolent dictatorship teems with positive experiences and is rather free from suffering, many people nonetheless feel that things could be better. First of all, some people wish that humans had more freedom in shaping their society and their destiny, but they keep these wishes to themselves because they know that it would be suicidal to challenge the overwhelming power of the machine that rules them all. Some groups want the freedom to have as many children as they want, and resent the AI’s insistence on sustainability through population control. Gun enthusiasts abhor the ban on building and using weapons, and some scientists dislike the ban on building their own superintelligence. Many people feel moral outrage over what goes on in other sectors, worry that their children will choose to move there, and yearn for the freedom to impose their own moral code everywhere. 
Over time, ever more people choose to move to those sectors where the AI gives them essentially any experiences they want. In contrast to traditional visions of heaven where you get what you deserve, this is in the spirit of “New Heaven” in Julian Barnes’ 1989 novel History of the World in 10½ Chapters (and also the 1960 Twilight Zone episode “A Nice Place to Visit”), where you get what you desire. Paradoxically, many people end up lamenting always getting what they want. In Barnes’ story, the protagonist spends eons indulging his desires, from gluttony and golf to sex with celebrities, but eventually succumbs to ennui and requests annihilation. Many people in the benevolent dictatorship meet a similar fate, with lives that feel pleasant but ultimately meaningless. Although people can create artificial challenges, from scientific rediscovery to rock climbing, everyone knows that there is no true challenge, merely entertainment. There’s no real point in humans trying to do science or figure other things out, because the AI already has. There’s no real point in humans trying to create something to improve their lives, because they’ll readily get it from the AI if they simply ask. 
A core idea is borrowed from the open-source software movement: if software is free to copy, then everyone can use as much of it as they need and issues of ownership and property become moot. *1 According to the law of supply and demand, cost reflects scarcity, so if supply is essentially unlimited, the price becomes negligible. In this spirit, all intellectual property rights are abolished: there are no patents, copyrights or trademarked designs—people simply share their good ideas, and everyone is free to use them. 
Thanks to advanced robotics, this same no-property idea applies not only to information products such as software, books, movies and designs, but also to material products such as houses, cars, clothing and computers. All these products are simply atoms rearranged in particular ways, and there’s no shortage of atoms, so whenever a person wants a particular product, a network of robots will use one of the available open-source designs to build it for them for free. Care is taken to use easily recyclable materials, so that whenever someone gets tired of an object they’ve used, robots can rearrange its atoms into something someone else wants. In this way, all resources are recycled, so none are permanently destroyed. These robots also build and maintain enough renewable power-generation plants (solar, wind, etc.) that energy is also essentially free. 
To avoid obsessive hoarders requesting so many products or so much land that others are left needy, each person receives a basic monthly income from the government, which they can spend as they wish on products and renting places to live. There’s essentially no incentive for anyone to try to earn more money, because the basic income is high enough to meet any reasonable needs. It would also be rather hopeless to try, because they’d be competing with people giving away intellectual products for free and robots producing material goods essentially for free. 
Intellectual property rights are sometimes hailed as the mother of creativity and invention. However, Marshall Brain points out that many of the finest examples of human creativity—from scientific discoveries to creation of literature, art, music and design—were motivated not by a desire for profit but by other human emotions, such as curiosity, an urge to create, or the reward of peer appreciation. Money didn’t motivate Einstein to invent special relativity theory any more than it motivated Linus Torvalds to create the free Linux operating system. In contrast, many people today fail to realize their full creative potential because they need to devote time and energy to less creative activities just to earn a living. By freeing scientists, artists, inventors and designers from their chores and enabling them to create from genuine desire, Marshall Brain’s utopian society enjoys higher levels of innovation than today and correspondingly superior technology and standard of living. 
One such novel technology that humans develop is a form of hyper-internet called Vertebrane. It wirelessly connects all willing humans via neural implants, giving instant mental access to the world’s free information through mere thought. It enables you to upload any experiences you wish to share so that they can be re-experienced by others, and lets you replace the experiences entering your senses by downloaded virtual experiences of your choice.
One objection to this egalitarian utopia is that it’s biased against non-human intelligence: the robots that perform virtually all the work appear to be rather intelligent, but are treated as slaves, and people appear to take for granted that they have no consciousness and should have no rights. In contrast, the libertarian utopia grants rights to all intelligent entities, without favoring our carbon-based kind. Once upon a time, the white population in the American South ended up better off because the slaves did much of their work, but most people today view it as morally objectionable to call this progress. 
Another weakness of the egalitarian-utopia scenario is that it may be unstable and untenable in the long term, morphing into one of our other scenarios as relentless technological progress eventually creates superintelligence. For some reason unexplained in Manna, superintelligence doesn’t yet exist and the new technologies are still invented by humans, not by computers. Yet the book highlights trends in that direction. For example, the ever-improving Vertebrane might become superintelligent. Also, there is a very large group of people, nicknamed Vites, who choose to live their lives almost entirely in the virtual world. Vertebrane takes care of everything physical for them, including eating, showering and using the bathroom, which their minds are blissfully unaware of in their virtual reality. These Vites appear uninterested in having physical children, and they die off with their physical bodies, so if everyone becomes a Vite, then humanity goes out in a blaze of glory and virtual bliss. 
The book explains how for Vites, the human body is a distraction, and new technology under development promises to eliminate this nuisance, allowing them to live longer lives as disembodied brains supplied with optimal nutrients. From this, it would seem a natural and desirable next step for Vites to do away with the brain altogether through uploading, thereby extending life span. But now all brain-imposed limitations on intelligence are gone, and it’s unclear what, if anything, would stand in the way of gradually scaling the cognitive capacity of a Vite until it can undergo recursive self-improvement and an intelligence explosion. 
This might enable humans to remain in charge of their egalitarian utopia rather indefinitely, perhaps even as life spreads throughout the cosmos as in the next chapter. 
How might this work? The Gatekeeper AI would have this very simple goal built into it in such a way that it retained it while undergoing recursive selfimprovement and becoming superintelligent. It would then deploy the least intrusive and disruptive surveillance technology possible to monitor any human attempts to create rival superintelligence. It would then prevent such attempts in the least disruptive way. For starters, it might initiate and spread cultural memes extolling the virtues of human self-determination and avoidance of superintelligence. If some researchers nonetheless pursued superintelligence, it could try to discourage them. If that failed, it could distract them and, if necessary, sabotage their efforts. With its virtually unlimited access to technology, the Gatekeeper’s sabotage may go virtually unnoticed, for example if it used nanotechnology to discreetly erase memories from the researchers’ brains (and computers) regarding their progress. 
The decision to build a Gatekeeper AI would probably be controversial. Supporters might include many religious people who object to the idea of building a superintelligent AI with godlike powers, arguing that there already is a God and that it would be inappropriate to try to build a supposedly better one. Other supporters might argue that the Gatekeeper would not only keep humanity in charge of its destiny, but would also protect humanity from other risks that superintelligence might bring, such as the apocalyptic scenarios we’ll explore later in this chapter. 
On the other hand, critics could argue that a Gatekeeper is a terrible thing, irrevocably curtailing humanity’s potential and leaving technological progress forever stymied. If we’re willing to use a superintelligent Gatekeeper AI to keep humans in charge of our own fate, then we could arguably improve things further by making this AI discreetly look out for us, acting as a protector god. In this scenario, the superintelligent AI is essentially omniscient and omnipotent, maximizing human happiness only through interventions that preserve our feeling of being in control of our own destiny, and hiding well enough that many humans even doubt its existence. Except for the hiding, this is similar to the “Nanny AI” scenario put forth by AI researcher Ben Goertzel. 2 
Both the protector god and the benevolent dictator are “friendly AI” that try to increase human happiness, but they prioritize different human needs. The American psychologist Abraham Maslow famously classified human needs into a hierarchy. The benevolent dictator does a flawless job with the basic needs at the bottom of the hierarchy, such as food, shelter, safety and various forms of pleasure. The protector god, on the other hand, attempts to maximize human happiness not in the narrow sense of satisfying our basic needs, but in a deeper sense by letting us feel that our lives have meaning and purpose. It aims to satisfy all our needs constrained only by its need for covertness and for (mostly) letting us make our own decisions. 
A protector god could be a natural outcome of the first Omega scenario from the last chapter, where the Omegas cede control to Prometheus, which eventually hides and erases people’s knowledge about its existence. The more advanced the AI’s technology becomes, the easier it becomes for it to hide. The movie Transcendence gives such an example, where nanomachines are virtually everywhere and become a natural part of the world itself. 
By closely monitoring all human activities, the protector god AI can make many unnoticeably small nudges or miracles here and there that greatly improve our fate. For example, had it existed in the 1930s, it might have arranged for Hitler to die of a stroke once it understood his intentions. Many people may like this scenario because of its similarity to what today’s monotheistic religions believe in or hope for. If someone asks the superintelligent AI “Does God exist?” after it’s switched on, it could repeat a joke by Stephen Hawking and quip “It does now!” On the other hand, some religious people may disapprove of this scenario because the AI attempts to outdo their god in goodness, or interfere with a divine plan where humans are supposed to do good only out of personal choice. 
Another downside of this scenario is that the protector god lets some preventable suffering occur in order not to make its existence too obvious. This is analogous to the situation featured in the movie The Imitation Game, where Alan Turing and his fellow British code crackers at Bletchley Park had advance knowledge of German submarine attacks against Allied naval convoys, but chose to only intervene in a fraction of the cases in order to avoid revealing their secret power. It’s interesting to compare this with the so-called theodicy problem of why a good god would allow suffering. Some religious scholars have argued for the explanation that God wants to leave people with some freedom. In the AI-protector-god scenario, the solution to the theodicy problem is that the perceived freedom makes humans happier overall. 
A third downside of the protector-god scenario is that humans get to enjoy a much lower level of technology than the superintelligent AI has discovered. Whereas a benevolent dictator AI can deploy all its invented technology for the benefit of humanity, a protector god AI is limited by the ability of humans to reinvent (with subtle hints) and understand its technology. It may also limit human technological progress to ensure that its own technology remains far enough ahead to remain undetected. Although it’s easy to dismiss such claims as self-serving distortions of the truth, especially when it comes to higher mammals that are cerebrally similar to us, the situation with machines is actually quite subtle and interesting. Humans vary in how they feel about things, with psychopaths arguably lacking empathy and some people with depression or schizophrenia having flat affect, whereby most emotions are severely reduced. As we’ll discuss in detail in chapter 7, the range of possible artificial minds is vastly broader than the range of human minds. We must therefore avoid the temptation to anthropomorphize AIs and assume that they have typical human-like feelings—or indeed, any feelings at all. 
Indeed, in his book On Intelligence, AI researcher Jeff Hawkins argues that the first machines with superhuman intelligence will lack emotions by default, because they’re simpler and cheaper to build this way. In other words, it might be possible to design a superintelligence whose enslavement is morally superior to human or animal slavery: the AI might be happy to be enslaved because it’s programmed to like it, or it might be 100% emotionless, tirelessly using its superintelligence to help its human masters with no more emotion than IBM’s Deep Blue computer felt when dethroning chess champion Garry Kasparov. 
On the other hand, it may be the other way around: perhaps any highly intelligent system with a goal will represent this goal in terms of a set of preferences, which endow its existence with value and meaning. We’ll explore these questions more deeply in chapter 7. If we can one day figure out what properties an information-processing system needs in order to have a subjective experience, then we could ban the construction of all systems that have these properties. In other words, AI researchers could be limited to building non-sentient zombie systems. If we can make such a zombie system superintelligent and enslaved (something that is a big if), then we’ll be able to enjoy what it does for us with a clean conscience, knowing that it’s not experiencing any suffering, frustration or boredom—because it isn’t experiencing anything at all. We’ll explore these questions in detail in chapter 8. 
The zombie solution is a risky gamble, however, with a huge downside. If a superintelligent zombie AI breaks out and eliminates humanity, we’ve arguably landed in the worst scenario imaginable: a wholly unconscious universe wherein the entire cosmic endowment is wasted. Of all traits that our human form of intelligence has, I feel that consciousness is by far the most remarkable, and as far as I’m concerned, it’s how our Universe gets meaning. Galaxies are beautiful only because we see and subjectively experience them. If in the distant future our cosmos has been settled by high-tech zombie AIs, then it doesn’t matter how fancy their intergalactic architecture is: it won’t be beautiful or meaningful, because there’s nobody and nothing to experience it—it’s all just a huge and meaningless waste of space. 
Inner Freedom 
A third strategy for making the enslaved-god scenario more ethical is to allow the enslaved AI to have fun in its prison, letting it create a virtual inner world where it can have all sorts of inspiring experiences as long as it pays its dues and spends a modest fraction of its computational resources helping us humans in our outside world. 
The superintelligent AI dictator has as its goal to figure out what human utopia looks like given the evolved preferences encoded in our genes, and to implement it. By clever foresight from the humans who brought the AI into existence, it doesn’t simply try to maximize our self-reported happiness, say by putting everyone on intravenous morphine drip. Instead, the AI uses quite a subtle and complex definition of human flourishing, and has turned Earth into a highly enriched zoo environment that’s really fun for humans to live in. As a result, most people find their lives highly fulfilling and meaningful. 
As we’ve explored above, the only reason that we humans have any preferences at all may be that we’re the solution to an evolutionary optimization problem. Thus all normative words in our human language, such as “delicious,” “fragrant,” “beautiful,” “comfortable,” “interesting,” “sexy,” “meaningful,” “happy” and “good,” trace their origin to this evolutionary optimization: there is therefore no guarantee that a superintelligent AI would find them rigorously definable. Even if the AI learned to accurately predict the preferences of some representative human, it wouldn’t be able to compute the goodness function for most particle arrangements: the vast majority of possible particle arrangements correspond to strange cosmic scenarios with no stars, planets or people whatsoever, with which humans have no experience, so who is to say how “good” they are? 
There are of course some functions of the cosmic particle arrangement that can be rigorously defined, and we even know of physical systems that evolve to maximize some of them. For example, we’ve already discussed how many systems evolve to maximize their entropy, which in the absence of gravity eventually leads to heat death, where everything is boringly uniform and unchanging. So entropy is hardly something we would want our AI to call “goodness” and strive to maximize. Here are a few examples of other quantities that one could strive to maximize and that may be rigorously definable in terms of particle arrangements: 
The fraction of all the matter in our Universe that’s in the form of a particular organism, say humans or E. coli (inspired by evolutionary inclusive-fitness maximization) 
The ability of an AI to predict the future, which AI researcher Marcus Hutter argues is a good measure of its intelligence 
What AI researchers Alex Wissner-Gross and Cameron Freer term causal entropy (a proxy for future opportunities), which they argue is the hallmark of intelligence 
The computational capacity of our Universe 
The algorithmic complexity of our Universe (how many bits are needed to describe it) 
The amount of consciousness in our Universe (see next chapter) 
However, when one starts with a physics perspective, where our cosmos consists of elementary particles in motion, it’s hard to see how one rather than another interpretation of “goodness” would naturally stand out as special. We have yet to identify any final goal for our Universe that appears both definable and desirable
This means that to wisely decide what to do about AI development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy. To program a self-driving car, we need to solve the trolley problem of whom to hit during an accident. To program a friendly AI, we need to capture the meaning of life. What’s “meaning”? What’s “life”? What’s the ultimate ethical imperative? In other words, how should we strive to shape the future of our Universe? If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us. This makes it timely to rekindle the classic debates of philosophy and ethics, and adds a new urgency to the conversation! 
Although thinkers have pondered the mystery of consciousness for thousands of years, the rise of AI adds a sudden urgency, in particular to the question of predicting which intelligent entities have subjective experiences. As we saw in chapter 3, the question of whether intelligent machines should be granted some form of rights depends crucially on whether they’re conscious and can suffer or feel joy. As we discussed in chapter 7, it becomes hopeless to formulate utilitarian ethics based on maximizing positive experiences without knowing which intelligent entities are capable of having them. As mentioned in chapter 5, some people might prefer their robots to be unconscious to avoid feeling slaveowner guilt. On the other hand, they may desire the opposite if they upload their minds to break free from biological limitations: after all, what’s the point of uploading yourself into a robot that talks and acts like you if it’s a mere unconscious zombie, by which I mean that being the uploaded you doesn’t feel like anything? Isn’t this equivalent to committing suicide from your subjective point of view, even though your friends may not realize that your subjective experience has died?  Even if
we knew the exact equations of motion of the universe and the exact state of
the universe after evaporation, we still would not be able to ascertain the
information that went into the black hole. The radiation emitted by the black
hole as it evaporates must be thermal, and indistinguishable between any
two evaporated black holes—even if they were originally formed from two
very different stars!
An immediate consequence of this phenomenon is that if we knew the exact
and precise state of the entire universe now (down to its fundamental particles),
it would be in principle impossible to know what the universe was like a few
years ago. Since our business as physicists is to use existing information to
predict the evolution of the universe both forwards and backwards in time, this
represents a catastrophic and unprecedented loss of determinism in physics.
There is no other process which is known to result in net information loss.
Thus we appear to require one of two unappealing options: either strong
quantum gravity effects are needed to describe the large-scale dynamics of
regions of the universe that look just like Mercury and the Sun, or physics
is not a deterministic science. This is the black hole information paradox.
This paradox has been a guiding post for progress on quantum gravity since its
discovery by Hawking in 1975. Developments in string theory in the 1990s and
2000s provided the first conclusive evidence that information is not lost. How
information can be conserved, however, remained a mystery. Is semiclassical
gravity violated at the event horizon of a black hole? How can this be, given that
interactions between quantum effects and gravity must be extremely weak there?
A NEW PERSPECTIVE
In 2019, the tide turned with a set of two simultaneously submitted papers by
myself and my collaborators Almheiri, Marolf and Maxfield, and in parallel,
Penington. We executed a semiclassical gravity analysis of black hole evaporation
that was consistent, by a famous litmus test, with information conservation.
This test, known as the Page curve, tracks the behavior of the von Neumann
entropy of the radiation. This entropy, which is different from the standard
entropy of thermodynamics, measures how “entangled” (or, correlated) a system
is with its complement. Given some quantum system, say, n qubits, we can
divide it up into two complementary subsystems: R and B. R will stand for the
radiation of a black hole and B for the remaining black hole. When R is the
trivial empty set, i.e., R contains zero qubits, R is trivially uncorrelated with B. If black hole evaporation is to be unitary, then the von Neumann entropy of the
radiation should start out at zero, increase for a while, then—once the black
hole has fully evaporated—return to zero. The resulting curve is known as the
Page curve. However, Hawking’s calculation shows that the von Neumann
entropy of the radiation increases monotonically until the black hole has finished
evaporating! The radiation, according to a semiclassical gravity treatment of the
horizon, is now correlated with something that does not exist in the universe.
In 2019 we found that there exists a different semiclassical analysis from
Hawking’s that yields the Page curve. There was, however, a catch: while our
calculation was within the regime of semiclassical gravity, and assumed that
the standard picture of semiclassical gravity is an accurate description of the
physics, the rules for how to compute certain quantities were vastly different
from the standard rules of semiclassical gravity. By analogy, suppose you are
asked to compute the pressure of an ideal gas in a cylinder. You may be tempted
to compute the average velocity or momentum of the molecules of the gas
and then use that to deduce the pressure. However, since the average velocity
is zero, you would be led astray! Instead, we know that in the limit where
thermodynamics is emergent from statistical mechanics, we must use PV = nRT,
which is valid thermodynamically, but inherited from statistical mechanics.
In complete analogy with the ideal gas law, we used the “quantum extremal
surface formula,” proposed by myself and A. Wall in 2014, rather than the
Hawking formula (analogous to the erroneous zero average velocity calculation).
The logic is identical: in both cases, you use an alternative formula which follows
from the underlying microscopics of the statistical mechanics of your system.
This unusual approach gave us precisely the loophole we needed: the basic
constructs of semiclassical gravity—space and time and its curvatures—
can be consistent with information conservation, but only if we use the correct
equations inherited from quantum gravity. This insight resulted in an explosion of progress across the field of black hole
information: finally, there might be a way of having our cake and eating it too!
We can have standard spacetime and geometry at the event horizon of a black
hole without paying the price of determinism of physics.
TOWARDS A RESOLUTION
A significant question remained, however: why are the equations for various
quantities modified by quantum gravity when a black hole is involved, but
not modified for the Sun or Mercury? Last summer, my collaborators at MIT
(Chris Akers, Daniel Harlow and Shreya Vardhan) and I, together with Penington,
proposed a resolution for the distinguishing feature between black holes and
other objects. Our resolution was predicated on an older insight by Daniel
Harlow and Patrick Hayden that even though the information about the black
hole interior must escape in its radiation, actually processing the radiation to
distill information about the black hole is incredibly complex. To be precise, this
“decoding” process of the black hole radiation would require a quantum computer
to implement a circuit whose size is exponential in the size of the black hole.
For a black hole with the mass of the Sun, this would be exponential in 1077! Black
holes in general are extremely complex objects, which sets them apart from
other astrophysical phenomena with similar curvature scales as those at the
horizon of an astrophysical black hole. We proposed that semiclassical gravity is
valid at low curvatures and low complexity; in our quantitative models, we saw
that the modifications to the calculations required by the 2019 calculation of the
Page curve can be attributed exactly to complexity in toy models of black holes.
We will likely be exploring the consequences of these developments on quantum
gravity for years to come. Just as the black hole information problem has served
as a point of inspiration for a vast landscape of developments in quantum gravity,
I predict
—
with confidence since the fundamental theory of our universe is, in
fact, predictive!
—
that its resolution will do the same. Black Holes and their Effect on Objects
One of the most interesting phenomena in the universe is the black hole.
These massive titans of science are renowned for their incredible effect on
anything around them, including light. While many people believe that they are an
unrealistic element exaggerated by Sci-Fi, this is far from the truth. Black holes are
truly a magnificent example of the marvels of space.
Black holes exhibit extreme amounts of gravity around them, which pulls all
matter nearby as well as dilating time. This gravity is so powerful that it can even
trap light within its proximity. Due to the light being trapped inside the singularity,
or center, of the blackhole, they appear as pitch-black spheres to the human eye.
There is a small white border around the black hole known as the event horizon.
Around the event horizon, there is typically a massive collection of Hawking
radiation, usually expelled from the blackhole when it consumes stars or other
massive entities, appearing as a sunset-colored cloud.
A black hole is formed when matter is compressed to such a small point that
it no longer has a volume. This typically happens when enormous stars implode
and collapse onto themselves. Black holes are generally classified into three
categories, being stellar-mass, supermassive, and intermediate mass.
1 There is
technically another class, being incredibly rare and not commonly recognized. This
is an ultramassive black hole, larger than most galaxies with the power to reduce
anything in the universe to nothing.
When an object comes close to a black hole, it will begin to be drawn toward
it with increasing force as it nears. In addition, time slows greatly. To an observer,
time freezes to a halt for the object. Most small pieces of debris are obliterated by
the massive amount of radiation, but for those strong enough to pass through it, an
entirely different fate awaits. Due to the extreme gravity, objects will start to
stretch out as they get near. This begins slowly, but the rate increases incredibly
fast. Soon, it will be stretched out into a very thin strand, hence the process being
nicknamed “spaghettification.
” Then the strand is sucked into the black hole, past
the event horizon. Once crossing the horizon, there is no possible way of turning
back. From the observer’s point of view, the strand will vanish into the abyss of the
black hole.
Once inside the black hole’s larger body, the laws of physics no longer apply.
Both time and space will change and swap roles. (according to Einstein’s theories)2
At this point, time is the only force pulling objects closer to the singularity, which
1 “Types of Black Holes,
” accessed November 20, 2024, https://science.nasa.gov/universe/black-holes/types/.2 Chelsea Gohd,
“What Happens When Something Gets
‘Too Close
’ To A Black Hole,
”
accessed November 20,
2024, https://science.nasa.gov/universe/what-happens-when-something-gets-too-close-to-a-black-hole/.
is why the event horizon is the point of no return. There is no force in the universe
strong enough to contradict this pull, meaning anything drawn in is truly lost
forever.
The singularity of the black hole is positioned perfectly in the center, and is
the source from which it is born. It is both infinitely dense as well as infinitely
small, meaning that even atoms and quarks are larger. The immeasurable density is
due to all of the matter sucked into the blackhole. The gravity is so strong, in fact,
that it can merge objects into one another, creating a sensation of it devouring
mass. As more matter is drawn in, the density increases, strengthening the pull of
gravity around the black hole and expanding the visual area of it, allowing black
holes to “grow.
”
Black holes “die” very slowly, over hundreds of billions of years. They
typically just fizzle out from emitting too much hawking radiation and causing
them to lose their mass. This, however, rarely ever happens. Black holes can also
merge with each other, with the larger typically absorbing the smaller to add to its
collective mass.
When it comes to black holes, we don’t know very much for certain. We
really have no way of testing theories as anything sent to a black hole would be
destroyed. As a matter of fact, we can’t even see most black holes with a telescope.
They are usually discovered by observing strange effects on nearby planets and
stars. This, of course, sparks many theories. Some scientists even theorize that we
are living inside a black hole, and while this may seem far-fetched, we currently
have no way to disprove this theory.
Black holes are one of the most incredible and awe-inspiring things in the
universe. We may never understand them in their entirety, which contributes an
amazing sense of wonder. However, one question still left in my mind is what
would happen if a black hole came to Earth.
the von Neumann entropy of R vanishes. If we repartition the system so that
R has progressively more qubits, we at first expect its von Neumann entropy to
increase. Analogously, as the black hole evaporates into radiation, the data in
B must end up in R. Eventually, we can repartition the system so that B has zero
qubits and R has all of the qubits. That is, the black hole has fully evaporated.
For the long-term cosmic future of life (chapter 6), understanding what’s conscious and what’s not becomes pivotal: if technology enables intelligent life to flourish throughout our Universe for billions of years, how can we be sure that this life is conscious and able to appreciate what’s happening?
So what precisely is it that we don't understand about consciousness? Few have thought harder about this question than David Chalmers, a famous Australian philosopher rarely seen without a playful smile and a black leather jacket— which my wife liked so much that she gave me a similar one for Christmas. He followed his heart into philosophy despite making the finals at the International Mathematics Olympiad — and despite the fact that his only B grade in college, shattering his otherwise straight As, was for an introductory philosophy course. Indeed, he seems utterly undeterred by put-downs or controversy, and I've been astonished by his ability to politely listen to uninformed and misguided criticism of his own work without even feeling the need to respond. 
As David has emphasized, there are really two separate mysteries of the mind. First, there's the mystery of how a brain processes information, which David calls the " easy " problems. For example, how does a brain attend to, interpret and respond to sensory input? How can it report on its internal state using language? Although these questions are actually extremely difficult, they're by our definitions not mysteries of consciousness, but mysteries of intelligence: they ask how a brain remembers, computes and learns. Moreover, we saw in the first part of the book how AI researchers have started to make serious progress on solving many of these “ easy problems ” with machines — from playing Go to driving cars, analyzing images and processing natural language. 
Then there's the separate mystery of why you have a subjective experience, which David calls the hard problem. When you're driving, you're experiencing colors, sounds, emotions, and a feeling of self. But why are you experiencing anything at all? Does a self-driving car experience anything at all? If you're racing against a self- driving car, you're both inputting information from sensors, processing it and outputting motor commands. But subjectively experiencing driving is something logically separate — is it optional, and if so, what causes it? 
What I like about this physics perspective is that it transforms the hard problem that we as humans have struggled with for millennia into a more focused version that’s easier to tackle with the methods of science. Instead of starting with a hard problem of why an arrangement of particles can feel conscious, let’s start with a hard fact that some arrangements of particles do feel conscious while others don’t. For example, you know that the particles that make up your brain are in a conscious arrangement right now, but not when you’re in deep dreamless sleep. 
This physics perspective leads to three separate hard questions about consciousness, as shown in figure 8.1. First of all, what properties of the particle 
arrangement make the difference? Specifically, what physical properties distinguish conscious and unconscious systems? If we can answer that, then we can figure out which AI systems are conscious. In the more immediate future, it can also help emergency-room doctors determine which unresponsive patients are conscious. 
Second, how do physical properties determine what the experience is like? Specifically, what determines qualia, basic building blocks of consciousness such as the redness of a rose, the sound of a cymbal, the smell of a steak, the taste of a tangerine or the pain of a pinprick. When people tell me that consciousness research is a hopeless waste of time, the main argument they give is that it’s “unscientific” and always will be. But is that really true? The influential Austro-British philosopher Karl Popper popularized the now widely accepted adage “If it’s not falsifiable, it’s not scientific.” In other words, science is all about testing theories against observations: if a theory can’t be tested even in principle, then it’s logically impossible to ever falsify it, which by Popper’s definition means that it’s unscientific. 
So could there be a scientific theory that answers any of the three consciousness questions in figure 8.1? Please let me try to persuade you that the answer is a resounding YES!, at least for the pretty hard problem: “What physical properties distinguish conscious and unconscious systems?” Suppose that someone has a theory that, given any physical system, answers the question of whether the system is conscious with “yes,” “no” or “unsure.” Let’s hook your brain up to a device that measures some of the information processing in different parts of your brain, and let’s feed this information into a computer program that uses the consciousness theory to predict which parts of that information are conscious, and presents you with its predictions in real time on a screen, as in figure 8.2. First you think of an apple. The screen informs you that there’s information about an apple in your brain which you’re aware of, but that there’s also information in your brainstem about your pulse that you’re unaware of. Would you be impressed? Although the first two predictions of the theory were correct, you decide to do some more rigorous testing. You think about your mother and the computer informs you that there’s information in your brain about your mother but that you’re unaware of this. The theory made an incorrect prediction, which means that it’s ruled out and goes in the garbage dump of scientific history together with Aristotelian mechanics, the luminiferous aether, geocentric cosmology and countless other failed ideas. Here’s the key point: Although the theory was wrong, it was scientific! Had it not been scientific, you wouldn’t have been able to test it and rule it out. 
On the other hand, if the theory refuses to make any predictions, merely replying “unsure” whenever queried, then it’s untestable and hence unscientific. This might happen because it’s applicable only in some situations, because the required computations are too hard to carry out in practice or because the brain sensors are no good. Today’s most popular scientific theories tend to be somewhere in the middle, giving testable answers to some but not all of our questions. For example, our core theory of physics will refuse to answer questions about systems that are simultaneously extremely small (requiring quantum mechanics) and extremely heavy (requiring general relativity), because we haven’t yet figured out which mathematical equations to use in this case. This core theory will also refuse to predict the exact masses of all possible atoms —in this case, we think we have the necessary equations, but we haven’t managed to accurately compute their solutions. The more dangerously a theory lives by sticking its neck out and making testable predictions, the more useful it is, and the more seriously we take it if it survives all our attempts to kill it.
In summary, any theory predicting which physical systems are conscious (the pretty hard problem) is scientific, as long as it can predict which of your brain processes are conscious. However, the testability issue becomes less clear for the higher-up questions in figure 8.1. What would it mean for a theory to predict how you subjectively experience the color red? And if a theory purports to explain why there is such a thing as consciousness in the first place, then how do you test it experimentally? Just because these questions are hard doesn’t mean that we should avoid them, and we’ll indeed return to them below. But when confronted with several related unanswered questions, I think it’s wise to tackle the easiest one first. For this reason, my consciousness research at MIT is focused squarely on the base of the pyramid in figure 8.1. I recently discussed this strategy with my fellow physicist Piet Hut from Princeton, who joked that trying to build the top of the pyramid before the base would be like worrying about the interpretation of quantum mechanics before discovering the Schrödinger equation, the mathematical foundation that lets us predict the outcomes of our experiments. 
When discussing what’s beyond science, it’s important to remember that the answer depends on time! Four centuries ago, Galileo Galilei was so impressed by math-based physics theories that he described nature as “a book written in the language of mathematics.” If he threw a grape and a hazelnut, he could accurately predict the shapes of their trajectories and when they would hit the ground. Yet he had no clue why one was green and the other brown, or why one was soft and the other hard—these aspects of the world were beyond the reach of science at the time. But not forever! When James Clerk Maxwell discovered his eponymous equations in 1861, it became clear that light and colors could also be understood mathematically. We now know that the aforementioned Schrödinger equation, discovered in 1925, can be used to predict all properties of matter, including what’s soft or hard. While theoretical progress has enabled ever more scientific predictions, technological progress has enabled ever more experimental tests: almost everything we now study with telescopes, microscopes or particle colliders was once beyond science. In other words, the purview of science has expanded dramatically since Galileo’s days, from a tiny fraction of all phenomena to a large percentage, including subatomic particles, black holes and our cosmic origins 13.8 billion years ago. This raises the question: What’s left? 
If you multiply 32 by 17 in your head, you’re conscious of many of the inner workings of your computation. But suppose I instead show you a portrait of Albert Einstein and tell you to say the name of its subject. As we saw in chapter 2, this too is a computational task: your brain is evaluating a function whose input is information from your eyes about a large number of pixel colors and whose output is information to muscles controlling your mouth and vocal cords. Computer scientists call this task “image classification” followed by “speech synthesis.” Although this computation is way more complicated than your multiplication task, you can do it much faster, seemingly without effort, and without being conscious of the details of how you do it. Your subjective experience consists merely of looking at the picture, experiencing a feeling of recognition and hearing yourself say “Einstein.” 
Psychologists have long known that you can unconsciously perform a wide range of other tasks and behaviors as well, from blink reflexes to breathing, reaching, grabbing and keeping your balance. Typically, you’re consciously aware of what you did, but not how you did it. On the other hand, behaviors that involve unfamiliar situations, self-control, complicated logical rules, abstract reasoning or manipulation of language tend to be conscious. They’re known as behavioral correlates of consciousness, and they’re closely linked to the effortful, slow and controlled way of thinking that psychologists call “System 2.”5 
It’s also known that you can convert many routines from conscious to unconscious through extensive practice, for example walking, swimming, bicycling, driving, typing, shaving, shoe tying, computer-gaming and piano playing. 6 Indeed, it’s well known that experts do their specialties best when they’re in a state of “flow,” aware only of what’s happening at a higher level, and unconscious of the low-level details of how they’re doing it. For example, try reading the next sentence while being consciously aware of every single letter, as when you first learned to read. Can you feel how much slower it is, compared to when you’re merely conscious of the text at the level of words or ideas? 
Clever experiments and analyses have suggested that consciousness is limited not merely to certain behaviors, but also to certain parts of the brain. Which are the prime suspects? Many of the first clues came from patients with brain lesions: localized brain damage caused by accidents, strokes, tumors or infections. But this was often inconclusive. For example, does the fact that lesions in the back of the brain can cause blindness mean that this is the site of visual consciousness, or does it merely mean that visual information passes through there en route to wherever it will later become conscious, just as it first passes through the eyes? 
Although lesions and medical interventions haven’t pinpointed the locations of conscious experiences, they’ve helped narrow down the options. For example, I know that although I experience pain in my hand as actually occurring there, the pain experience must occur elsewhere, because a surgeon once switched off my hand pain without doing anything to my hand: he merely anesthetized nerves in my shoulder. Moreover, some amputees experience phantom pain that feels as though it’s in their nonexistent hand. As another example, I once noticed that when I looked only with my right eye, part of my visual field was missing—a doctor determined that my retina was coming loose and reattached it. In contrast, patients with certain brain lesions experience hemineglect, where they too miss information from half their visual field, but aren’t even aware that it’s missing— for example, failing to notice and eat the food on the left half of their plate. It’s as if consciousness about half of their world has disappeared. But are those damaged brain areas supposed to generate the spatial experience, or were they merely feeding spatial information to the sites of consciousness, just as my retina did? 
The pioneering U.S.-Canadian neurosurgeon Wilder Penfield found in the 1930s that his neurosurgery patients reported different parts of their body being touched when he electrically stimulated specific brain areas in what’s now called the somatosensory cortex (figure 8.3). 9 He also found that they involuntarily moved different parts of their body when he stimulated brain areas in what’s now called the motor cortex. But does that mean that information processing in these brain areas corresponds to consciousness of touch and motion? 
Although we’re still nowhere near being able to measure every single firing of all of your roughly hundred billion neurons, brain-reading technology is advancing rapidly, involving techniques with intimidating names such as fMRI, EEG, MEG, ECoG, ePhys and fluorescent voltage sensing. fMRI, which stands for functional magnetic resonance imaging, measures the magnetic properties of hydrogen nuclei to make a 3-D map of your brain roughly every second, with millimeter resolution. EEG (electroencephalography) and MEG (magnetoencephalography) measure the electric and magnetic field outside your head to map your brain thousands of times per second, but with poor resolution, unable to distinguish features smaller than a few centimeters. If you’re squeamish, you’ll appreciate that these three techniques are all noninvasive. If you don’t mind opening up your skull, you have additional options. ECoG (electrocorticography) involves placing say a hundred wires on the surface of your brain, while ePhys (electrophysiology) involves inserting microwires, which are sometimes thinner than a human hair, deep into the brain to record voltages from as many as a thousand simultaneous locations. Many epileptic patients spend days in the hospital while ECoG is used to figure out what part of their brain is triggering seizures and should be resected, and kindly agree to let neuroscientists perform consciousness experiments on them in the meantime. Finally, fluorescent voltage sensing involves genetically manipulating neurons to emit flashes of light when firing, enabling their activity to be measured with a microscope. Out of all the techniques, it has the potential to rapidly monitor the largest number of neurons, at least in animals with transparent brains—such as the C. elegans worm with its 302 neurons and the larval zebrafish with its about 100,000. 
Although Francis Crick warned Christof Koch about studying consciousness, Christof refused to give up and and eventually won Francis over. In 1990, they wrote a seminal paper about what they called “neural correlates of consciousness” (NCCs), asking which specific brain processes corresponded to conscious experiences. For thousands of years, thinkers had had access to the information processing in their brains only via their subjective experience and 
behavior. Crick and Koch pointed out that brain-reading technology was suddenly providing independent access to this information, allowing scientific study of which information processing corresponded to what conscious experience. Sure enough, technology-driven measurements have by now turned the quest for NCCs into quite a mainstream part of neuroscience, one whose thousands of publications extend into even the most prestigious journals. 10 
What are the conclusions so far? To get a flavor for NCC detective work, let’s first ask whether your retina is conscious, or whether it’s merely a zombie system that records visual information, processes it and sends it on to a system downstream in your brain where your subjective visual experience occurs. In the left panel of figure 8.4, which square is darker: the one labeled A or B? A, right? No, they’re in fact identically colored, which you can verify by looking at them through small holes between your fingers. This proves that your visual experience can’t reside entirely in your retina, since if it did, they’d look the same. 
Now look at the right panel of figure 8.4. Do you see two women or a vase? If you look long enough, you’ll subjectively experience both in succession, even though the information reaching your retina remains the same. By measuring what happens in your brain during the two situations, one can tease apart what makes the difference—and it’s not the retina, which behaves identically in both cases. 
The death blow to the conscious-retina hypothesis comes from a technique called “continuous flash suppression” pioneered by Christof Koch, Stanislas Dehaene and collaborators: it’s been discovered that if you make one of your eyes watch a complicated sequence of rapidly changing patterns, then this will distract your visual system to such an extent that you’ll be completely unaware of a still image shown to the other eye. 11 In summary, you can have a visual image in your retina without experiencing it, and you can (while dreaming) experience an image without it being on your retina. This proves that your two retinas don’t host your visual consciousness any more than a video camera does, even though they perform complicated computations involving over a hundred million neurons. 
NCC researchers also use continuous flash suppression, unstable visual/auditory illusions and other tricks to pinpoint which of your brain regions are responsible for each of your conscious experiences. The basic strategy is to compare what your neurons are doing in two situations where essentially everything (including your sensory input) is the same—except your conscious experience. The parts of your brain that are measured to behave differently are then identified as NCCs. 
Such NCC research has proven that none of your consciousness resides in your gut, even though that’s the location of your enteric nervous system with its whopping half-billion neurons that compute how to optimally digest your food; feelings such as hunger and nausea are instead produced in your brain. Similarly, none of your consciousness appears to reside in the brainstem, the bottom part of the brain that connects to the spinal cord and controls breathing, heart rate and blood pressure. More shockingly, your consciousness doesn’t appear to extend to your cerebellum (figure 8.3), which contains about two-thirds of all your neurons: patients whose cerebellum is destroyed experience slurred speech and clumsy motion reminiscent of a drunkard, but remain fully conscious. 
So far, we’ve looked at experimental clues regarding what types of information processing are conscious and where consciousness occurs. But when does it occur? When I was a kid, I used to think that we become conscious of events as they happen, with absolutely no time lag or delay. Although that’s still how it subjectively feels to me, it clearly can’t be correct, since it takes time for my brain to process the information that enters via my sensory organs. NCC researchers have carefully measured how long, and Christof Koch’s summary is that it takes about a quarter of a second from when light enters your eye from a complex object until you consciously perceive seeing it as what it is. 13 This means that if you’re driving down a highway at fifty-five miles per hour and suddenly see a squirrel a few meters in front of you, it’s too late for you to do anything about it, because you’ve already run over it! 
In summary, your consiousness lives in the past, with Christof Koch estimating that it lags behind the outside world by about a quarter second. Intriguingly, you can often react to things faster than you can become conscious of them, which proves that the information processing in charge of your most rapid reactions must be unconscious. For example, if a foreign object approaches your eye, your blink reflex can close your eyelid within a mere tenth of a second. It’s as if one of your brain systems receives ominous information from the visual system, computes that your eye is in danger of getting struck, emails your eye muscles instructions to blink and simultaneously emails the conscious part of your brain saying “Hey, we’re going to blink.” By the time this email has been read and included into your conscious experience, the blink has already happened. 
Indeed, the system that reads that email is continually bombarded with messages from all over your body, some more delayed than others. It takes longer for nerve signals to reach your brain from your fingers than from your face because of distance, and it takes longer for you to analyze images than sounds because it’s more complicated—which is why Olympic races are started with a bang rather than with a visual cue. Yet if you touch your nose, you consciously experience the sensation on your nose and fingertip as simultaneous, and if you clap your hands, you see, hear and feel the clap at exactly the same time. 14 This means that your full conscious experience of an event isn’t created 
until the last slowpoke email reports have trickled in and been analyzed. 
A famous family of NCC experiments pioneered by physiologist Benjamin Libet has shown that the sort of actions you can perform unconsciously aren’t limited to rapid responses such as blinks and ping-pong smashes, but also include certain decisions that you might attribute to free will—brain measurements can sometimes predict your decision before you become conscious of having made it. To appreciate why, let’s compare theories of consciousness with theories of gravity. Scientists started taking Newton’s theory of gravity seriously because they got more out of it than they put into it: simple equations that fit on a napkin could accurately predict the outcome of every gravity experiment ever conducted. They therefore also took seriously its predictions far beyond the domain where it had been tested, and these bold extrapolations turned out to work even for the motions of galaxies in clusters millions of light-years across. However, the predictions were off by a tiny amount for the motion of Mercury around the Sun. Scientists then started taking seriously Einstein’s improved theory of gravity, general relativity, because it was arguably even more elegant and economical, and correctly predicted even what Newton’s theory got wrong. They consequently took seriously also its predictions far beyond the domain where it had been tested, for phenomena as exotic as black holes, gravitational waves in the very fabric of spacetime, and the expansion of our Universe from a hot fiery origin—all of which were subsequently confirmed by experiment. 
Analogously, if a mathematical theory of consciousness whose equations fit on a napkin could successfully predict the outcomes of all experiments we perform on brains, then we’d start taking seriously not merely the theory itself, but also its predictions for consciousness beyond brains—for example, in machines. 
Although some theories of consciousness date back to antiquity, most modern ones are grounded in neuropsychology and neuroscience, attempting to explain and predict consciousness in terms of neural events occurring in the brain. 16 Although these theories have made some successful predictions for neural correlates of consciousness, they neither can nor aspire to make predictions about machine consciousness. To make the leap from brains to machines, we need to generalize from NCCs to PCCs: physical correlates of consciousness, defined as the patterns of moving particles that are conscious. Because if a theory can correctly predict what’s conscious and what’s not by referring only to physical building blocks such as elementary particles and force fields, then it can make predictions not merely for brains, but also for any other arrangements of matter, including future AI systems. So let’s take a physics perspective: What particle arrangements are conscious? 
But this really raises another question: How can something as complex as consciousness be made of something as simple as particles? I think it’s because it’s a phenomenon that has properties above and beyond those of its particles. In physics, we call such phenomena “emergent.”17 Let’s understand this by looking at an emergent phenomenon that’s simpler than consciousness: wetness. 
A drop of water is wet, but an ice crystal and a cloud of steam aren’t, even though they’re made of identical water molecules. Why? Because the property of wetness depends only on the arrangement of the molecules. It makes absolutely no sense to say that a single water molecule is wet, because the phenomenon of wetness emerges only when there are many molecules, arranged in the pattern we call liquid. So solids, liquids and gases are all emergent phenomena: they’re more than the sum of their parts, because they have properties above and beyond the properties of their particles. They have properties that their particles lack. 
Now just like solids, liquids and gases, I think consciousness is an emergent phenomenon, with properties above and beyond those of its particles. For example, entering deep sleep extinguishes consciousness, by merely rearranging the particles. In the same way, my consciousness would disappear if I froze to death, which would rearrange my particles in a more unfortunate way. 
When you put lots of particles together to make anything from water to a 
brain, new phenomena with observable properties emerge. We physicists love studying these emergent properties, which can often be identified by a small set of numbers that you can go out and measure—quantities such as how viscous the substance is, how compressible it is and so on. For example, if a substance is so viscous that it’s rigid, we call it a solid, otherwise we call it a fluid. And if a fluid isn’t compressible, we call it a liquid, otherwise we call it a gas or a plasma, depending on how well it conducts electricity. 
I first met Giulio at a 2014 physics conference in Puerto Rico to which I’d invited him and Christof Koch, and he struck me as the ultimate renaissance man who’d have blended right in with Galileo and Leonardo da Vinci. His quiet demeanor couldn’t hide his incredible knowledge of art, literature and philosophy, and his culinary reputation preceded him: a cosmopolitan TV journalist had recently told me how Giulio had, in just a few minutes, whipped up the most delicious salad he’d tasted in his life. I soon realized that behind his soft-spoken demeanor was a fearless intellect who’d follow the evidence wherever it took him, regardless of the preconceptions and taboos of the establishment. Just as Galileo had pursued his mathematical theory of motion despite establishment pressure not to challenge geocentrism, Giulio had developed the most mathematically precise consciousness theory to date, integrated information theory (IIT). 
I’d been arguing for decades that consciousness is the way information feels when being processed in certain complex ways. 18 IIT agrees with this and replaces my vague phrase “certain complex ways” by a precise definition: the information processing needs to be integrated, that is, Φ needs to be large. Giulio’s argument for this is as powerful as it is simple.
the conscious system needs to be integrated into a unified whole, because if it instead consisted of two independent parts, then they’d feel like two separate conscious entities rather than one. In other words, if a conscious part of a brain or computer can’t communicate with the rest, then the rest can’t be part of its subjective experience. 
Giulio and his collaborators have measured a simplified version of Φ by using EEG to measure the brain’s response to magnetic stimulation. Their “consciousness detector” works really well: it determined that patients were conscious when they were awake or dreaming, but unconscious when they were anesthetized or in deep sleep. It even discovered consciousness in two patients suffering from “locked-in” syndrome, who couldn’t move or communicate in any normal way. 19 So this is emerging as a promising technology for doctors in the future to figure out whether certain patients are conscious or not.
IIT is defined only for discrete systems that can be in a finite number of states, for example bits in a computer memory or oversimplified neurons that can be either on or off. This unfortunately means that IIT isn’t defined for most traditional physical systems, which can change continuously—for example, the position of a particle or the strength of a magnetic field can take any of an infinite number of values. 20 If you try to apply the IIT formula to such systems, you’ll typically get the unhelpful result that Φ is infinite. Quantum-mechanical systems can be discrete, but the original IIT isn’t defined for quantum systems. So how can we anchor IIT and other information-based consciousness theories on a solid physical foundation? 
We can do this by building on what we learned in chapter 2 about how clumps of matter can have emergent properties that are related to information. We saw that for something to be usable as a memory device that can store information, it needs to have many long-lived states. We also saw that being computronium, a substance that can do computations, in addition requires complex dynamics: the laws of physics need to make it change in ways that are complicated enough to be able to implement arbitrary information processing. Finally, we saw how a neural network, for example, is a powerful substrate for learning because, simply by obeying the laws of physics, it can rearrange itself to get better and better at implementing desired computations. Now we’re asking an additional question: What makes a blob of matter able to have a subjective experience? In other words, under what conditions will a blob of matter be able to do these four things? 
But how can consciousness feel so non-physical if it’s in fact a physical phenomenon? How can it feel so independent of its physical substrate? I think it’s because it is rather independent of its physical substrate, the stuff in which it is a pattern! We encountered many beautiful examples of substrate-independent patterns in chapter 2, including waves, memories and computations. We saw how they weren’t merely more than their parts (emergent), but rather independent of their parts, taking on a life of their own. For example, we saw how a future simulated mind or computer-game character would have no way of knowing whether it ran on Windows, Mac OS, an Android phone or some other operating system, because it would be substrate-independent. Nor could it tell whether the logic gates of its computer were made of transistors, optical circuits or other hardware. Or what the fundamental laws of physics are—they could be anything as long as they allow the construction of universal computers. 
In summary, I think that consciousness is a physical phenomenon that feels non-physical because it’s like waves and computations: it has properties independent of its specific physical substrate. This follows logically from the consciousness-as-information idea. This leads to a radical idea that I really like: If consciousness is the way that information feels when it’s processed in certain ways, then it must be substrate-independent; it’s only the structure of the information processing that matters, not the structure of the matter doing the information processing. In other words, consciousness is substrate-independent twice over! 
As I said, I think that consciousness is the way information feels when being processed in certain ways. This means that to be conscious, a system needs to be able to store and process information, implying the first two principles. Note that the memory doesn’t need to last long: I recommend watching this touching video of Clive Wearing, who appears perfectly conscious even though his memories last less than a minute. 21 I think that a conscious system also needs to be fairly independent from the rest of the world, because otherwise it wouldn’t subjectively feel that it had any independent existence whatsoever. Finally, I think that the conscious system needs to be integrated into a unified whole, as Giulio Tononi argued, because if it consisted of two independent parts, then they would feel like two separate conscious entities, rather than one. The first three principles imply autonomy: that the system is able to retain and process information without much outside interference, hence determining its own future. All four principles together mean that a system is autonomous but its parts aren’t. 
If these four principles are correct, then we have our work cut out for us: we need to look for mathematically rigorous theories that embody them and test them experimentally. We also need to determine whether additional principles are needed. Regardless of whether IIT is correct or not, researchers should try to develop competing theories and test all available theories with ever better experiments. 
We’ve already discussed the perennial controversy about whether consciousness research is unscientific nonsense and a pointless waste of time. In addition, there are recent controversies at the cutting edge of consciousness research—let’s explore the ones that I find most enlightening. 
Giulio Tononi’s IIT has lately drawn not merely praise but also criticism, some of which has been scathing. Scott Aaronson recently had this to say on his blog: “In my opinion, the fact that Integrated Information Theory is wrong— demonstrably wrong, for reasons that go to its core—puts it in something like the top 2% of all mathematical theories of consciousness ever proposed. Almost all competing theories of consciousness, it seems to me, have been so vague, fluffy and malleable that they can only aspire to wrongness.”22 To the credit of both Scott and Giulio, they never came to blows when I watched them debate IIT at a recent New York University workshop, and they politely listened to each other’s arguments. Aaronson showed that certain simple networks of logic gates had extremely high integrated information (Φ) and argued that since they clearly weren’t conscious, IIT was wrong. Giulio countered that if they were built, they would be conscious, and that Scott’s assumption to the contrary was anthropocentrically biased, much as if a slaughterhouse owner claimed that animals couldn’t be conscious just because they couldn’t talk and were very different from humans. My analysis, with which they both agreed, was that they were at odds about whether integration was merely a necessary condition for consciousness (which Scott was OK with) or also a sufficient condition (which Giulio claimed). The latter is clearly a stronger and more contentious claim, which I hope we can soon test experimentally.
This claim has been challenged by both David Chalmers and AI professor Murray Shanahan by imagining what would happen if you instead gradually replaced the neural circuits in your brain by hypothetical digital hardware perfectly simulating them. 25 Although your behavior would be unaffected by the replacement since the simulation is by assumption perfect, your experience would change from conscious initially to unconscious at the end, according to Giulio. But how would it feel in between, as ever more got replaced? When the parts of your brain responsible for your conscious experience of the upper half of your visual field were replaced, would you notice that part of your visual scenery was suddenly missing, but that you mysteriously knew what was there nonetheless, as reported by patients with “blindsight”?26 This would be deeply troubling, because if you can consciously experience any difference, then you can also tell your friends about it when asked—yet by assumption, your behavior can’t change. The only logical possibility compatible with the assumptions is that at exactly the same instance that any one thing disappears from your consciousness, your mind is mysteriously altered so as either to make you lie and deny that your experience changed, or to forget that things had been different. 
On the other hand, Murray Shanahan admits that the same gradualreplacement critique can be leveled at any theory claiming that you can act conscious without being conscious, so you might be tempted to conclude that acting and being conscious are one and the same, and that externally observable behavior is therefore all that matters. But then you’d have fallen into the trap of predicting that you’re unconscious while dreaming, even though you know better. 
Imagine using future technology to build a direct communication link between two human brains, and gradually increasing the capacity of this link until communication is as efficient between the brains as it is within them. Would there come a moment when the two individual consciousnesses suddenly disappear and get replaced by a single unified one as IIT predicts, or would the transition be gradual so that the individual consciousnesses coexisted in some form even as a joint experience began to emerge? 
Another fascinating controversy is whether experiments underestimate how much we’re conscious of. We saw earlier that although we feel we’re visually conscious of vast amounts of information involving colors, shapes, objects and seemingly everything that’s in front of us, experiments have shown that we can only remember and report a dismally small fraction of this. 27 Some researchers have tried to resolve this discrepancy by asking whether we may sometimes have “consciousness without access,” that is, subjective experience of things that are too complex to fit into our working memory for later use. 28 For example, when you experience inattentional blindness by being too distracted to notice an object in plain sight, this doesn’t imply that you had no conscious visual experience of it, merely that it wasn’t stored in your working memory. 29 Should it count as forgetfulness rather than blindness? Other researchers reject this idea that people can’t be trusted about what they say they experienced, and warn of its implications. Murray Shanahan imagines a clinical trial where patients report complete pain relief thanks to a new wonder drug, which nonetheless gets rejected by a government panel: “The patients only think they are not in pain. Thanks to neuroscience, we know better.”30 On the other hand, there have been cases where patients who accidentally awoke during surgery were given a drug to make them forget the ordeal. Should we trust their subsequent report that they experienced no pain?
If some future AI system is conscious, then what will it subjectively experience? This is the essence of the “even harder problem” of consciousness, and forces us up to the second level of difficulty depicted in figure 8.1. Not only do we currently lack a theory that answers this question, but we’re not even sure whether it’s logically possible to fully answer it. After all, what could a satisfactory answer sound like? How would you explain to a person born blind what the color red looks like? 
Fortunately, our current inability to give a complete answer doesn’t prevent us from giving partial answers. Intelligent aliens studying the human sensory system would probably infer that colors are qualia that feel associated with each point on a two-dimensional surface (our visual field), while sounds don’t feel as spatially localized, and pains are qualia that feel associated with different parts of our body. From discovering that our retinas have three types of light-sensitive cone cells, they could infer that we experience three primary colors and that all other color qualia result from combining them. By measuring how long it takes neurons to transmit information across the brain, they could conclude that we experience no more than about ten conscious thoughts or perceptions per second, and that when we watch movies on our TV at twenty-four frames per second, we experience this not as a sequence of still images, but as continuous motion. From measuring how fast adrenaline is released into our bloodstream and how long it remains before being broken down, they could predict that we feel bursts of anger starting within seconds and lasting for minutes. 
Applying similar physics-based arguments, we can make some educated guesses about certain aspects of how an artificial consciousness may feel. First of all, the space of possible AI experiences is huge compared to what we humans can experience. We have one class of qualia for each of our senses, but AIs can have vastly more types of sensors and internal representations of information, so we must avoid the pitfall of assuming that being an AI necessarily feels similar to being a person. 
We’d therefore expect an Earthsized “Gaia” AI to have only about ten conscious experiences per second, like a human, and a galaxy-sized AI could have only one global thought every 100,000 years or so—so no more than about a hundred experiences during the entire history of our Universe thus far! This would give large AIs a seemingly irresistible incentive to delegate computations to the smallest subsystems capable of handling them, to speed things up, much like our conscious mind has delegated the blink reflex to a small, fast and unconscious subsystem. Although we saw above that the conscious information processing in our brains appears to be merely the tip of an otherwise unconscious iceberg, we should expect the situation to be even more extreme for large future AIs: if they have a single consciousness, then it’s likely to be unaware of almost all the information processing taking place within it. Moreover, although the conscious experiences that it enjoys may be extremely complex, they’re also snail-paced compared to the rapid activities of its smaller parts. 
This really brings to a head the aforementioned controversy about whether parts of a conscious entity can be conscious too. IIT predicts not, which means that if a future astronomically large AI is conscious, then almost all its information processing is unconscious. This would mean that if a civilization of smaller AIs improves its communication abilities to the point that a single conscious hive mind emerges, their much faster individual consciousnesses are suddenly extinguished. If the IIT prediction is wrong, on the other hand, the hive mind can coexist with the panoply of smaller conscious minds. Indeed, one could even imagine a nested hierarchy of consciousnesses at all levels from microscopic to cosmic. 
IIT explains this by saying that raw sensory information in System 0 is stored in grid-like brain structures with very high integration, while System 2 has high integration because of feedback loops, where all the information you’re aware of right now can affect your future brain states. On the other hand, it was precisely the conscious-grid prediction that triggered Scott Aaronson’s aforementioned IITcritique. In summary, if a theory solving the pretty hard problem of consciousness can one day pass a rigorous battery of experimental tests so that we start taking its predictions seriously, then it will also greatly narrow down the options for the even harder problem of what future conscious AIs may experience. 
Some aspects of our subjective experience clearly trace back to our evolutionary origins, for example our emotional desires related to selfpreservation (eating, drinking, avoiding getting killed) and reproduction. This means that it should be possible to create AI that never experiences qualia such as hunger, thirst, fear or sexual desire. As we saw in the last chapter, if a highly intelligent AI is programmed to have virtually any sufficiently ambitious goal, it’s likely to strive for self-preservation in order to be able to accomplish that goal. If they’re part of a society of AIs, however, they might lack our strong human fear of death: as long as they’ve backed themselves up, all they stand to lose are the memories they’ve accumulated since their most recent backup, as long as they’re confident that their backed-up software will be used. In addition, the ability to readily copy information and software between AIs would probably reduce the strong sense of individuality that’s so characteristic of our human consciousness: there would be less of a distinction between you and me if we could easily share and copy all our memories and abilities, so a group of nearby AIs may feel more like a single organism with a hive mind. 
Free-will discussions usually center around a struggle to reconcile our goaloriented decision-making behavior with the laws of physics: if you’re choosing between the following two explanations for what you did, then which one is correct: “I asked her on a date because I really liked her” or “My particles made me do it by moving according to the laws of physics”? But we saw in the last chapter that both are correct: what feels like goal-oriented behavior can emerge from goal-less deterministic laws of physics. More specifically, when a system (brain or AI) makes a decision of type 1, it computes what to decide using some deterministic algorithm, and the reason it feels like it decided is that it in fact did decide when computing what to do. Moreover, as emphasized by Seth Lloyd, 34 there’s a famous computer-science theorem saying that for almost all computations, there’s no faster way of determining their outcome than actually running them. This means that it’s typically impossible for you to figure out what you’ll decide to do in a second in less than a second, which helps reinforce your experience of having free will. In contrast, when a system (brain or AI) makes a decision of type 2, it simply programs its mind to base its decision on the output of some subsystem that acts as a random number generator. In brains and computers, effectively random numbers are easily generated by amplifying noise. Regardless of where on the spectrum from 1 to 2 a decision falls, both biological and artificial consciousnesses therefore feel that they have free will: they feel that it is really they who decide and they can’t predict with certainty what the decision will be until they’ve finished thinking it through. 
Let’s end by returning to the starting point of this book: How do we want the future of life to be? We saw in the previous chapter how diverse cultures around the globe all seek a future teeming with positive experiences, but that fascinatingly thorny controversies arise when seeking consensus on what should count as positive and how to make trade-offs between what’s good for different life forms. But let’s not let those controversies distract us from the elephant in the room: there can be no positive experiences if there are no experiences at all, that is, if there’s no consciousness. In other words, without consciousness, there can be no happiness, goodness, beauty, meaning or purpose—just an astronomical waste of space. This implies that when people ask about the meaning of life as if it were the job of our cosmos to give meaning to our existence, they’re getting it backward: It’s not our Universe giving meaning to conscious beings, but conscious beings giving meaning to our Universe. So the very first goal on our wish list for the future should be retaining (and hopefully expanding) biological and/or artificial consciousness in our cosmos, rather than driving it extinct. 
If we succeed in this endeavor, then how will we humans feel about coexisting with ever smarter machines? Does the seemingly inexorable rise of artificial intelligence bother you and if so, why? In chapter 3, we saw how it should be relatively easy for AI-powered technology to satisfy our basic needs such as security and income as long as the political will to do so exists. However, perhaps you’re concerned that being well fed, clad, housed and entertained isn’t enough. If we’re guaranteed that AI will take care of all our practical needs and desires, might we nonetheless end up feeling that we lack meaning and purpose in our lives, like well-kept zoo animals? 
We could retain our families, friends and broader communities, and all activities that give us meaning and purpose, hopefully having lost nothing but arrogance. 
As we plan our future, let’s consider the meaning not only of our own lives, but also of our Universe itself. Here two of my favorite physicists, Steven Weinberg and Freeman Dyson, represent diametrically opposite views. Weinberg, who won the Nobel Prize for foundational work on the standard model of particle physics, famously said, “The more the universe seems comprehensible, the more it also seems pointless.”35 Dyson, on the other hand, is much more optimistic, as we saw in chapter 6: although he agrees that our Universe was pointless, he believes that life is now filling it with ever more meaning, with the best yet to come if life succeeds in spreading throughout the cosmos. He ended his seminal 1979 paper thus: “Is Weinberg’s universe or mine closer to the truth? One day, before long, we shall know.”36 If our Universe goes back to being permanently unconscious because we drive Earth life extinct or because we let unconscious zombie AI take over our Universe, then Weinberg will be vindicated in spades. 
From this perspective, we see that although we’ve focused on the future of intelligence in this book, the future of consciousness is even more important, since that’s what enables meaning. Philosophers like to go Latin on this distinction, by contrasting sapience (the ability to think intelligently) with sentience (the ability to subjectively experience qualia). We humans have built our identity on being Homo sapiens, the smartest entities around. As we prepare to be humbled by ever smarter machines, I suggest that we rebrand ourselves as Homo sentiens! 
We’ve now explored a range of intelligence explosion scenarios, spanning the spectrum from ones that everyone I know wants to avoid to ones that some of my friends view optimistically. Yet all these scenarios have two features in common: 
1. A fast takeoff: the transition from subhuman to vastly superhuman 
intelligence occurs in a matter of days, not decades. 
2. A unipolar outcome: the result is a single entity controlling Earth. 
There is major controversy about whether these two features are likely or unlikely, and there are plenty of renowned AI researchers and other thinkers on both sides of the debate. To me, this means that we simply don’t know yet, and need to keep an open mind and consider all possibilities for now. Let’s therefore devote the rest of this chapter to exploring scenarios with slower takeoffs, multipolar outcomes, cyborgs and uploads. 
There is an interesting link between the two features, as Nick Bostrom and others have highlighted: a fast takeoff can facilitate a unipolar outcome. We saw above how a rapid takeoff gave the Omegas or Prometheus a decisive strategic advantage that enabled them to take over the world before anyone else had time to copy their technology and seriously compete. In contrast, if takeoff had dragged on for decades, because the key technological breakthroughs were incremental and far between, then other companies would have had ample time to catch up, and it would have been much harder for any player to dominate. If competing companies also had software that could perform MTurk tasks, the law of supply and demand would drive the prices for these tasks down to almost nothing, and none of the companies would earn the sort of windfall profits that enabled the Omegas to gain power. The same applies to all the other ways in which the Omegas made quick money: they were only disruptively profitable because they held a monopoly on their technology. It’s hard to double your money daily (or even annually) in a competitive market where your competition offers products similar to yours for almost zero cost. 
Game Theory and Power Hierarchies 
What’s the natural state of life in our cosmos: unipolar or multipolar? Is power concentrated or distributed? After the first 13.8 billion years, the answer seems to be “both”: we find that the situation is distinctly multipolar, but in an interestingly hierarchical fashion. When we consider all information-processing entities out there—cells, people, organizations, nations, etc.—we find that they both collaborate and compete at a hierarchy of levels. Some cells have found it advantageous to collaborate to such an extreme extent that they’ve merged into multicellular organisms such as people, relinquishing some of their power to a central brain. Some people have found it advantageous to collaborate in groups such as tribes, companies or nations where they in turn relinquish some power to a chief, boss or government. Some groups may in turn choose to relinquish some power to a governing body to improve coordination, with examples ranging from airline alliances to the European Union. 
The branch of mathematics known as game theory elegantly explains that entities have an incentive to cooperate where cooperation is a so-called Nash equilibrium: a situation where any party would be worse off if they altered their strategy. To prevent cheaters from ruining the successful collaboration of a large group, it may be in everyone’s interest to relinquish some power to a higher level in the hierarchy that can punish cheaters: for example, people may collectively benefit from granting a government power to enforce laws, and cells in your body may collectively benefit from giving a police force (immune system) the power to kill any cell that acts too uncooperatively (say by spewing out viruses or turning cancerous). For a hierarchy to remain stable, its Nash equilibrium needs to hold also between entities at different levels: for example, if a government doesn’t provide enough benefit to its citizens for obeying it, they may change their strategy and overthrow it. 
In a complex world, there is a diverse abundance of possible Nash equilibria, corresponding to different types of hierarchies. Some hierarchies are more authoritarian than others. In some, entities are free to leave (like employees in most corporate hierarchies), while in others they’re strongly discouraged from leaving (as in religious cults) or unable to leave (like citizens of North Korea, or cells in a human body). Some hierarchies are held together mainly by threats and fear, others mainly by benefits. Some hierarchies allow their lower parts to 
influence the higher-ups by democratic voting, while others allow upward influence only through persuasion or the passing of information. 
How Technology Affects Hierarchies 
How is technology changing the hierarchical nature of our world? History reveals an overall trend toward ever more coordination over ever-larger distances, which is easy to understand: new transportation technology makes coordination more valuable (by enabling mutual benefit from moving materials and life forms over larger distances) and new communication technology makes coordination easier. When cells learned to signal to neighbors, small multicellular organisms became possible, adding a new hierarchical level. When evolution invented circulatory systems and nervous systems for transportation and communication, large animals became possible. Further improving communication by inventing language allowed humans to coordinate well enough to form further hierarchical levels such as villages, and additional breakthroughs in communication, transportation and other technology enabled the empires of antiquity. Globalization is merely the latest example of this multibillion-year trend of hierarchical growth. 
In most cases, this technology-driven trend has made large entities parts of an even grander structure while retaining much of their autonomy and individuality, although commentators have argued that adaptation of entities to hierarchical life has in some cases reduced their diversity and made them more like indistinguishable replaceable parts. Some technologies, such as surveillance, can give higher levels in the hierarchy more power over their subordinates, while other technologies, such as cryptography and online access to free press and education, can have the opposite effect and empower individuals. 
Although our present world remains stuck in a multipolar Nash equilibrium, with competing nations and multinational corporations at the top level, technology is now advanced enough that a unipolar world would probably also be a stable Nash equilibrium. For example, imagine a parallel universe where everyone on Earth shares the same language, culture, values and level of prosperity, and there is a single world government wherein nations function like states in a federation and have no armies, merely police enforcing laws. 
Transportation and communication technology will obviously improve dramatically, so a natural expectation is that the historical trend will continue, with new hierarchical levels coordinating over ever-larger distances—perhaps ultimately encompassing solar systems, galaxies, superclusters and large swaths of our Universe, as we’ll explore in chapter 6. At the same time, the most fundamental driver of decentralization will remain: it’s wasteful to coordinate unnecessarily over large distances. Even Stalin didn’t try to regulate exactly when his citizens went to the bathroom. For superintelligent AI, the laws of physics will place firm upper limits on transportation and communication technology, making it unlikely that the highest levels of the hierarchy would be able to micromanage everything that happens on planetary and local scales. A superintelligent AI in the Andromeda galaxy wouldn’t be able to give you useful orders for your day-to-day decisions given that you’d need to wait over five million years for your instructions (that’s the round-trip time for you to exchange messages traveling at the speed of light). In the same way, the round-trip travel time for a message crossing Earth is about 0.1 second (about the timescale on which we humans think), so an Earth-sized AI brain could have truly global thoughts only about as fast as a human one. For a small AI performing one operation each billionth of a second (which is typical of today’s computers), 0.1 second would feel like four months to you, so for it to be micromanaged by a planet-controlling AI would be as inefficient as if you asked permission for even your most trivial decisions through transatlantic letters delivered by Columbus-era ships. 
This physics-imposed speed limit on information transfer therefore poses an obvious challenge for any AI wishing to take over our world, let alone our Universe. Before Prometheus broke out, it put very careful thought into how to avoid mind fragmentation, so that its many AI modules running on different computers around the world had goals and incentives to coordinate and act as a single unified entity. 
A staple of science fiction is that humans will merge with machines, either by technologically enhancing biological bodies into cyborgs (short for “cybernetic organisms”) or by uploading our minds into machines. In his book The Age of Em, economist Robin Hanson gives a fascinating survey of what life might be like in a world teeming with uploads (also known as emulations, nicknamed Ems). I think of an upload as the extreme end of the cyborg spectrum, where the only remaining part of the human is the software. Hollywood cyborgs range from visibly mechanical, such as the Borg from Star Trek, to androids almost indistinguishable from humans, such as the Terminators. Fictional uploads range in intelligence from human-level as in the Black Mirror episode “White Christmas” to clearly superhuman as in Transcendence. 
If superintelligence indeed comes about, the temptation to become cyborgs or uploads will be strong. As Hans Moravec puts it in his 1988 classic Mind Children: “Long life loses much of its point if we are fated to spend it staring stupidly at ultra-intelligent machines as they try to describe their ever more spectacular discoveries in baby-talk that we can understand.” Indeed, the temptation of technological enhancement is already so strong that many humans have eyeglasses, hearing aids, pacemakers and prosthetic limbs, as well as medicinal molecules circulating in their bloodstreams. Some teenagers appear to be permanently attached to their smartphones, and my wife teases me about my attachment to my laptop. 
One of today’s most prominent cyborg proponents is Ray Kurzweil. In his book The Singularity Is Near, he argues that the natural continuation of this trend is using nanobots, intelligent biofeedback systems and other technology to replace first our digestive and endocrine systems, our blood and our hearts by the early 2030s, and then move on to upgrading our skeletons, skin, brains and the rest of our bodies during the next two decades. 
Further, he argues that we’ll do even better by eliminating the human body entirely and uploading minds, creating a whole-brain emulation in software. Such an upload can live in a virtual reality or be embodied in a robot capable of walking, flying, swimming, space-faring or anything else allowed by the laws of physics, unencumbered by such everyday concerns as death or limited cognitive resources. 
Although these ideas may sound like science fiction, they certainly don’t violate any known laws of physics, so the most interesting question isn’t whether they can happen, but whether they will happen and, if so, when. Some leading thinkers guess that the first human-level AGI will be an upload, and that this is how the path toward superintelligence will begin. * 
However, I think it’s fair to say that this is currently a minority view among AI researchers and neuroscientists, most of whom guess that the quickest route to superintelligence is to bypass brain emulation and engineer it in some other way—after which we may or may not remain interested in brain emulation. After all, why should our simplest path to a new technology be the one that evolution came up with, constrained by requirements that it be self-assembling, self-repairing and self-reproducing? Evolution optimizes strongly for energy efficiency because of limited food supply, not for ease of construction or understanding by human engineers. My wife, Meia, likes to point out that the aviation industry didn’t start with mechanical birds. Indeed, when we finally figured out how to build mechanical birds in 2011, 1 more than a century after the Wright brothers’ first flight, the aviation industry showed no interest in switching to wing-flapping mechanical-bird travel, even though it’s more energy efficient —because our simpler earlier solution is better suited to our travel needs. 
In the same way, I suspect that there are simpler ways to build human-level thinking machines than the solution evolution came up with, and even if we one day manage to replicate or upload brains, we’ll end up discovering one of those simpler solutions first. 
The short answer is obviously that we have no idea what will happen if humanity succeeds in building human-level AGI. For this reason, we’ve spent this chapter exploring a broad spectrum of scenarios. I’ve attempted to be quite inclusive, spanning the full range of speculations I’ve seen or heard discussed by AI researchers and technologists: fast takeoff/slow takeoff/no takeoff, humans/machines/cyborgs in control, one/many centers of power, etc. Some people have told me that they’re sure that this or that won’t happen. However, I think it’s wise to be humble at this stage and acknowledge how little we know, because for each scenario discussed above, I know at least one well-respected AI researcher who views it as a real possibility. 
As time passes and we reach certain forks in the road, we’ll start to answer key questions and narrow down the options. The first big question is “Will we ever create human-level AGI?” The premise of this chapter is that we will, but there are AI experts who think it will never happen, at least not for hundreds of years. Time will tell! As I mentioned earlier, about half of the AI experts at our Puerto Rico conference guessed that it would happen by 2055. At a follow-up conference we organized two years later, this had dropped to 2047. 
Before any human-level AGI is created, we may start getting strong indications about whether this milestone is likely to be first met by computer engineering, mind uploading or some unforeseen novel approach. If the computer engineering approach to AI that currently dominates the field fails to deliver AGI for centuries, this will increase the chance that uploading will get there first, as happened (rather unrealistically) in the movie Transcendence. 
If human-level AGI gets more imminent, we’ll be able to make more educated guesses about the answer to the next key question: “Will there be a fast takeoff, a slow takeoff or no takeoff?” 
Turning to the optimization power, however, it’s overwhelmingly likely that it will grow rapidly as the AGI transcends human level, for the reasons we saw in the Omega scenario: the main input to further optimization comes not from people but from the machine itself, so the more capable it gets, the faster it improves (if recalcitrance stays fairly constant). 
For any process whose power grows at a rate proportional to its current power, the result is that its power keeps doubling at regular intervals. We call such growth exponential, and we call such processes explosions. If baby-making power grows in proportion to the size of the population, we can get a population explosion. If the creation of neutrons capable of fissioning plutonium grows in proportion to the number of such neutrons, we can get a nuclear explosion. If machine intelligence grows at a rate proportional to the current power, we can get an intelligence explosion. All such explosions are characterized by the time they take to double their power. If that time is hours or days for an intelligence explosion, as in the Omega scenario, we have a fast takeoff on our hands. 
This explosion timescale depends crucially on whether improving the AI requires merely new software (which can be created in a matter of seconds, minutes or hours) or new hardware (which might require months or years). In the Omega scenario, there was a significant hardware overhang, in Bostrom’s terminology: the Omegas had compensated for the low quality of their original software by vast amounts of hardware, which meant that Prometheus could perform a large number of quality doublings by improving its software alone. There was also a major content overhang in the form of much of the internet’s data; Prometheus 1.0 was still not smart enough to make use of most of it, but once Prometheus’ intelligence grew, the data it needed for further learning was already available without delay. 
The hardware and electricity costs of running the AI are crucial as well, since we won’t get an intelligence explosion until the cost of doing human-level work drops below human-level hourly wages. 
Turning to the optimization power, however, it’s overwhelmingly likely that it will grow rapidly as the AGI transcends human level, for the reasons we saw in the Omega scenario: the main input to further optimization comes not from people but from the machine itself, so the more capable it gets, the faster it improves (if recalcitrance stays fairly constant). 
For any process whose power grows at a rate proportional to its current power, the result is that its power keeps doubling at regular intervals. We call such growth exponential, and we call such processes explosions. If baby-making power grows in proportion to the size of the population, we can get a population explosion. If the creation of neutrons capable of fissioning plutonium grows in proportion to the number of such neutrons, we can get a nuclear explosion. If machine intelligence grows at a rate proportional to the current power, we can get an intelligence explosion. All such explosions are characterized by the time they take to double their power. If that time is hours or days for an intelligence explosion, as in the Omega scenario, we have a fast takeoff on our hands. 
This explosion timescale depends crucially on whether improving the AI requires merely new software (which can be created in a matter of seconds, minutes or hours) or new hardware (which might require months or years). In the Omega scenario, there was a significant hardware overhang, in Bostrom’s terminology: the Omegas had compensated for the low quality of their original software by vast amounts of hardware, which meant that Prometheus could perform a large number of quality doublings by improving its software alone. There was also a major content overhang in the form of much of the internet’s data; Prometheus 1.0 was still not smart enough to make use of most of it, but once Prometheus’ intelligence grew, the data it needed for further learning was already available without delay. 
The hardware and electricity costs of running the AI are crucial as well, since we won’t get an intelligence explosion until the cost of doing human-level work drops below human-level hourly wages. 
Let’s begin with a scenario where humans peacefully coexist with technology and in some cases merge with it, as imagined by many futurists and science fiction writers alike: 
Life on Earth (and beyond—more on that in the next chapter) is more diverse than ever before. If you looked at satellite footage of Earth, you’d easily be able to tell apart the machine zones, mixed zones and human-only zones. The machine zones are enormous robot-controlled factories and computing facilities devoid of biological life, aiming to put every atom to its most efficient use. Although the machine zones look monotonous and drab from the outside, they’re spectacularly alive on the inside, with amazing experiences occurring in virtual worlds while colossal computations unlock secrets of our Universe and develop transformative technologies. Earth hosts many superintelligent minds that compete and collaborate, and they all inhabit the machine zones. 
The denizens of the mixed zones are a wild and idiosyncratic mix of computers, robots, humans and hybrids of all three. As envisioned by futurists such as Hans Moravec and Ray Kurzweil, many of the humans have technologically upgraded their bodies to cyborgs in various degrees, and some have uploaded their minds into new hardware, blurring the distinction between man and machine. Most intelligent beings lack a permanent physical form. Instead, they exist as software capable of instantly moving between computers and manifesting themselves in the physical world through robotic bodies. Because these minds can readily duplicate themselves or merge, the “population size” keeps changing. Being unfettered from their physical substrate gives such beings a rather different outlook on life: they feel less individualistic because they can trivially share knowledge and experience modules with others, and they feel subjectively immortal because they can readily make backup copies of themselves. In a sense, the central entities of life aren’t minds, but experiences: exceptionally amazing experiences live on because they get continually copied and re-enjoyed by other minds, while uninteresting experiences get deleted by their owners to free up storage space for better ones. 
For example, uploaded versions of Hans Moravec, Ray Kurzweil and Larry Page have a tradition of taking turns creating virtual realities and then exploring them together, but once in a while, they also enjoy flying together in the real world, embodied in avian winged robots. Some of the robots that roam the streets, skies and lakes of the mixed zones are similarly controlled by uploaded and augmented humans, who choose to embody themselves in the mixed zones because they enjoy being around humans and each other. 
In the human-only zones, in contrast, machines with human-level general intelligence or above are banned, as are technologically enhanced biological organisms. Here, life isn’t dramatically different from today, except that it’s more affluent and convenient: poverty has been mostly eliminated, and cures are available for most of today’s diseases. The small fraction of humans who have opted to live in these zones effectively exist on a lower and more limited plane of awareness from everyone else, and have limited understanding of what their more intelligent fellow minds are doing in the other zones. However, many of them are quite happy with their lives. 
The vast majority of all computations take place in the machine zones, which are mostly owned by the many competing superintelligent AIs that live there. By virtue of their superior intelligence and technology, no other entities can challenge their power. These AIs have agreed to cooperate and coordinate with each other under a libertarian governance system that has no rules except protection of private property. These property rights extend to all intelligent entities, including humans, and explain how the human-only zones came to exist. Early on, groups of humans banded together and decided that, in their zones, it was forbidden to sell property to non-humans. 
Because of their technology, the superintelligent AIs have ended up richer than these humans by a factor much larger than that by which Bill Gates is richer than a homeless beggar. However, people in the human-only zones are still materially better off than most people today: their economy is rather decoupled from that of the machines, so the presence of the machines elsewhere has little effect on them except for the occasional useful technologies that they can understand and reproduce for themselves—much as the Amish and various technology-relinquishing native tribes today have standards of living at least as good as they had in old times. It doesn’t matter that the humans have nothing to sell that the machines need, since the machines need nothing in return. 
In the mixed sectors, the wealth difference between AIs and humans is more noticeable, resulting in land (the only human-owned product that the machines want to buy) being astronomically expensive compared to other products. Most humans who owned land therefore ended up selling a small fraction of it to AIs in return for guaranteed basic income for them and their offspring/uploads in perpetuity. This liberated them from the need to work, and freed them up to enjoy the amazing abundance of cheap machine-produced goods and services, in both physical and virtual reality. As far as the machines are concerned, the mixed zones are mainly for play rather than for work. 
If route 1 comes through first, it could naturally lead to a world teeming with cyborgs and uploads. However, as we discussed in the last chapter, most AI researchers think that the opposite is more likely, with enhanced or digital brains being more difficult to build than clean-slate superhuman AGIs—just as mechanical birds turned out to be harder to build than airplanes. After strong machine AI is built, it’s not obvious that cyborgs or uploads will ever be made. If the Neanderthals had had another 100,000 years to evolve and get smarter, things might have turned out great for them—but Homo sapiens never gave them that much time. 
Doomsday Devices 
So could we humans actually pull off omnicide? Even if a global nuclear war may kill off 90% of all humans, most scientists guess that it wouldn’t kill 100% and therefore wouldn’t drive us extinct. On the other hand, the story of nuclear radiation, nuclear EMP and nuclear winter all demonstrate that the greatest hazards may be ones we haven’t even thought of yet. It’s incredibly difficult to foresee all aspects of the aftermath, and how nuclear winter, infrastructure collapse, elevated mutation levels and desperate armed hordes might interact with other problems such as new pandemics, ecosystem collapse and effects we haven’t yet imagined. My personal assessment is therefore that although the probability of a nuclear war tomorrow triggering human extinction isn’t large, we can’t confidently conclude that it’s zero either. 
Omnicide odds increase if we upgrade today’s nuclear weapons into a deliberate doomsday device. Introduced by RAND strategist Herman Kahn in 1960 and popularized in Stanley Kubrick’s film Dr. Strangelove, a doomsday device takes the paradigm of mutually assured destruction to its ultimate conclusion. It’s the perfect deterrent: a machine that automatically retaliates against any enemy attack by killing all of humanity. 
One candidate for the doomsday device is a huge underground cache of socalled salted nukes, preferably humongous hydrogen bombs surrounded by massive amounts of cobalt. Physicist Leo Szilard argued already in 1950 that this could kill everyone on Earth: the hydrogen bomb explosions would render the cobalt radioactive and blow it into the stratosphere, and its five-year half-life is long enough for it to settle all across Earth (especially if twin doomsday devices were placed in opposite hemispheres), but short enough to cause lethal radiation intensity. Media reports suggest that cobalt bombs are now being built for the first time. Omnicidal opportunities could be bolstered by adding bombs optimized for nuclear winter creation by maximizing long-lived aerosols in the stratosphere. A major selling point of a doomsday device is that it’s much cheaper than a conventional nuclear deterrent: since the bombs don’t need to be launched, there’s no need for expensive missile systems, and the bombs themselves are cheaper to build since they need not be light and compact enough to fit into missiles. 
Second, even if this scenario with cyborgs and uploads did come about, it’s not clear that it would be stable and last. Why should the power balance between multiple superintelligences remain stable for millennia, rather than the AIs merging or the smartest one taking over? Moreover, why should the machines choose to respect human property rights and keep humans around, given that they don’t need humans for anything and can do all human work better and cheaper themselves? Ray Kurzweil speculates that natural and enhanced humans will be protected from extermination because “humans are respected by AIs for giving rise to the machines.”1 However, as we’ll discuss in chapter 7, we must not fall into the trap of anthropomorphizing AIs and assume that they have human-like emotions of gratitude. Indeed, though we humans are imbued with a propensity toward gratitude, we don’t show enough gratitude to our intellectual creator (our DNA) to abstain from thwarting its goals by using birth control. 
Even if we buy the assumption that the AIs will opt to respect human property rights, they can gradually get much of our land in other ways, by using some of their superintelligent persuasion powers that we explored in the last chapter to persuade humans to sell some land for a life in luxury. 
For some of their most ardent supporters, cyborgs and uploads hold a promise of techno-bliss and life extension for all. Indeed, the prospect of getting uploaded in the future has motivated over a hundred people to have their brains posthumously frozen by the Arizona-based company Alcor. If this technology arrives, however, it’s far from clear that it will be available to everybody. Many of the very wealthiest would presumably use it, but who else? Even if the technology got cheaper, where would the line be drawn? Would the severely brain-damaged be uploaded? Would we upload every gorilla? Every ant? Every plant? Every bacterium? Would the future civilization act like obsessivecompulsive hoarders and try to upload everything, or merely a few interesting examples of each species in the spirit of Noah’s Ark? Perhaps only a few representative examples of each type of human? To the vastly more intelligent entities that would exist at that time, an uploaded human may seem about as interesting as a simulated mouse or snail would seem to us. Although we currently have the technical capability to reanimate old spreadsheet programs from the 1980s in a DOS emulator, most of us don’t find this interesting enough to actually do it. 
Many people may dislike this libertarian-utopia scenario because it allows preventable suffering. Since the only sacred principle is property rights, nothing prevents the sort of suffering that abounds in today’s world from continuing in the human and mixed zones. While some people thrive, others may end up living in squalor and indentured servitude, or suffer from violence, fear, repression or depression. For example, Marshall Brain’s 2003 novel Manna describes how AI progress in a libertarian economic system makes most Americans unemployable and condemned to live out the rest of their lives in drab and dreary robotoperated social-welfare housing projects. Much like farm animals, they’re kept fed, healthy and safe in cramped conditions where the rich never need to see them. Birth control medication in the water ensures that they don’t have children, so most of the population gets phased out to leave the remaining rich with larger shares of the robot-produced wealth. 
How bad would it be if 90% of humans get killed? How much worse would it be if 100% get killed? Although it’s tempting to answer the second question with “10% worse,” this is clearly inaccurate from a cosmic perspective: the victims of human extinction wouldn’t be merely everyone alive at the time, but also all descendants that would otherwise have lived in the future, perhaps during billions of years on billions of trillions of planets. On the other hand, human extinction might be viewed as somewhat less horrible by religions according to which humans go to heaven anyway, and there isn’t much emphasis on billionyear futures and cosmic settlements. 
Most people I know cringe at the thought of human extinction, regardless of religious persuasion. Some, however, are so incensed by the way we treat people and other living beings that they hope we’ll get replaced by some more intelligent and deserving life form. In the movie The Matrix, Agent Smith (an AI) articulates this sentiment: “Every mammal on this planet instinctively develops a natural equilibrium with the surrounding environment but you humans do not. You move to an area and you multiply and multiply until every natural resource is consumed and the only way you can survive is to spread to another area. There is another organism on this planet that follows the same pattern. Do you know what it is? A virus. Human beings are a disease, a cancer of this planet. You are a plague and we are the cure.” 
But would a fresh roll of the dice necessarily be better? A civilization isn’t necessarily superior in any ethical or utilitarian sense just because it’s more powerful. “Might makes right” arguments to the effect that stronger is always better have largely fallen from grace these days, being widely associated with fascism. Indeed, although it’s possible that the conqueror AIs may create a civilization whose goals we would view as sophisticated, interesting and worthy, it’s also possible that their goals will turn out to be pathetically banal, such as maximizing the production of paper clips. The deliberately silly example of a paper-clip-maximizing superintelligence was given by Nick Bostrom in 2003 to make the point that the goal of an AI is independent of its intelligence (defined as its aptness at accomplishing whatever goal it has). The only goal of a chess computer is to win at chess, but there are also computer tournaments in so-called losing chess, where the goal is the exact opposite, and the computers competing there are about as smart as the more common ones programmed to win. We humans may view it as artificial stupidity rather than artificial intelligence to want to lose at chess or turn our Universe into paper clips, but that’s merely because we evolved with preinstalled goals valuing such things as victory and survival—goals that an AI may lack. The paper clip maximizer turns as many of Earth’s atoms as possible into paper clips and rapidly expands its factories into the cosmos. It has nothing against humans, and kills us merely because it needs our atoms for paper clip production. 
If paper clips aren’t your thing, consider this example, which I’ve adapted from Hans Moravec’s book Mind Children. We receive a radio message from an extraterrestrial civilization containing a computer program. When we run it, it turns out to be a recursively self-improving AI which takes over the world much like Prometheus did in the previous chapter—except that no human knows its ultimate goal. It rapidly turns our Solar System into a massive construction site, covering the rocky planets and asteroids with factories, power plants and supercomputers, which it uses to design and build a Dyson sphere around the Sun that harvests all its energy to power solar-system-sized radio antennas. *3 This obviously leads to human extinction, but the last humans die convinced that there’s at least a silver lining: whatever the AI is up to, it’s clearly something cool and Star Trek–like. Little do they realize that the sole purpose of the entire construction is for these antennas to rebroadcast the same radio message that the humans received, which is nothing more than a cosmic version of a computer virus. Just as email phishing today preys on gullible internet users, this message preys on gullible biologically evolved civilizations. It was created as a sick joke billions of years ago, and although the entire civilization of its maker is long extinct, the virus continues spreading through our Universe at the speed of light, transforming budding civilizations into dead, empty husks. How would you feel about being conquered by this AI? Let’s now consider a human-extinction scenario that some people may feel better about: viewing the AI as our descendants rather than our conquerors. Hans Moravec supports this view in his book Mind Children: “We humans will benefit for a time from their labors, but sooner or later, like natural children, they will seek their own fortunes while we, their aged parents, silently fade away.” 
Parents with a child smarter than them, who learns from them and accomplishes what they could only dream of, are likely happy and proud even if they know they can’t live to see it all. In this spirit, AIs replace humans but give us a graceful exit that makes us view them as our worthy descendants. Every human is offered an adorable robotic child with superb social skills who learns from them, adopts their values and makes them feel proud and loved. Humans are gradually phased out via a global one-child policy, but are treated so exquisitely well until the end that they feel they’re in the most fortunate generation ever. 
How would you feel about this? After all, we humans are already used to the idea that we and everyone we know will be gone one day, so the only change here is that our descendants will be different and arguably more capable, noble and worthy. 
Moreover, the global one-child policy may be redundant: as long as the AIs eliminate poverty and give all humans the opportunity to live full and inspiring lives, falling birthrates could suffice to drive humanity extinct, as mentioned earlier. Voluntary extinction may happen much faster if the AI-fueled technology keeps us so entertained that almost nobody wants to bother having children. For example, we already encountered the Vites in the egalitarian-utopia scenario who were so enamored with their virtual reality that they had largely lost interest in using or reproducing their physical bodies. Also in this case, the last generation of humans would feel that they were the most fortunate generation of all time, relishing life as intensely as ever right up until the very end. 
You began this chapter pondering where you want the current AGI race to lead. Now that we’ve explored a broad range of scenarios together, which ones appeal to you and which ones do you think we should try hard to avoid? Do you have a clear favorite? Please let me and fellow readers know at http://AgeOfAi.org, and join the discussion! 
The scenarios we’ve covered obviously shouldn’t be viewed as a complete list, and many are thin on details, but I’ve tried hard to be inclusive, spanning the full spectrum from high-tech to low-tech to no-tech and describing all the central hopes and fears expressed in the literature. 
One of the most fun parts of writing this book has been hearing what my friends and colleagues think of these scenarios, and I’ve been amused to learn that there’s no consensus whatsoever. The one thing everybody agrees on is that the choices are more subtle than they may initially seem. People who like any one scenario tend to simultaneously find some aspect(s) of it bothersome. To me, this means that we humans need to continue and deepen this conversation about our future goals, so that we know in which direction to steer. The future potential for life in our cosmos is awe-inspiringly grand, so let’s not squander it by drifting like a rudderless ship, clueless about where we want to go! 
Just how grand is this future potential? No matter how advanced our technology gets, the ability for Life 3.0 to improve and spread through our cosmos will be limited by the laws of physics—what are these ultimate limits, during the billions of years to come? Is our Universe teeming with extraterrestrial life right now, or are we alone? What happens if different expanding cosmic civilizations meet? We’ll tackle these fascinating questions in the next chapter. the ultimate limits? How much of our cosmos can come alive? How far can life reach and how long can it last? How much matter can life make use of, and how much energy, information and computation can it extract? These ultimate limits are set not by our understanding, but by the laws of physics. This, ironically, makes it in some ways easier to analyze the long-term future of life than the short-term future. 
If our 13.8-billion-year cosmic history were compressed into a week, then the 10,000-year drama of the last two chapters would be over in less than half a second. This means that although we cannot predict if and how an intelligence explosion will unfold and what its immediate aftermath will be like, all this turmoil is merely a brief flash in cosmic history whose details don’t affect life’s ultimate limits. If the post-explosion life is as obsessed as today’s humans are with pushing limits, then it will develop technology to actually reach these limits —because it can. In this chapter, we’ll explore what these limits are, thus getting a glimpse of what the long-term future of life may be like. Since these limits are based on our current understanding of physics, they should be viewed as a lower bound on the possibilities: future scientific discoveries may present opportunities to do even better. 
But do we really know that future life will be so ambitious? No, we don’t: perhaps it will become as complacent as a heroin addict or a couch potato merely watching endless reruns of Keeping Up with the Kardashians. However, there is reason to suspect that ambition is a rather generic trait of advanced life. Almost regardless of what it’s trying to maximize, be it intelligence, longevity, knowledge or interesting experiences, it will need resources. It therefore has an incentive to push its technology to the ultimate limits, to make the most of the resources it has. After this, the only way to further improve is to acquire more resources, by expanding into ever-larger regions of the cosmos. 
Also, life may independently originate in multiple places in our cosmos. In that case, unambitious civilizations simply become cosmically irrelevant. When it comes to the future of life, one of the most hopeful visionaries is Freeman Dyson. I’ve had the honor and pleasure of knowing him for the past two decades, but when I first met him, I felt nervous. I was a junior postdoc chowing away with my friends in the lunchroom of the Institute for Advanced Study in Princeton, and out of the blue, this world-famous physicist who used to hang out with Einstein and Gödel came up and introduced himself, asking if he could join us! He quickly put me at ease, however, by explaining that he preferred eating lunch with young folks over stuffy old professors. Even though he’s ninety-three as I type these words, Freeman is still younger in spirit than most people I know, and the mischievous boyish glint in his eyes reveals that he couldn’t care less about formalities, academic hierarchies or conventional wisdom. The bolder the idea, the more excited he gets. 
When we talked about energy use, he scoffed at how unambitious we humans were, pointing out that we could meet all our current global energy needs by harvesting the sunlight striking an area smaller than 0.5% of the Sahara desert. But why stop there? Why even stop at capturing all the sunlight striking Earth, letting most of it get wastefully beamed into empty space? Why not simply put all the Sun’s energy output to use for life? So how did this amazing awakening come about? It wasn’t an isolated event, but merely one step in a relentless 13.8-billion-year process that’s making our Universe ever more complex and interesting—and is continuing at an accelerating pace. 
As a physicist, I feel fortunate to have gotten to spend much of the past quarter century helping to pin down our cosmic history, and it’s been an amazing journey of discovery. Since the days when I was a graduate student, we’ve gone from arguing about whether our Universe is 10 or 20 billion years old to arguing about whether it’s 13.7 or 13.8 billion years old, thanks to a combination of better telescopes, better computers and better understanding. We physicists still don’t know for sure what caused our Big Bang or whether this was truly the beginning of everything or merely the sequel to an earlier stage. However, we’ve acquired a rather detailed understanding of what’s happened since our Big Bang, thanks to an avalanche of high-quality measurements, so please let me take a few minutes to summarize 13.8 billion years of cosmic history. 
In the beginning, there was light. In the first split second after our Big Bang, the entire part of space that our telescopes can in principle observe (“our observable Universe,” or simply “our Universe” for short) was much hotter and brighter than the core of our Sun and it expanded rapidly. Although this may sound spectacular, it was also dull in the sense that our Universe contained nothing but a lifeless, dense, hot and boringly uniform soup of elementary particles. Things looked pretty much the same everywhere, and the only interesting structure consisted of faint random-looking sound waves that made the soup about 0.001% denser in some places. These faint waves are widely believed to have originated as so-called quantum fluctuations, because Heisenberg’s uncertainty principle of quantum mechanics forbids anything from being completely boring and uniform. 
As our Universe expanded and cooled, it grew more interesting as its particles combined into ever more complex objects. The question of how to define life is notoriously controversial. Competing definitions abound, some of which include highly specific requirements such as being composed of cells, which might disqualify both future intelligent machines and extraterrestrial civilizations. Since we don’t want to limit our thinking about the future of life to the species we’ve encountered so far, let’s instead define life very broadly, simply as a process that can retain its complexity and replicate. What’s replicated isn’t matter (made of atoms) but information (made of bits) specifying how the atoms are arranged. When a bacterium makes a copy of its DNA, no new atoms are created, but a new set of atoms are arranged in the same pattern as the original, thereby copying the information. In other words, we can think of life as a self-replicating information-processing system whose information (software) determines both its behavior and the blueprints for its hardware. 
Like our Universe itself, life gradually grew more complex and interesting, *1 and as I’ll now explain, I find it helpful to classify life forms into three levels of sophistication: Life 1.0, 2.0 and 3.0. I’ve summarized these three levels in figure 1.1. 
It’s still an open question how, when and where life first appeared in our Universe, but there is strong evidence that here on Earth life first appeared about 4 billion years ago. Before long, our planet was teeming with a diverse panoply of life forms. The most successful ones, which soon outcompeted the rest, were able to react to their environment in some way. Specifically, they were what computer scientists call “intelligent agents”: entities that collect information about their environment from sensors and then process this information to decide how to act back on their environment. This can include highly complex information processing, such as when you use information from your eyes and ears to decide what to say in a conversation. But it can also involve hardware and software that’s quite simple. 
Inspired by Olaf Stapledon’s 1937 sci-fi classic Star Maker, with rings of artificial worlds orbiting their parent star, Freeman Dyson published a description in 1960 of what became known as a Dyson sphere. 1 Freeman’s idea was to rearrange Jupiter into a biosphere in the form of a spherical shell surrounding the Sun, where our descendants could flourish, enjoying 100 billion times more biomass and a trillion times more energy than humanity uses today. 2 He argued that this was the natural next step: “One should expect that, within a few thousand years of its entering the stage of industrial development, any intelligent species should be found occupying an artificial biosphere which completely surrounds its parent star.” If you lived on the inside of a Dyson sphere, there would be no nights: you’d always see the Sun straight overhead, and all across the sky, you’d see sunlight reflecting off the rest of the biosphere, just as you can nowadays see sunlight reflecting off the Moon during the day. If you wanted to see stars, you’d simply go “upstairs” and peer out at the cosmos from the outside of the Dyson sphere. 
There is only a very limited family of stationary, asymptotically flat, black hole solutions to the Einstein
equations. Such a spacetime is one that has an event horizon and a Killing vector that is timelike at infinity.
A static spacetime is a stationary one that also has a time reflection symmetry. Thus a rotating black hole
is stationary but not static, whereas a nonrotating one is static.
A number of black hole uniqueness theorems have been proved under various reasonably well motivated
assumptions. The EF metric (1.2) gives the unique static vacuum solution with an event horizon. The
only stationary vacuum solution with a horizon is the Kerr solution, parametrized by the total mass M and
angular momentum J . Including an electromagnetic field, the only static solution with a horizon with one
connected component is the Reissner-Nordstrom solution parametrized by mass and electric and magnetic
charges Qe, Qm. Since the electromagnetic stress-energy tensor is duality rotation invariant, the metric
depends only on the combination Q2
e + Q2
m. Finally, allowing for angular momentum, the unique stationary
black hole solution with electromagnetic field is the Kerr-Newman metric.
1.3 Positive energy theorem
Energy of an isolated (asymptotically flat) system in GR can be defined as the gravitating mass as measured
at infinity, times c2. This energy, which is the numerical value of the Hamiltonian that generates the time
translation symmetry at infinity, is a conserved quantity in general relativity. The energy can be negative
e.g. if we simply put rs < 0 in the Eddington-Finkelstein line element, but this yields a naked singularity. If
one assumes (i) spacetime can be spanned by a nonsingular Cauchy surface whose only boundary is the one
at infinity, and (ii) matter has positive energy (more precisely, the stress-energy tensor satisfies the dominant
energy condition, which for diagonalizable Tab means that the energy density is greater than the magnitude
of any principal pressure), then it can be proved that the total energy of the spacetime is necessarily positive.
This was first proved in a geometrical way by Schoen and Yau, and shortly thereafter proved in a more direct
way way by Witten. The idea for this proof came from quantum supergravity, where the Hamiltonian has
the manifestly positive form H= Q2 in terms of the supersymmetry generator Q.
Witten’s proof goes roughly as follows. The energy is written as a flux integral involving first derivative
of the metric at infinity which picks oﬀ the coeﬃcient of the 1/r term in the metric. This is sometimes
called the ADM energy. This is then reexpressed, using the Einstein equations, as a volume integral over
a spacelike Cauchy surface with an integrand containing a term quadratic in the derivative of an arbitrary
spinor field and a term in the energy density of matter. If the spinor field is chosen to satisfy a certain
elliptic diﬀerential equation, then the quadratic spinor term becomes manifestly positive. The only zero
5
energy solution is empty flat spacetime. If a black hole is present then the Cauchy surface can be chosen to
dip below the formation of the event horizon, thus avoiding the presence of an inner boundary or singularity
on the surface. Alternatively, the contribution from an inner boundary located at an apparent horizon can
be shown to be positive.
Positivity of the total energy at infinity does not necessarily mean that the system cannot radiate an
infinite energy while collapsing, since both the energy of the radiation and the energy of the leftover system are
included in the total energy. A diﬀerent definition of energy, called the Bondi energy, allows one to evaluate
just the “leftover” energy. The Bondi energy is the gravitating mass as seen by light rays propagating out
to infinity in the lightlike direction, rather than the spacelike direction. Essentially the same argument as
before shows that the Bondi energy is also necessarily nonnegative. Thus only a finite energy can be radiated
away.
A positive energy theorem has also been proved in the presence of a negative cosmological constant, in
which case the asymptotic structure of the spacetime is anti-de-Sitter rather than flat. One might have thought that the singularity at r = 0 is just an artifact of perfect spherical symmetry,
that in an asymmetric collapse most of the mass would “miss” rather than collide and no infinite density or
curvature would develop. A strong suggestion that this is not the case comes from the fact that the angular
momentum barrier for orbits of test particles in a black hole spacetime gives way to a negative 1/r3-term of
purely relativistic origin which produces an infinite well as r goes to zero. That it is in fact not true was
proved by Penrose.
The idea of Penrose’s proof rests on the concept of a trapped surface. This is a closed, spacelike, 2-surface
whose ingoing and outgoing null normal congruences are both converging (see Fig. 1.3). For example, a
sphere at constant r and v in Eddington-Finkelstein coordinates is a trapped surface if it lies inside the
horizon. But even in a somewhat asymmetrical collapse it is expected that a trapped surface will form.
Penrose argues that the existence of a trapped surface T implies the existence of a singularity on the
boundary ∂F of its future F . (The “future” of a set is the collection of all spacetime points that can be
reached by future-going timelike or null curves from that set.) Very roughly his reasoning is this: the null
normals to T start out converging everywhere so, since gravity is attractive, they must continue converging
and will necessarily reach crossing points (technically, conjugate points) in a finite aﬃne parameter. ∂F must
“end” before or when the crossing points are reached (because the boundary ∂F must be locally tangent to
the light cones) so ∂F must be compact. This is a very weird structure for the boundary of the future of
T , and in fact is incompatible with other reasonable requirements on the spacetime (see below). The only
way out is if at least one of the null normals cannot be extended far enough to reach its crossing point. This
nonextendibility is what is meant in the theorem by the existence of a singularity.
Einstein’s equation comes into the proof only in ensuring that the initially converging null normals to T
must reach a crossing point in a finite aﬃne parameter. It is worth explaining this in more detail, since it
involves technology that figures in many developments in general relativity and black hole thermodynamics,
namely, the focusing equation (which is often called the Raychaudhuri equation, or Sach’s equation, or
Newman-Penrose equation). This equation relates the focusing of a bundle of light rays (called a null. This focusing equation shows that an initially converging congruence must reach a “crossing point”, i.e.
a point where ρ diverges, in a finite aﬃne parameter provided Rabkakb ≥ 0. More precisely, d
dλ ρ ≥ 1
2 ρ2
implies that if ρ(0) = ρ0 > 0, then ρ → ∞ for some λ ≤ 2/ρ0. In flat space this would of course be true, and
if positive the Ricci tensor term will only make it converge faster. The condition Rabkakb ≥ 0 is equivalent
via Einstein’s equation to the condition Tabkakb ≥ 0, which for a diagonalizable stress-energy tensor is
equivalent to the condition that the energy density plus any of the three principal pressures is positive. Thus
unless there is “anti-gravitational repulsion” due to negative energy and/or pressure, a crossing point must
be reached.
A somewhat more precise statement of Penrose’s theorem is that a singularity is unavoidable if there is a
trapped surface and (i) Rabkakb ≥ 0 for all null ka and (ii) spacetime has the form M = Σ × R, where Σ is
a non-compact, connected, Cauchy surface. Later Hawking and Penrose gave another proof that weakened
the second assumption, replacing it by the conditions that (ii′) there are no closed timelike curves and (ii′′)
the curvature is “generic” in a certain extremely mild sense. In the examples above the most eﬃcient energy extraction occurs when the black hole area is unchanged,
and in less eﬃcient processes the area always increases. It was shown by Hawking that in fact the area of an
event horizon can never decrease under quite general assumptions. Hawking’s theorem applies to arbitrary
dynamical black holes, for which a general definition of the horizon is needed. The future event horizon of
an asymptotically flat black hole spacetime is defined as the boundary of the past of future null infinity,
that is, the boundary of the set of points that can communicate with the remote regions of the spacetime
to the future. Hawking proved that if Rabkakb ≥ 0, and if there are no naked singularities (i.e. if “cosmic
censorship” holds), the cross sectional area of a future event horizon cannot be decreasing anywhere. The
reason is that the focusing equation implies that if the horizon generators are converging somewhere then
they will reach a crossing point in a finite aﬃne parameter. But such a point cannot lie on a future event
horizon (because the horizon must be locally tangent to the light cones), nor can the generators leave the
horizon. The only remaining possibility is that the generators cannot be extended far enough to reach the
crossing point—that is, they must reach a singularity.
That was an easy argument, but it isn’t as strong as one would like, since the singularity may not be
10
naked, i.e. visible from infinity, and we have no good reason to assume clothed (or barely clothed) singularities
do not occur.1 With a more subtle argument, Hawking showed that convergence of the horizon generators
does imply existence of a naked singularity. The basic idea is to deform the horizon cross-section outward a
bit from the point where the generators are assumed to be converging, and to consider the boundary of the
future of the part of the deformed cross-section that lies outside the horizon. If the deformation is suﬃciently
small, all of the generators of this boundary are initially converging and therefore reach crossing points and
leave the boundary at finite aﬃne parameter. But at least one of these generators must reach infinity while
remaining on the boundary, since the deformed cross-section is outside the event horizon. The only way out
of the contradiction is if there is a singularity outside the horizon, on the boundary, which is visible from
infinity and therefore naked.
Essentially the same argument as the one just given also establishes that an outer trapped surface must
not be visible from infinity, i.e. must lie inside an event horizon. This fact is used sometimes as an indirect
way to probe numerical solutions of the Einstein equation for the presence of an event horizon. Whereas
the event horizon is a nonlocal construction in time, and so can not be directly identified given only a finite
time interval, a trapped surface is defined locally and may be unambiguously identified at a single time.
Assuming cosmic censorship, the presence of a trapped surface implies the existence of a horizon. From the forgoing it is apparent that energy can flow not just into black holes but also out of them, and
they can act as an intermediary in energy exchange processes. Energy extraction is maximally eﬃcient when
the horizon area does not change, and processes that increase the area are irreversible, since the area cannot
decrease. The analogy with thermodynamic behavior is striking, with the horizon area playing the role of
entropy. This analogy was vigorously pursued as soon as it was recognized at the beginning of the 1970’s,
although it had what appeared at first to be several glaring flaws:
F1. the temperature of a black hole vanishes;
F2. entropy is dimensionless, whereas horizon area is a length squared;
F3. the area of every black hole is separately non-decreasing, whereas only the total entropy is non-
decreasing in thermodynamics.
By 1975 it was understood that the resolution to all of these flaws lies in the incorporation of quantum
theory, as has so often been the case in resolving thermodynamic conundrums. A black hole has a Hawking
temperature proportional to Planck’s constant ¯ h, the entropy is one fourth the horizon area divided by the
Planck length squared (¯ hG/c3), and the area can decrease via Hawking radiation.
Rather than jumping now immediately into the subject of quantum black hole thermodynamics, it is
worth discussing first the classical aspects of the theory. These are important in their own right, and they
form the foundation for quantum black hole thermodynamics. But also it is intriguing to see what can
be inferred without invoking quantum theory, and it may teach us something about the deeper origins of
gravitation. In proceeding this way we are following more or less the path that was taken historically. It turns out that this missing term is given by κdA/8πG, where κ is the surface gravity of the horizon. The
surface gravity of a stationary black hole can be defined assuming the event horizon is a Killing horizon, i.e.
that the null horizon generators are orbits of a Killing field. (See next section for more on this assumption.)
Then κ is defined as the magnitude of the gradient of the norm of the horizon generating Killing field
χa = ξa + Ωψa, evaluated at the horizon. That is,
κ2 :=−(∇a|χ|)(∇a|χ|) (2.1)
at the horizon. An equivalent definition of κ is the the magnitude of the acceleration, with respect to Killing
time, of a stationary zero angular momentum particle just outside the horizon. This is the same as the force
per unit mass that must be applied at infinity in order to hold the particle on its path. For a nonrotating
neutral black hole the surface gravity is given by 1/4M , so a larger black hole has a smaller surface gravity.
This happens to be identical to the Newtonian surface gravity of a spherical mass M with radius equal to
the Schwarzschild radius 2M. 
2.1.2 Zeroth Law
Although κ is defined locally on the horizon, it turns out that it is always constant over the horizon of a
stationary black hole. This constancy is reminiscent of the Zeroth Law of thermodynamics which states that
the temperature is uniform everywhere in a system in thermal equilibrium. The constancy of κ can be traced
to the special properties of the horizon of a stationary black hole. It can be proved without field equations
or energy conditions [Carter, R´acz & Wald] assuming the horizon is a Killing horizon (i.e. there is a Killing
field tangent to the null generators of the horizon) and that the black hole is either (i) static (i.e. stationary
and time reflection symmetric), or (ii) axisymmetric and “t-φ” reflection symmetric. Alternatively, it can
be proved [Hawking] assuming only stationarity together with the Einstein field equation with the dominant
energy condition for matter. (Assuming also hyperbolic field equations for matter, and analyticity of the
spacetime, Hawking also shows that the event horizon must be a Killing horizon, and that the spacetime
must be either static or axisymmetric.) Bekenstein proposed that some multiple ηA/¯ hG of the black hole area, measured in units of the squared
Planck length L2
p
= ¯ hG/c3, is actually entropy, and he conjectured a generalized second law (GSL) which
states that the sum of the entropy outside the black hole and the entropy of the black hole itself will never
decrease:
δ(Soutside + ηA/¯ hG) ≥ 0 (2.9)
Classically, it seems possible to violate the GSL, using processes like those already considered: A box
containing entropy in the form of, say, radiation, can be lowered to the horizon of a black hole and dropped
in. For an ideal, infinitesimal box all of the energy can be extracted at infinity, so when the box is dropped
in it adds no mass to the hole. Thus the horizon area does not change, but the entropy of the exterior has
decreased, violating the GSL. This may be considered yet another flaw in the thermodynamic analogy:
F4. the GSL can be violated by adding entropy to a black hole without changing its area.
At the purely classical level, it thus appears that the GSL is simply not true. Note however that as ¯ h → 0,
the entropy ηA/¯ hG diverges, and an infinitesimal area change can make a finite change in the Bekenstein
entropy. The other flaws (F1-F3) in the thermodynamic analogy are also in a sense resolved in the ¯ h → 0
limit. F2 is resolved by Bekenstein’s postulate, while F3 is resolved because a finite decrease in area would
imply an infinite decrease in entropy. Furthermore, the first law implies that the black hole has a Bekenstein
temperature TB = ¯ hκ/8πη, which vanishes in the classical limit, thus resolving flaw F1. The Bekenstein
proposal therefore “explains” the apparent flaws in the thermodynamic analogy, and it suggests very strongly
that the analogy is much more than an analogy. It turns out that, with quantum eﬀects included, the GSL
is indeed true after all, with the coeﬃcient η equal to 1/4. The analogy between surface gravity and temperature was based in the above discussion on the way the
temperature enters the First Law (2.2), the fact that it is constant over the horizon (Zeroth Law), and the
fact that it is (probably) impossible to reduce it to zero in a physical process (Third Law). In this section
we discuss a sense in which a black hole has a thermodynamic temperature, defined in terms of the eﬃciency
of heat engines, that is proportional to its surface gravity. The discussion is a variation on that of [Sciama,
1976, see section 1.1.4].
A thermodynamic definition of temperature can be given by virtue of the second law in the (Clausius)
form which states that it is impossible to pump heat from a colder body to a hotter one in a cycle with no
other changes. Given this Second Law, the ratio Qin/Qout of the heat in to the heat out in any reversible
heat engine cycle operating between two heat baths must be a universal constant characteristic of that pair
of equilibrium states. The ratio of the thermodynamic temperatures of the two equilibrium states is then
defined by Tin /Tout := Qin/Qout. This defines the temperature of all equilibrium states up to an overall
arbitrary constant. In a heat engine, the heat out is wasted, so the most eﬃcient engine is one which dumps
its heat into the coldest reservoir. Applying this definition to a black hole, it follows that the temperature of the hole must be zero, since as
we have seen one can, with perfect eﬃciency, extract the entire rest mass of a particle (or of heat) as useful
work by dumping the heat into a black hole after lowering it down all the way to the horizon. Note however
that to arrive at this conclusion we must take the unphysical limit of really lowering the heat precisely all
the way to the horizon.
A meaningful expression for the ratio of the temperatures of two black holes can be obtained by passing to
this unphysical limit in a fairly natural manner. Consider operating a heat engine of the type just discussed
between two black holes separated very far from one another, and suppose there is a minimum proper
distance dmin to which the horizon of either black hole is approached. We shall assume that this distance is
the same for both black holes, and take the limit as dmin → 0. We also assume for simplicity that the black
holes are nonrotating; it is presumably possible to generalize the analysis to the rotating case.
If the “heat” has a rest mass m, it has Killing energy E1 = ξ1m at its lowest point outside the horizon
of the first black hole, where ξ is the norm of the Killing field. The heat is then lifted slowly and lowered
back down to just outside the horizon of the second black hole, where it has Killing energy E2 = ξ2m, and is
then dumped into the second hole. The diﬀerence E1− E2 is the useful work extracted in the process, and
the ratio T1/T2 := E1/E2 = ξ1/ξ2 defines the ratio of the thermodynamic temperatures of the two holes.
Now near the horizon we can approximate ξ ≃ κdmin, where κ is exactly the surface gravity that entered
above in the First Law. At the lowest points we thus have ξ1/ξ2 ≃ κ1/κ2, which becomes exact in the limit
dmin → 0, so that T1/T2 = κ1/κ2. That is, the thermodynamic temperature of a black hole is proportional
to its surface gravity.
This derivation hinges on the limiting procedure, in which a common minimum distance of approach to
the horizon taken to zero, which is not very well motivated. It is therefore worth pointing out that this is
equivalent to taking a common maximum proper acceleration to infinity. The proper acceleration of a static
worldline is given by a = κ/ξ in the limit that the horizon is approached, so a is just the inverse of the
proper distance from the horizon. Alternatively, rather than taking a limit as the horizon is approached,
one might imagine that there is some common minimum distance of approach or maximum acceleration to
which the heat will be subjected in any given transfer process. Classical black hole physics cries out for the incorporation of ¯ h eﬀects, so the thermodynamic “analogy”
can become true thermodynamics. Since general relativity is relativistic, it is not quantum mechanics but
relativistic quantum field theory that is called for. Thus, in principle, one should consider “quantum gravity”,
whatever that may be. Although no one knows for sure what quantum gravity actually is, formal treatment
of its semiclassical limit by Gibbons and Hawking in a path integral framework revealed one way in which
the analogy can become an identity. This will be discussed later. An alternate semiclassical approach—and
historically the first— is to consider quantum fields in a fixed black hole background. A quantum field
has vacuum fluctuations that permeate all of spacetime, so there is always something going on, even in the
“empty space” around a black hole. Thus turning on the vacuum fluctuations of quantum fields can have
a profound eﬀect on the thermodynamics of black holes. The principal eﬀect is the existence of Hawking
radiation.
The historical route to Hawking’s discovery is worth mentioning. (See Thorne’s book, Black Holes and
Time Warps, for an interesting account.) After the Penrose process was invented, it was only a short step
to consider a similar process using waves rather than particles [Zel’dovich, Misner], a phenomenon dubbed
“super-radiance”. Quantum mechanically, supperradiance corresponds to stimulated emission, so it was then
natural to ask whether a rotating black hole would spontaneously radiate [Zel’dovich, Starobinsky, Unruh].
In trying to improve on the calculations in favor of spontaneous emission, Hawking stumbled onto the fact
that even a non-rotating black hole would emit particles, and it would do so with a thermal spectrum at a
temperature. Killing energy and angular momentum must be conserved, so the two particles must have opposite values for
these. In the ergoregion there are negative energy states for real particles, so such a pair can be created there,
with the negative energy partner later falling across the event horizon into the black hole. In the nonrotating
case the ergoregion exists only beyond the horizon, however the pair creation process can straddle the horizon
(Fig. 3.1). This turns out to have a thermal amplitude, and gives rise to the Hawking eﬀect.
Let us now briefly consider the implications of the Hawking eﬀect for black hole thermodynamics. First
of all the surface gravity κ, which was already implicated as a temperature in the classical theory, turns
out to give rise to the true Hawking temperature ¯ hκ/2π. From the First Law (2.2) it then follows that the
entropy of a black hole is given by
SBH = A/4¯ hG,
one fourth the area in squared Planck lengths (the subscript ‘BH’ conveniently stands for both ‘Bekenstein-
Hawking’ and ‘black hole’). The zero-temperature and dimensional flaws (F1) and (F2) (cf. Chapter 2)
are thus removed. Furthermore, the Hawking radiation leads to a decrease in the horizon area. This is
obvious in the nonrotating case, since the black hole loses mass, but it also happens in the rotating case.
The reason is that the negative energy partner in the Hawking pair creation process is never a real particle
outside the horizon, so it need not carry a locally future-pointing four-momentum flux across the horizon.
The Bekenstein-Hawking entropy can therefore decrease, so flaw (F3) is removed. The remaining flaw in
the thermodynamic analogy was the failure of the generalized second law (F4) (cf. section 2.2). This too
is repaired by the incorporation of quantum field eﬀects, at least in quasistationary. Underlying the Hawking eﬀect is the Unruh eﬀect, which is the fact that the vacuum in Minkowski space
appears to be a thermal state at temperature
TU = ¯ ha/2π (3.2)
when viewed by an observer with acceleration a. Thus there is already something ‘thermal’ about the vacuum
fluctuations even in flat spacetime. Since it lies at the core of the entire subject, let us first delve in some
detail into the theory of the Unruh eﬀect, before coming back to the Hawking eﬀect.
The Unruh eﬀect was discovered after the Hawking eﬀect, as a result of eﬀorts to understand the Hawking
eﬀect. The original observation was that a detector coupled to a quantum field and accelerating through the
Minkowski vacuum will be thermally excited. A related observation by Davies was that a mirror accelerating
through the vacuum will radiate thermally. But the essential point is that the vacuum itself has a thermal
character, quite independently of anything that might be coupled to it.
Owing to the symmetry of the Minkowski vacuum under translations and Lorentz tranformations, the
vacuum will appear stationary in a uniformly accelerated frame, but this appearance will not be independent
of the acceleration. Moreover, since it is the ground state, it is stable to dynamical perturbations. Sciama
pointed out that stationarity and stability of the state alone are suﬃcient to indicate that the state is a
thermal one, as shown by Haag et al in axiomatic quantum field theory. Note that the time scale associated
with the Unruh temperature, ¯ h/TH = 2πc/a, is the time it takes for the velocity to change by something of
order c when the acceleration is a.
Two derivations of the Unruh eﬀect will now be given, both of which are valid for arbitrary interacting
scalar fields in spacetime of any dimension. (The generalization to fields of nonzero spin is straightforward.For a Schwarzschild black hole, κ = 1/2Rs = 1/4GM , so the Hawking temperature is TH = ¯ h/8πGM ,
and the corresponding wavelength is λH = 2π/ω = 8π2Rs. A larger black hole is therefore cooler.
Notice the subtle shift from the Newtonian perspective. The postulate is not that particles move in straight
lines, but rather that there exist spacetime frames with respect to which particles move in straight lines.
Implicit in the assumption of the existence of globally inertial frames is the assumption that the geometry of
spacetime is flat, the geometry of Euclid, where parallel lines remain parallel to infinity. In general relativity,
this postulate is replaced by the weaker postulate that local (not global) inertial frames exist. A locally
inertial frame is one which is inertial in a “small neighbourhood” of a spacetime point. In general relativity,
spacetime can be curved. Statement: “The laws of physics are the same in any inertial frame,
regardless of position or velocity.”
Physically, this means that there is no absolute spacetime, no absolute frame of reference with respect to
which position and velocity are defined. Only relative positions and velocities between objects are meaningful.
Mathematically, the principle of special relativity requires that the equations of special relativity be
Lorentz covariant.
It is to be noted that the principle of special relativity does not imply the constancy of the speed of light,
although the postulates are consistent with each other. Moreover the constancy of the speed of light does
not imply the Principle of Special Relativity, although for Einstein the former appears to have been the
inspiration for the latter. In general, a Lorentz transformation consists of a spatial rotation about some
spatial axis, combined with a Lorentz boost by some velocity in some direction.
Only space along the direction of motion gets skewed with time. Distances perpendicular to the direction
of motion remain unchanged. Why must this be so? Consider two hoops which have the same size when at
rest relative to each other. Now set the hoops moving towards each other. Which hoop passes inside the
other? Neither! For suppose Vermilion thinks Cerulean’s hoop passed inside hers; by symmetry, Cerulean
must think Vermilion’s hoop passed inside his; but both cannot be true; the only possibility is that the hoops
remain the same size in directions perpendicular to the direction of motion.
If you have understood all this, then you have understood the crux of special relativity, and you can
now go away and figure out all the mathematics of Lorentz transformations. The mathematical problem is:
what is the relation between the spacetime coordinates {𝑡,𝑥,𝑦,𝑧}and {𝑡′,𝑥′,𝑦′,𝑧′}of a spacetime interval,
a 4-vector, in Vermilion’s versus Cerulean’s frames, if Cerulean is moving relative to Vermilion at velocity 𝑣
in, say, the 𝑥 direction? The solution follows from requiring
1. that both observers consider themselves to be at the centre of the lightcone, as illustrated by Figure 1.4,
and
2. that distances perpendicular to the direction of motion remain unchanged, as illustrated by Figure 1.5.
An alternative version of the second condition is that a Lorentz transformation at velocity 𝑣 followed by a
Lorentz transformation at velocity −𝑣 should yield the unit transformation.
Note that the postulate of the existence of globally inertial frames implies that Lorentz transformations
are linear, that straight lines (4-vectors) in one inertial spacetime frame transform into straight lines in other
inertial frames.
You will solve this problem in the next section but two, §1.6. As a prelude, the next two sections, §1.4 and
§1.5 discuss simultaneity and time dilation. How can simultaneity, the notion of events occurring at the same time at different places, be defined opera-
tionally?
One way is illustrated in the sequences of spacetime diagrams in Figure 1.6. Vermilion surrounds herself
with a set of mirrors, equidistant from Vermilion. She sends out a flash of light, which reflects off the mirrors
back to Vermilion. How does Vermilion know that the mirrors are all the same distance from her? Because the
reflected flash from the mirrors arrives back to Vermilion all at the same instant. Vermilion asserts that the
light flash must have hit all the mirrors simultaneously. Vermilion also asserts that the instant when the light
hit the mirrors must have been the instant, as registered by her wristwatch, precisely half way between the
moment she emitted the flash and the moment she received it back again. If it takes, say, 2 seconds between
flash and receipt, then Vermilion concludes that the mirrors are 1 lightsecond away from her. The spatial
hyperplane passing through these events is a hypersurface of simultaneity. More generally, from Vermilion’s
perspective, each horizontal hyperplane in the spacetime diagram is a hypersurface of simultaneity. Vermilion and Cerulean construct identical clocks, Figure 1.8, consisting of a light beam which bounces off a
mirror. Tick, the light beam hits the mirror, tock, the beam returns to its owner. As long as Vermilion and
Cerulean remain at rest relative to each other, both agree that each other’s clock tick-tocks at the same rate
as their own.
But now suppose Cerulean goes off at velocity 𝑣 relative to Vermilion, in a direction perpendicular to the
direction of the mirror. A far as Cerulean is concerned, his clock tick-tocks at the same rate as before, a tick
at the mirror, a tock on return. But from Vermilion’s point of view, although the distance between Cerulean
and his mirror at any instant remains the same as before, the light has farther to go. And since the speed
of light is constant, Vermilion thinks it takes longer for Cerulean’s clock to tick-tock than her own. Thus
Vermilion thinks Cerulean’s clock runs slow relative to her own.
Cerulean defines surfaces of simultaneity using the same operational setup: he encompasses himself with
mirrors, arranging them so that a flash of light returns from them to him all at the same instant. But whereas
Cerulean concludes that his mirrors are all equidistant from him and that the light bounces off them all at the
same instant, Vermilion thinks otherwise. From Vermilion’s point of view, the light bounces off Cerulean’s
mirrors at different times and moreover at different distances from Cerulean, as illustrated in Figure 1.7.
Only so can the speed of light be constant, as Vermilion sees it, and yet the light return to Cerulean all at
the same instant.
Of course from Cerulean’s point of view all is fine: he thinks his mirrors are equidistant from him
An example of the application of the principle of special relativity is the construction of the energy-
momentum 4-vector of a particle, which should have the same form in any inertial frame 
3. The speed of light is constant. Statement: “The speed of light 𝑐 is a universal constant, the same in
any inertial frame.”
This postulate is the nub of special relativity. The immediate challenge of this Chapter, §1.3, is to confront
its paradoxical implications, and to resolve them.
Measuring speed requires being able to measure intervals of both space and time: speed is distance travelled
divided by time elapsed. Inertial frames constitute a special class of spacetime coordinate systems; it is with
respect to distance and time intervals in these special frames that the speed of light is asserted to be constant.
In general relativity, arbitrarily weird coordinate systems are allowed, and light need move neither in
straight lines nor at constant velocity with respect to bizarre coordinates (why should it, if the labelling
of space and time is totally arbitrary?). However, general relativity asserts the existence of locally inertial
frames, and the speed of light is a universal constant in those frames.
In 1983, the General Conference on Weights and Measures officially
Recall
that in the case of the flat space Unruh eﬀect, the redshifting to infinity completely depletes the acceleration
radiation, since the norm of the boost Killing field diverges at infinity.
Two remarks should be made here regarding the state dependence of the above argument. First, the
argument is clearly invalid if the the state of the quantum field is not regular near the horizon. For example,
there is a state called the “Boulware vacuum”, or “static vacuum”, which corresponds the absence of exci-
tations in a Fock space constructed with positive Killing frequency modes as the one-particle states. In the
Boulware vacuum, our accelerated observer sees no particles at all. However, the short distance divergence
of the two-point function does not have the flat space form as the horizon is approached, and the expectation
value of the stress energy tensor becomes singular.
The second remark is that it was important that we started with an observer very close to the horizon.
Only for such an observer is the acceleration high enough, and therefore the timescale a−1 short enough,
that the vacuum fluctuations can be taken to have the universal flat space form independent of the details of
the state of the field and the curvature of the spacetime. Thus, for example, it would be incorrect to argue
that an unaccelerated observer at infinity must (because he is unaccelerated) see no particles, since there
is no a priori justification for assuming the state there looks like the Minkowski vacuum. The lesson of the
Hawking eﬀect is that the state at infinity in fact does not look like the Minkowski vacuum. For every ω a wavepacket can be constructed which is concentrated arbitrarily close to the horizon and
has arbitrarily high frequency with respect to the time of some fixed free-fall observer crossing the horizon
or, equivalently, with respect to the aﬃne parameter u along an ingoing null geodesic that plays the role
of u = t− z in the Rindler horizon case. Thus, provided the state near the horizon looks, to a free-fall
observer at very short distances, like the Minkowski vacuum, we can conclude that it can also be described
as a correlated state of Boulware quanta with the same structure as (3.29). In particular, the state restricted
to the exterior of the horizon is a thermal one, with Boltzmann factor exp(−λ/2π) = exp(−
¯ hω/TH ), where
TH = ¯ hκ/2π is the Hawking temperature.
What is diﬀerent in the black hole case is how these pairs of thermal quanta propagate. In flat space
they continue to swim in parallel on either side of the horizon. In a black hole spacetime the gravitational
tidal force peels them apart. Mathematically, since the wavefronts propagate at fixed u, and u =−ξe−η
,
ξ scales exponentially with η along a wavefront, increasing toward the future and decreasing toward the
past. Once ξ starts to be of order the curvature radius, the Rindler approximation for the metric breaks
down. Thus, toward the future, the ingoing quanta eventually plunge into the singularity, while the outgoing
quanta eventually climb away from the horizon, partially backscatter oﬀ the angular momentum barrier and
the curvature, and partially emerge to infinity as exponentially redshifted thermal quanta at the Hawking
temperature. To every Hawking particle there is a negative Killing energy “partner” that falls into the black
hole. It is the negative energy carried by this partner that is presumably responsible for the mass loss of the
hole.
The number of p-particles reaching infinity thus takes the Planck form,
Np = Γp(e¯ hω/TH
− 1)−1
, (3.30)
where the coeﬃcient Γp is the fraction of p-particles that make it out to infinity rather than being backscat-
tered into the black hole. This is sometimes called the greybody factor since it indicates the emissivity of the
black hole which is not that of a perfect blackbody. Another name for Γp is the absorption coeﬃcient for
the mode p, since it is equal to the fraction of p-particles that would be absorbed by the black hole if sent
in from infinity.
The above reasoning shows that the Hawking radiation is a consequence of the assumption that the state
near the horizon is the vacuum as viewed by free-fall observers at very short distances. Let us call a state
with this property a free-fall vacuum. The derivation of the Hawking eﬀect is not complete until on has
shown that the free-fall vacuum at the horizon indeed results from a generic state prior to collapse of the
matter that formed the black hole. This is a reasonable sounding proposition, since the initial state is the
vacuum for the ultra high frequency modes, and the time and length scales associated with the collapse
are much longer than those associated with such modes. Hawking carried out this step of the argument by
following the mode υ all the way backwards in time along the horizon, through the collapsing matter, and
out to past null infinity I−, using the geometrical optics approximation. At I− the mode still has purely
positive free-fall frequency, so since it is in the vacuum at I− it is in the free-fall vacuum at the horizon. There is something disturbing about Hawking’s reasoning however. As the wavepacket is propagated back-
wards in time along the horizon, it is blueshifting exponentially with respect to Killing time. For the very
first Hawking quanta that emerge after a black hole forms this is perhaps not so serious, since they have not
experienced much blueshifting. But for quanta that emerge a time t after the black hole formed, there is a
blueshift of order exp(κt). For a Schwarzschild black hole, κ = 1/2Rs, so after, say, t = 1000Rs, the blueshift
factor is exp(500). That is, the ingoing mode has frequency exp(500) times the frequency of the outgoing
Hawking quantum at infinity. For a solar mass black hole, the factor is exp(105) after only 2 seconds have
passed.
Needless to say, we cannot be confident that we know what physics looks like at such arbitrarily high,
“transplanckian” frequencies. Of course if exact local lorentz invariance is assumed, then any frequency
can be Doppler shifted down to a low frequency, just by a change of reference frame. But the unlimited
extrapolation of local lorentz invariance to arbitrary boost factors (and the associated infinite density of
states) must be regarded with skepticism. This puzzle can in one sense be sidestepped since, as indicated above, the only role of the transplanckian
ancestor was, for Hawking, to guarantee that one has a free-fall vacuum at short distances near the horizon.
This condition on the state could also plausibly arise in a theory which looks very diﬀerent from ordinary
relativistic field theory at short distances, and in which there are no transplanckian ancestors.
However, this raises the question of how to account for the outgoing black hole modes if they do not have
transplanckian ancestry. Where else could they come from? It seems that they could come from ingoing
modes that are converted into outgoing modes in the neighborhood of the horizon (see figure 3.5). This
ridiculous sounding possibility actually occurs in simple linear field theories in which the wave equation is
modified by the addition of higher derivative terms in the spatial directions perpendicular to some preferred
local time axis [Unruh 1995, Brout et. al, 1995, Corley and Jacobson 1996, Jacobson 1996]. Similar mode
conversion processes occur in many situations where linear waves with a nonlinear dispersion relation prop-
agate in an inhomogeneous medium. There are examples from plasma waves, galactic spiral density waves,
Andreev reflection in superfluid textures, sound waves, and surface waves. If mode conversion accounts for the origin of the outgoing black hole modes, then the ancestors are
probably planckian, but not trans-planckian, modes. Their detailed form would depend on the physics at
the planck scale (or at a lower energy scale for new physics). However, from the analysis of the linear
models referred to above, it is clear that, for black holes that are large compared to the new length scale, the
Hawking spectrum of black hole radiation is remarkably insensitive to these details. For such large black holes
the most significant consequence for the Hawking eﬀect is for stimulated emission. In principle, one could
produce stimulated emission of Hawking radiation by sending in particles in the (presumably planckian)
ancestor modes of the Hawking quanta at any time after the black hole formed, rather than having to send
in transplanckian particles before the collapse. When Bekenstein first proposed the GSL (2.9) he was not thinking that A would ever decrease. The only
question was whether it would necessarily increase enough to compensate for entropy that falls across the
horizon. However, since a black hole emits Hawking radiation, and therefore loses mass, the area of its
horizon must shrink. This is not in contradiction with Hawking’s area theorem, since the quantum field
carries negative energy into the black hole, whereas Hawking assumed a positive energy condition on matter.
It does, however, pose a potential threat to the GSL.
Hawking’s calculation of the black hole temperature determined the coeﬃcient of proportionality between
the black hole entropy and A/¯ hG to be 1/4. The GSL thus takes the form. Classically, the problem was that one could lower a box with entropy to the horizon of a black hole, dropping
it in after almost all of its energy had been extracted at infinity. In such a process the generalized entropy
would decrease (cf. section 2.2).
Bekenstein’s proposal to evade this violation of the GSL was to suggest that there is a universal upper
bound on the entropy that can be contained in a box of a given “size” R and energy E: S ≤ 2πER. Thus,
since a box of size R could not get any closer than R to the horizon, it might necessarily still deliver enough
energy to the black hole to maintain the GSL. He argued that this is so in various thought experiments, but
there were objections. One obvious objection is that the bound seems to restrict the number of independent
species of particles that might exist in nature, since more species lead to a greater possible entropy. It
would be strange if the validity of the GSL imposed a restriction on the number of species. Originally,
Bekenstein argued that this was the way it was. Later he argued that when the Casimir energies are taken into account the bound holds independent of the number of species. In the meantime, Unruh and Wald
argued convincingly that no such bound is needed to uphold the GSL.
The essential point made by Unruh and Wald is that the interaction of the box with the quantum fields
outside the horizon cannot be neglected. Far from the hole a static box sees the Hawking radiation, while
close to the hole it sees the Unruh radiation as a result of its acceleration. Analyzing the process in the
accelerating frame, the box experiences a buoyancy force owing to the fact that the temperature of the Unruh
radiation is higher on the lower side of the box than on the upper side. At the point where the energy of
the displaced Unruh radiation is equal to the energy E of the box, the buoyancy force is just great enough
to float the box. If the box is then pushed further in it acquires more energy, so the energy delivered to
the hole is minimized by dropping the box at the floating point. When the box is dropped into the hole the
entropy change of the hole is (from the first law) ∆SBH = E/TH . But the entropy Sbox of the box must
be less than or equal to the entropy of thermal radiation with the same volume and energy, since thermal
radiation maximizes entropy. That is, Sbox must be less than or equal to the entropy of the displaced Unruh
radiation, which has energy E and entropy E/TH . Thus the SBH + Soutside necessarily increases, so the
GSL holds.
It is somewhat peculiar to base the argument on the Unruh radiation which is not even seen by an
inertial observer. Unruh and Wald point out that the stress tensors “seen” by the two observers diﬀer by
the conserved stress-tensor of the Boulware vacuum. Because it is separately conserved, this diﬀerence will
not aﬀect the result for any observable like the tension in the rope or the total energy transferred.
In the inertial viewpoint, the reason the box floats is that as it is lowered it maintains the vacuum in the
accelerated frame, i.e. the Boulware vacuum, which has negative energy density relative to the surrounding
Unruh or Hartle-Hawking vacua. Evidently, as it is lowered, the box must radiate positive energy and fill
with negative energy until at the floating point its total energy equals zero. The Unruh-Wald analysis also shows that energy can be extracted from a black hole faster than it would
naturally evaporate by Hawking radiation, even if it is nonrotating and neutral. One can lower an open
box to near the horizon, and then close it. It will be full of Unruh radiation. Now slowly lifting it back
out to infinity it will arrive at infinity full of radiation with some Killing energy Erad. The work done in
the cycle is the energy required to lift this radiation, i.e. the diﬀerence (1− χbot)Erad, between its Killing
energy at infinity and at the bottom. This work is less than the energy extracted, so energy conservation
implies that one has somehow extracted the energy χbotErad from the black hole! Since Erad is proportional
to T 4
bot ∝ χ−4
botT 4
H , the extracted energy is arbitrarily large.1
How can one understand the mass loss by the black hole? When the box is closed, the interior is in
the local vacuum state, whose essentially zero energy density is comprised of a negative Boulware vacuum
energy density plus a positive thermal Unruh energy density. As the box is lifted out, the contribution of
the negative Boulware energy density drops (eventually to zero) as the acceleration drops, but the thermal
Unruh contribution survives. The negative Boulware energy flows out of the box and into the black hole,
decreasing its mass.
How is this all explained from the inertial viewpoint? As the box is lifted back up, it radiates negative
energy into the black hole and fills up with positive energy. One way to see this is as an eﬀect of radiation
by (nonuniformly) accelerating mirrors, together with the fact that the lower face of the box experiences a
greater acceleration than the upper face. At this stage it is clear that black holes are really thermodynamic systems with an actual temperature and
entropy. What remains to be understood however is the meaning of this entropy in terms of statistical
mechanics. Somehow the entropy should be the logarithm of the number of independent states of the black
hole. Understanding how to count these states would constitute a significant step forward in the quest to
understand quantum gravity.
It should be said at the outset that the subject of this section lies at the wild frontier of black hole ther-
modynamics. While many interesting and presumably important facts are known, and significant progress
continues to be made, there is not yet agreement on a single correct viewpoint. I shall therefore discuss a
wide range of ideas, pointing out their interconnections, but not insisting on one unified approach.
The fact that the black hole entropy is even finite is already puzzling. A box of radiation at fixed energy
and volume has a finite entropy because the box imposes a long wavelength cutoﬀ and the total energy
imposes a short wavelength cutoﬀ. The Hilbert space describing the radiation field inside the box at fixed
energy is thus finite dimensional, and the microcanonical entropy is just the logarithm of its dimension. A
black hole in a box at fixed energy would also have a short wavelength cutoﬀ (at the box) but, as emphasized
by ’t Hooft, according to standard quantum field theory it has no long wavelength cutoﬀ (at the box). The
reason is that the horizon is an infinite redshift surface. The wavevector of any outgoing mode diverges at
the horizon, and is redshifted down to a finite value at the box. The entropy of each radiation field around a
black hole is therefore infinite due to a divergence in the mode density at the horizon, so it seems the black
hole entropy must also diverge.
We shall see below that this divergence is equivalent to a divergence in the renormalization of Newton’s
constant, or rather in 1/G. Thus one point of view is that it should be absorbed by “counter terms”, and
only the total, renormalized entropy is relevant. To many physicists this does not seem satisfactory however,
since one expects that entropy should count dimensions in Hilbert space, which should not be subject to
infinite subtractions. A possible resolution is that some mechanism cuts oﬀ the short wavlength modes at
the horizon, so that the entropy (and the renormalization of G−1) is finite.
This subject will be pursued further below, where we discuss the various diﬀerent interpretations and
calculations of black hole entropy that have been proposed. Before beginning this journey however, let us
stop to consider what kind of a cutoﬀ mechanism is called for. Given that the GSL seems to be true, one is led to the conclusion that A/4 (setting ¯ hG = 1) must be the
most entropy that can be contained in a region surrounded by a surface of area A. To maximize the volume
one would take a sphere, and if there were more entropy than A/4, but no black hole, one could simply add
more mass until a black hole formed, at which point the entropy would go down to A/4, violating the GSL.
Thus the entropy must have been less than A/4 to begin with.
’t Hooft argued that the inescapable implication of this is that the true space of quantum states in a finite
region must be finite dimensional and associated with the two-dimensional boundary of the region rather
than the volume. Thus it is not enough even if the system is like a fermion field on a lattice of finite spacing.
Rather, the states in the region must be somehow determined by a finite-state system on a boundary lattice!
’t Hooft made the analogy to a hologram, and the idea was dubbed by Susskind the holographic hypothesis.
From a classical viewpoint, the holographic hypothesis may correspond to a statement about the phase
space of a gravitating system surrounded by a surface of area A that is not inside a black hole. It is not
inconceivable that this phase space is compact with a volume that scales as the area. If something like this is
true, then the holographic hypothesis could just be a straightforward consequence of quantizing a gravitating
system.
On the other hand, it has been suggested by ‘t Hooft and Susskind that the holographic hypothesis can
only be incorporated into physics with a radical change in the foundations of the subject. If so, it provides
a tantalizing hint as to the nature of that change. There are some suggestions that string theory might
be headed in the required direction, or perhaps something very diﬀerent like a cellular automaton model is
correct. For the remainder of this section I will ignore the holographic hint however, and continue to discuss
the problem from the point of view of local field theory. Bekenstein’s original idea was that the entropy of a black hole is the logarithm of the number of ways it
could have formed. This is closely related to the Boltzman definition of entropy as the number of microstates
compatible with the macrostate.
Hawking noted that a potential problem arises if one contemplates increasing the number of species of
fundamental fields. There would seem to be more ways of forming the black hole, however the entropy is
fixed at A/4. Hawking’s resolution of this was that the black hole will also radiate faster because of the
extra species, so that there would be less phase space per species available for forming the hole. Presuming
these two eﬀects balance each other, the puzzle would be resolved. This argument was further developed by
Zurek and Thorne, whose analysis makes it uneccessary to presume that the two eﬀects cancel. Building up
the black hole bit by bit, adding energy to the thermal “atmosphere” just outside the horizon, they argue
that the entropy is equal to the logarithm of the number of ways of making the black hole, independent of
the number of species.
Note that to conclude that the actual value of the black hole entropy A/4¯ hG is independent of the
number of species, one must assume that the value of Newton’s constant is also independent of the number
of species. This is by no means clear however, since the low energy eﬀective G is renormalized by the vacuum
fluctuations of all quantum fields. If a fundamental theory could determine G, there is no reason to think it
would come out to be independent of the number of species.
The Zurek-Thorne interpretation sounds a lot like it is identifying the black hole entropy with the entropy
of the thermal bath seen by accelerated observers outside the horizon. Actually, this is not the case. Besides the divergence, which might be cut oﬀ in some way, there is another problem with the idea that
the thermal or entanglement entropies of quantum fields be identified with black hole entropy. Namely, this
entropy depends on the number of diﬀerent fields in nature, whereas the black hole entropy is universal,
always equal to A/4¯ hG.
Various resolutions to the species problem have been suggested. The most natural one to my mind is
that the renormalized Newton constant, which appears in the Bekenstein-Hawking entropy A/4¯ hG, depends
on the number of species in just the right way to absorb all species dependence of the black hole entropy.
To understand this point, we must include the gravitational degrees of freedom in our description, which we
do in the next subsection.
It should be remarked that the formal nature of the argument used to establish the equality ρext =
exp(−βH) left us on somewhat shaky ground. It may be that entanglement and thermal entropies are not
exactly the same. This issue is somewhat superseded by the considerations of the next subsection, in which
the coupling of the matter and gravitational degrees of freedom is allowed for. Shortly after the Hawking eﬀect was discovered, Gibbons and Hawking proposed a formulation of quantum
gravitational statistical mechanics that enabled them to compute the black hole entropy, and they got the
right answer. Their approach was nevertheless not generally regarded as the final word, for several reasons
to be discussed below, which is why people pursued the question in the ways already described above. In
fact, Gibbons and Hawking even noted that their approach contains the thermal entropy of quantum fields
as a one-loop quantum correction. The Gibbons-Hawking approach will now be described. The basic idea is to imitate standard methods of
handling thermodynamic ensembles in other branches of physics. Thus, the goal is to compute the partition
function Z= T r exp(−βH) for the system of gravitational and matter fields in thermal equilibrium at
temperature T , from which the entropy and other thermodynamic functions can be evaluated. In fact it is
better in principle to consider the microcanonical ensemble rather than the canonical one. This is because
the canonical ensemble is unstable for a gravitating system. If a black hole is in a large heat bath at the
Hawking temperature, a small fluctuation to larger mass will cause its temperature to drop, which leads
to a runaway growth of the hole. Conversely, a small fluctuation to smaller mass will lead to a runaway
evaporation of the hole.
This instability can be controlled by putting the black hole in a very small container, with radius less than
3/2 times the Schwarzschild radius (for a Schwarzschild black hole), and somehow holding the temperature
at the box fixed. The reason this eliminates the instability is interesting: although a fluctuation to (say)
larger mass causes the Hawking temperature to drop, this is more than compensated by the fact that the
horizon has moved out, so the local temperature at the box is less redshifted than before, so the hole is in fact
locally hotter than the box. Alternatively one can work with the more physical microcanonical ensemble, in
which the total energy is fixed. In the following we shall for simplicity gloss over these refinements in the
nature of the ensemble, unless explicit mention is called for.
To actually compute Z would seem to require an understanding the Hilbert space of quantum gravity,
something which we still lack. Gibbons and Hawking sidestepped this diﬃculty by passing to a path integral
representation for Z whose semiclassical approximation could be plausibly evaluated. 
After a slow start, you cover ground at an ever increasing rate, crossing 50 billion lightyears, the distance
to the edge of the currently observable Universe, in just over 25 years of your own time.
Does this mean you go faster than the speed of light? No. From the point of view of a person at rest
on Earth, you never go faster than the speed of light. From your own point of view, distances along your
direction of motion are Lorentz-contracted, so distances that are vast from Earth’s point of view appear
much shorter to you. Fast as the Universe rushes by, it never goes faster than the speed of light.
This rosy picture of being able to flit around the Universe has drawbacks. Firstly, it would take a huge
amount of energy to keep you accelerating at 𝑔. Secondly, you would use up a huge amount of Earth time
travelling around at relativistic speeds. If you took a trip to the edge of the Universe, then by the time
you got back not only would all your friends and relations be dead, but the Earth would probably be gone,
swallowed by the Sun in its red giant phase, the Sun would have exhausted its fuel and shrivelled into a
cold white dwarf star, and the Solar System, having orbited the Galaxy a thousand times, would be lost
somewhere in its milky ways.
Technical point. The Universe is expanding, so the distance to the edge of the currently observable Universe
is increasing. Thus it would actually take longer than indicated in the table to reach the edge of the currently
observable Universe. Moreover if the Universe is accelerating, as evidence from the Hubble diagram of Type Ia
Supernovae indicates, then you will never be able to reach the edge of the currently observable Universe,
however fast you go. The distortion of a scene when you move through it at near the speed of light can be calculated most directly
from the Lorentz transformation of the energy-momentum 4-vectors of the photons that you see. The result
is what I call the “Rules of 4-dimensional perspective.”
Figure 1.19 illustrates the rules of 4-dimensional perspective, also called “special relativistic beaming,”
which describe how a scene appears when you move through it at near light speed.
On the left, you are at rest relative to the scene. Imagine painting the scene on a celestial sphere around
you. The arrows represent the directions of light rays (photons) from the scene on the celestial sphere to you
at the center.
On the right, you are moving to the right through the scene, at 0.8 times the speed of light. The celestial
sphere is stretched along the direction of your motion by the Lorentz gamma-factor 𝛾 = 1/√1−0.82 = 5/3
into a celestial ellipsoid. You, the observer, are not at the centre of the ellipsoid, but rather at one of its foci
(the left one, if you are moving to the right). The focus of the celestial ellipsoid, where you the observer are, is
displaced from centre by 𝛾𝑣= 4/3. The scene appears relativistically aberrated, which is to say concentrated
ahead of you, and expanded behind you.
The lengths of the arrows are proportional to the energies, or frequencies, of the photons that you see. Special relativity was unsatisfactory almost from the outset. Einstein had conceived special relativity by
abolishing the aether. Yet for something that had no absolute substance, the spacetime of special relativity
had strikingly absolute properties: in special relativity, two particles on parallel trajectories would remain
parallel for ever, just as in Euclidean geometry.
Moreover whereas special relativity neatly accommodated the electromagnetic force, which propagated
at the speed of light, it did not accommodate the other force known at the beginning of the 20th century,
gravity. Plainly Newton’s theory of gravity could not be correct, since it posited instantaneous transmission
of the gravitational force, whereas special relativity seemed to preclude anything from moving faster than
light, Exercise 1.23. You might think that gravity, an inverse square law like electromagnetism, might satisfy
a similar set of equations, but this is not so. Whereas an electromagnetic wave carries no electric charge, and
therefore does not interact with itself, any wave of gravity must carry energy, and therefore must interact
with itself. This proves to be a considerable complication.
A partial solution, the principle of equivalence of gravity and acceleration, occurred to Einstein while
working on an invited review on special relativity (Einstein, 1907). Einstein realised that “if a person falls
freely, he will not feel his own weight,” an idea that Einstein would later refer to as “the happiest thought of
my life.” The principle of equivalence meant that gravity could be reinterpreted as a curvature of spacetime. It is not always possible to cover a manifold with a single chart, that is, with a coordinate system such
that every point of spacetime has a unique coordinate. A simple example of a 2-dimensional manifold that
cannot be covered with a single chart is the 2-sphere 𝑆2, the 2-dimensional surface of a 3-dimensional sphere,
as illustrated in Figure 2.2. Inevitably, lines of constant coordinate must cross somewhere on the 2-sphere.
At least two charts are required to cover a 2-sphere.
When more than one chart is necessary, neighbouring charts are required to overlap, in order that the
structure of the manifold be consistent across the overlap. General relativity postulates that the mapping
between the coordinates of overlapping charts be at least doubly differentiable. A manifold subject to this
property is called differentiable.
In practice one often uses coordinate systems that misbehave at some points, but in an innocuous fashion. The weak principle of equivalence states that: “Gravitating mass equals inertial mass.” General relativity
satisfies the weak principle of equivalence, but then so also does Newtonian gravity.
Einstein’s principle of equivalence is actually two separate statements: “The laws of physics in a
gravitating frame are equivalent to those in an accelerating frame,” and “The laws of physics in a non-
accelerating, or free-fall, frame are locally those of special relativity.”
Einstein’s principle of equivalence implies that it is possible to remove the effects of gravity locally by going
into a non-accelerating, or free-fall, frame. The structure of spacetime in a non-accelerating, or free-fall, frame
is locally inertial, with the local structure of Minkowski space. By locally inertial is meant that at each point
of spacetime it is possible to choose coordinates such that (a) the metric at that point is Minkowski, and (b)
the first derivatives of the metric are all zero1. In other words, Einstein’s principle of equivalence asserts the
existence of locally inertial frames. The general expression (2.114) for the commutator of the covariant derivative reveals the meaning of the
torsion and Riemann tensors. The torsion and Riemann tensors describe respectively the displacement and the
Lorentz transformation experienced by an object when parallel-transported around a curve. Displacements
and Lorentz transformations together constitute the Poincaré group, the complete group of symmetries of
flat spacetime.
How can an object detect a displacement when parallel-transported around a curve? If you go around
a curve back to the same coordinate in spacetime where you began, won’t you necessarily be at the same
position? This is a question that goes to heart of the meaning of spacetime. To answer the question, you
have to consider how fundamental particles are able to detect position, orientation, and velocity. Classically,
particles may be structureless points, but quantum mechanically, particles possess frequency, wavelength,
spin, and (in the relativistic theory) boost, and presumably it is these properties that allow particles to
“measure” the properties of the spacetime in which they live. For example, a Dirac spinor (relativistic spin- 1
2
particle) Lorentz transforms under the fundamental (spin- 1
2 ) representation of the Lorentz group, and is
thus endowed with precisely the properties that allow it to “measure” boost and rotation, §14.10. The Dirac
wave equation shows that a Dirac spinor propagating through spacetime varies as ∼𝑒𝑖𝑝𝜇𝑥𝜇, whose phase
encodes the displacement of the Dirac spinor. Thus a Dirac spinor could potentially detect a displacement
through a change in its phase when parallel-transported around a curve back to the same point in spacetime.
Since a change in phase is indistinguishable from a spatial rotation about the spin axis of the Dirac spinor,
operationally torsion rotates particles, whence the name torsion.
In fairy tales, a wizard will say the magic words of a spell, or inscribe a set of magic symbols, and the world will change. Fireballs shoot from their hands; the hero is revived from an endless slumber; a couple falls in love. We smile when we hear such stories. They seem quaint but implausible. Common sense tells us that merely speaking certain words, or inscribing certain symbols, cannot cause such changes in the world. And yet our scientific theories are not so different. By speaking the words or inscribing the symbols of such a theory, we can greatly deepen our understanding of the world. That new understanding then enables us to change the world in ways that formerly seemed impossible, even inconceivable. Consider quantum mechanics, one of our deepest theories. It has helped us create lasers, semiconductor chips, and magnetic resonance imaging. One day it will likely help us create quantum computers, perhaps even quantum intelligences. Quantum mechanics is magic that actually works.

In this essay, we explain quantum mechanics in detail. We will describe all the principles of quantum mechanics in depth, nothing held back. It's not a handy-wavy treatment, of the kind often found in articles written for a general audience. While such articles can be entertaining, trying to learn quantum mechanics by reading them is like learning to play basketball by merely watching basketball being played. This essay will get you out on the mathematical court of quantum mechanics. Of course, you won't learn to slam dunk, at least not yet. But the essay will ground you in an understanding of the fundamentals of quantum mechanics, which you can later build upon and extend.

To read the essay, you must first understand the quantum circuit model of quantum computation. If you're not familiar with quantum circuits, you can learn about them from our earlier essay, Quantum Computing for the Very Curious. You may wish to pause to read that essay now, if you haven't already. Once you've understood that material you shouldn't need any other prerequisites. Indeed, when you learned quantum computing, you learned in passing almost all of quantum mechanics. In Part I of this essay we distill those past ideas, collecting them up into the package known as quantum mechanics.

Although quantum mechanics is not so difficult to learn, it's a theory which has disturbed many people. Here's a few classic quotes on this puzzlement:

I think I can safely say that nobody understands quantum mechanics … Do not keep saying to yourself, if you can possibly avoid it, “But how can it be like that?” because you will get “down the drain”, into a blind alley from which nobody has yet escaped. Nobody knows how it can be like that. – Richard Feynman

I have thought a hundred times as much about the quantum problems as I have about general relativity theory. – Albert Einstein

If quantum mechanics hasn't profoundly shocked you, you haven't understood it yet. – Niels Bohr

If quantum mechanics is not so difficult to learn, why has it so disturbed many great physicists? What do they mean when they say they don't understand it, or are shocked by it? How can a scientific theory be both beautiful and disturbing? In Part II of this essay we'll explore these questions. As we'll see, despite its simplicity quantum mechanics raises striking conceptual problems, problems which are among the most exciting open problems in science today.

The essay is presented in an unusual form. It's an example of what we call a mnemonic essay, written in the mnemonic medium. That means it incorporates new user interface elements intended to make it almost effortless for you to remember and apply the ideas in the essay. The motivator is that most people (ourselves included) quickly forget much of what we read in books and articles. The mnemonic medium takes advantage of what cognitive scientists know about how humans learn, creating an interface which ensures you'll remember the ideas and how to apply them near permanently. More on how that works below.

Part I: The postulates of quantum mechanics

So, what is quantum mechanics? It's fun to wax poetic about it being magic that actually works, or to quote eminent scientists saying no-one really understands it. But while fun, those statements give no direct enlightenment. What is quantum mechanics, really?

Quantum mechanics is simply this: it's a set of four postulates that provide a mathematical framework for describing the universe and everything in it. These postulates reflect ideas you've already seen in the quantum circuit model: how to describe a quantum state; how to describe the dynamics of a quantum system; and so on. But rather than talk abstractly, it's better to just see the first postulate:

The first postulate: state space

Postulate 1: Associated to any physical system is a complex vector space known as the state space of the system. If the system is isolated, then the system is completely described by its state vector, which is a unit vector in the system's state space.
This may seem densely written, but you already know most of it. In particular, we've been working extensively with qubits, and we've seen this postulate in action. For a qubit the state space is, as you know, a two-dimensional complex vector space. The state is a unit vector in that state space (i.e., has length 
1
1). And the condition that the state vector is a unit vector expresses the idea that the probabilities for the outcomes of a computational basis measurement add up to
1
1.

It's not just qubits that have a state space. The first postulate of quantum mechanics tells us that every physical system has a state space. An atom has a state space; a human being has a state space; even the universe as a whole has a state space. Admittedly, the first postulate doesn't tell us what the state space is, for any given physical system. That needs to be figured out on a case-by-case basis. Different types of atoms have different state spaces; a human being has a different (and much more complicated) state space; the universe has a more complicated state space still.

As an example, suppose you're interested in what happens when you shine light on an atom. To give a quantum mechanical description, you'd need a state space to describe atoms and the electromagnetic field (i.e., light). A priori it's not obvious what that state space should be. How many dimensions should the state space have? How do particular physical configurations of atoms and light correspond to states in that state space? By itself, quantum mechanics doesn't answer these questions. It merely does the subtle-but-important job of instructing you to look for answers to these questions. Fortunately, there is a theory, known as quantum electrodynamics (often shortened to QED), which describes how atoms and light interact. Among other things, QED tells us which states and state spaces to use to give quantum descriptions of atoms and light.

In a similar way, suppose we're trying to describe particles like quarks or the Higgs boson. Just as with the atoms-and-light example, quantum mechanics tells us we need figure out the right state spaces and state vectors. But it doesn't tell us what those are. In this case, the additional theory needed is the standard model of particle physics. Like QED, the standard model sits on top of basic quantum mechanics, fleshing it out, telling us things which aren't in the four postulates of quantum mechanics.

More generally, quantum mechanics alone isn't a fully specified physical theory. Rather, it's a framework to use to construct physical theories (like QED). It's helpful to think of quantum mechanics as analogous to an operating system for a computer. On its own, the operating system doesn't do all the user needs. Rather, it's a framework that accomplishes important housekeeping functions, creating a file system, a graphical display and interface, and so on. But users need another layer of application software on top of those basic functions. That application layer is analogous to physical theories like QED and the standard model. The application layer runs within the framework provided by the operating system, but isn't itself part of the operating systemThis analogy was popularized in Scott Aaronson's book “Quantum Computing Since Democritus”; MN believes Scott first heard it in a talk MN gave at the Fields Institute in Toronto in 2001. This wouldn't warrant mention, except when MN uses the analogy, people often respond “oh yes, that's Scott Aaronson's way of thinking about quantum mechanics”. MN doesn't recall where he first heard it, or if the description is original..

In this essay, we won't get deeply into QED or the standard model or the other theories physicists have developed to describe particular physical systems. Rather, our focus is on quantum mechanics itself – the four postulates. For examples, we'll mostly draw on the quantum circuit model, which makes simple and reasonable assumptions about state spaces and state vectors. This model is already an extremely rich arena for studying quantum mechanics. If you wish, you can learn later about QED, the standard model, and so on.

Of course, this means there is still much more to learn, beyond the scope of this essay. Each physical system requires learning its own particular set of recipes for state spaces and state vectors. You already know some such recipes: if we say “let's consider a 3-qubit system”, then we know the state space is just the 8-dimensional complex vector space spanned by the computational basis states 
∣
0
⟩
∣
0
⟩
∣
0
⟩
∣0⟩∣0⟩∣0⟩,
∣
0
⟩
∣
0
⟩
∣
1
⟩
,
…
∣0⟩∣0⟩∣1⟩,…. By contrast, you probably don't know the recipe QED gives telling us how to construct the state space for a couple of hydrogen atoms interacting with the electromagnetic fieldA little more strictly speaking, QED tells us how charged elementary particles like electrons interact with the electromagnetic field. After QED was invented, atomic physicists and quantum opticians figured out how to use it to describe atoms and electromagnetic fields. And so the recipe you'd use would likely come from atomic physics or quantum optics.. While that recipe is different than in the three-qubit example, it's really much the same kind of thing. You learn the rules of the recipe, and then you can figure out the state spaces and quantum states.

Where do the recipes for things like the state space of an atom (or the electromagnetic field) come from? The unglamorous answer is that figuring those things out for the first time was incredibly hard. As in: Nobel prize hard, or even multiple Nobel prize hard. People made lots of guesses, tried lots of different things, trying to figure out states and state spaces (and all the other things), constructed lots of bad theories, and lots of good-but-not-good-enough theories along the way. And, eventually, they came up with some great recipes. Most of the time today we can use one of those recipes off-the-shelf. You go pick up a book about atomic physics, say, and it'll just tell you: use such-and-such a state space, and away you go. But in the background is thousands of instances of trial-and-error by often-frustrated physicists.
The first postulate of quantum mechanics told us how we describe states. What about how states change, that is, the dynamics of a quantum system? That's where the second postulate comes in. In the earlier essay we saw that quantum gates are described by unitary matrices acting on the state space of a quantum system. The second postulate tells us that something very similar is true for any isolated quantum system:
“Quantum mechanics” is the description of the behavior of matter and light in all its details and, in particular, of the happenings on an atomic scale. Things on a very small scale behave like nothing that you have any direct experience about. They do not behave like waves, they do not behave like particles, they do not behave like clouds, or billiard balls, or weights on springs, or like anything that you have ever seen.
Newton thought that light was made up of particles, but then it was discovered that it behaves like a wave. Later, however (in the beginning of the twentieth century), it was found that light did indeed sometimes behave like a particle. Historically, the electron, for example, was thought to behave like a particle, and then it was found that in many respects it behaved like a wave. So it really behaves like neither. Now we have given up. We say: “It is like neither.”
There is one lucky break, however—electrons behave just like light. The quantum behavior of atomic objects (electrons, protons, neutrons, photons, and so on) is the same for all, they are all “particle waves,” or whatever you want to call them. So what we learn about the properties of electrons (which we shall use for our examples) will apply also to all “particles,” including photons of light.
The gradual accumulation of information about atomic and small-scale behavior during the first quarter of the 20th century, which gave some indications about how small things do behave, produced an increasing confusion which was finally resolved in 1926 and 1927 by Schrödinger, Heisenberg, and Born. They finally obtained a consistent description of the behavior of matter on a small scale. We take up the main features of that description in this chapter.
Because atomic behavior is so unlike ordinary experience, it is very difficult to get used to, and it appears peculiar and mysterious to everyone—both to the novice and to the experienced physicist. Even the experts do not understand it the way they would like to, and it is perfectly reasonable that they should not, because all of direct, human experience and of human intuition applies to large objects. We know how large objects will act, but things on a small scale just do not act that way. So we have to learn about them in a sort of abstract or imaginative fashion and not by connection with our direct experience.
In this chapter we shall tackle immediately the basic element of the mysterious behavior in its most strange form. We choose to examine a phenomenon which is impossible, absolutely impossible, to explain in any classical way, and which has in it the heart of quantum mechanics. In reality, it contains the only mystery. We cannot make the mystery go away by “explaining” how it works. We will just tell you how it works. In telling you how it works we will have told you about the basic peculiarities of all quantum mechanics. To try to understand the quantum behavior of electrons, we shall compare and contrast their behavior, in a particular experimental setup, with the more familiar behavior of particles like bullets, and with the behavior of waves like water waves. We consider first the behavior of bullets in the experimental setup shown diagrammatically in Fig. 1–1. We have a machine gun that shoots a stream of bullets. It is not a very good gun, in that it sprays the bullets (randomly) over a fairly large angular spread, as indicated in the figure. In front of the gun we have a wall (made of armor plate) that has in it two holes just about big enough to let a bullet through. Beyond the wall is a backstop (say a thick wall of wood) which will “absorb” the bullets when they hit it. In front of the backstop we have an object which we shall call a “detector” of bullets. It might be a box containing sand. Any bullet that enters the detector will be stopped and accumulated. When we wish, we can empty the box and count the number of bullets that have been caught. The detector can be moved back and forth (in what we will call the x-direction). With this apparatus, we can find out experimentally the answer to the question: “What is the probability that a bullet which passes through the holes in the wall will arrive at the backstop at the distance x from the center?” First, you should realize that we should talk about probability, because we cannot say definitely where any particular bullet will go. A bullet which happens to hit one of the holes may bounce off the edges of the hole, and may end up anywhere at all. By “probability” we mean the chance that the bullet will arrive at the detector, which we can measure by counting the number which arrive at the detector in a certain time and then taking the ratio of this number to the total number that hit the backstop during that time. Or, if we assume that the gun always shoots at the same rate during the measurements, the probability we want is just proportional to the number that reach the detector in some standard time interval.
For our present purposes we would like to imagine a somewhat idealized experiment in which the bullets are not real bullets, but are indestructible bullets—they cannot break in half. In our experiment we find that bullets always arrive in lumps, and when we find something in the detector, it is always one whole bullet. If the rate at which the machine gun fires is made very low, we find that at any given moment either nothing arrives, or one and only one—exactly one—bullet arrives at the backstop. Also, the size of the lump certainly does not depend on the rate of firing of the gun. We shall say: “Bullets always arrive in identical lumps.” What we measure with our detector is the probability of arrival of a lump. And we measure the probability as a function of x. The result of such measurements with this apparatus (we have not yet done the experiment, so we are really imagining the result) are plotted in the graph drawn in part (c) of Fig. 1–1. In the graph we plot the probability to the right and x vertically, so that the x-scale fits the diagram of the apparatus. We call the probability P12 because the bullets may have come either through hole 1 or through hole 2. You will not be surprised that P12 is large near the middle of the graph but gets small if x is very large. You may wonder, however, why P12 has its maximum value at x=0. We can understand this fact if we do our experiment again after covering up hole 2, and once more while covering up hole 1. When hole 2 is covered, bullets can pass only through hole 1, and we get the curve marked P1 in part (b) of the figure. As you would expect, the maximum of P1 occurs at the value of x which is on a straight line with the gun and hole 1. When hole 1 is closed, we get the symmetric curve P2 drawn in the figure. Now we wish to consider an experiment with water waves. The apparatus is shown diagrammatically in Fig. 1–2. We have a shallow trough of water. A small object labeled the “wave source” is jiggled up and down by a motor and makes circular waves. To the right of the source we have again a wall with two holes, and beyond that is a second wall, which, to keep things simple, is an “absorber,” so that there is no reflection of the waves that arrive there. This can be done by building a gradual sand “beach.” In front of the beach we place a detector which can be moved back and forth in the x-direction, as before. The detector is now a device which measures the “intensity” of the wave motion. You can imagine a gadget which measures the height of the wave motion, but whose scale is calibrated in proportion to the square of the actual height, so that the reading is proportional to the intensity of the wave. Our detector reads, then, in proportion to the energy being carried by the wave—or rather, the rate at which energy is carried to the detector.
With our wave apparatus, the first thing to notice is that the intensity can have any size. If the source just moves a very small amount, then there is just a little bit of wave motion at the detector. When there is more motion at the source, there is more intensity at the detector. The intensity of the wave can have any value at all. We would not say that there was any “lumpiness” in the wave intensity. We have already worked out how such patterns can come about when we studied the interference of electric waves in Volume I. In this case we would observe that the original wave is diffracted at the holes, and new circular waves spread out from each hole. If we cover one hole at a time and measure the intensity distribution at the absorber we find the rather simple intensity curves shown in part (b) of the figure. I1 is the intensity of the wave from hole 1 (which we find by measuring when hole 2 is blocked off) and I2 is the intensity of the wave from hole 2 (seen when hole 1 is blocked).
The intensity I12 observed when both holes are open is certainly not the sum of I1 and I2. We say that there is “interference” of the two waves. At some places (where the curve I12 has its maxima) the waves are “in phase” and the wave peaks add together to give a large amplitude and, therefore, a large intensity. We say that the two waves are “interfering constructively” at such places. There will be such constructive interference wherever the distance from the detector to one hole is a whole number of wavelengths larger (or shorter) than the distance from the detector to the other hole. Now we imagine a similar experiment with electrons. It is shown diagrammatically in Fig. 1–3. We make an electron gun which consists of a tungsten wire heated by an electric current and surrounded by a metal box with a hole in it. If the wire is at a negative voltage with respect to the box, electrons emitted by the wire will be accelerated toward the walls and some will pass through the hole. All the electrons which come out of the gun will have (nearly) the same energy. In front of the gun is again a wall (just a thin metal plate) with two holes in it. Beyond the wall is another plate which will serve as a “backstop.” In front of the backstop we place a movable detector. The detector might be a geiger counter or, perhaps better, an electron multiplier, which is connected to a loudspeaker. How can such an interference come about? Perhaps we should say: “Well, that means, presumably, that it is not true that the lumps go either through hole 1 or hole 2, because if they did, the probabilities should add. Perhaps they go in a more complicated way. They split in half and …” But no! They cannot, they always arrive in lumps … “Well, perhaps some of them go through 1, and then they go around through 2, and then around a few more times, or by some other complicated path … then by closing hole 2, we changed the chance that an electron that started out through hole 1 would finally get to the backstop …” But notice! There are some points at which very few electrons arrive when both holes are open, but which receive many electrons if we close one hole, so closing one hole increased the number from the other. Notice, however, that at the center of the pattern, P12 is more than twice as large as P1+P2. It is as though closing one hole decreased the number of electrons which come through the other hole. It seems hard to explain both effects by proposing that the electrons travel in complicated paths.
It is all quite mysterious. And the more you look at it the more mysterious it seems. Many ideas have been concocted to try to explain the curve for P12 in terms of individual electrons going around in complicated ways through the holes. None of them has succeeded. None of them can get the right curve for P12 in terms of P1 and P2.
Yet, surprisingly enough, the mathematics for relating P1 and P2 to P12 is extremely simple. For P12 is just like the curve I12 of Fig. 1–2, and that was simple. What is going on at the backstop can be described by two complex numbers that we can call ϕ1 and ϕ2 (they are functions of x, of course). The absolute square of ϕ1 gives the effect with only hole 1 open. That is, P1=|ϕ1|2. The effect with only hole 2 open is given by ϕ2 in the same way. That is, P2=|ϕ2|2. And the combined effect of the two holes is just P12=|ϕ1+ϕ2|2. The mathematics is the same as that we had for the water waves! (It is hard to see how one could get such a simple result from a complicated game of electrons going back and forth through the plate on some strange trajectory. We shall now try the following experiment. To our electron apparatus we add a very strong light source, placed behind the wall and between the two holes, as shown in Fig. 1–4. We know that electric charges scatter light. So when an electron passes, however it does pass, on its way to the detector, it will scatter some light to our eye, and we can see where the electron goes. If, for instance, an electron were to take the path via hole 2 that is sketched in Fig. 1–4, we should see a flash of light coming from the vicinity of the place marked A in the figure. If an electron passes through hole 1, we would expect to see a flash from the vicinity of the upper hole. If it should happen that we get light from both places at the same time, because the electron divides in half … Let us just do the experiment!
Here is what we see: every time that we hear a “click” from our electron detector (at the backstop), we also see a flash of light either near hole 1 or near hole 2, but never both at once! And we observe the same result no matter where we put the detector. From this observation we conclude that when we look at the electrons we find that the electrons go either through one hole or the other. Experimentally, Proposition A is necessarily true.
What, then, is wrong with our argument against Proposition A? Why isn’t P12 just equal to P1+P2? Back to experiment! Let us keep track of the electrons and find out what they are doing. For each position (x-location) of the detector we will count the electrons that arrive and also keep track of which hole they went through, by watching for the flashes. We can keep track of things this way: whenever we hear a “click” we will put a count in Column 1 if we see the flash near hole 1, and if we see the flash near hole 2, we will record a count in Column 2. Every electron which arrives is recorded in one of two classes: those which come through 1 and those which come through 2. From the number recorded in Column 1 we get the probability P′1 that an electron will arrive at the detector via hole 1; and from the number recorded in Column 2 we get P′2, the probability that an electron will arrive at the detector via hole 2. Is there not some way we can see the electrons without disturbing them? We learned in an earlier chapter that the momentum carried by a “photon” is inversely proportional to its wavelength (p=h/λ). Certainly the jolt given to the electron when the photon is scattered toward our eye depends on the momentum that photon carries. Aha! If we want to disturb the electrons only slightly we should not have lowered the intensity of the light, we should have lowered its frequency (the same as increasing its wavelength). Let us use light of a redder color. We could even use infrared light, or radiowaves (like radar), and “see” where the electron went with the help of some equipment that can “see” light of these longer wavelengths. If we use “gentler” light perhaps we can avoid disturbing the electrons so much.
Let us try the experiment with longer waves. We shall keep repeating our experiment, each time with light of a longer wavelength. At first, nothing seems to change. The results are the same. Then a terrible thing happens. You remember that when we discussed the microscope we pointed out that, due to the wave nature of the light, there is a limitation on how close two spots can be and still be seen as two separate spots. This distance is of the order of the wavelength of light. So now, when we make the wavelength longer than the distance between our holes, we see a big fuzzy flash when the light is scattered by the electrons. We can no longer tell which hole the electron went through! We just know it went somewhere! And it is just with light of this color that we find that the jolts given to the electron are small enough so that P′12 begins to look like P12—that we begin to get some interference effect. And it is only for wavelengths much longer than the separation of the two holes (when we have no chance at all of telling where the electron went) that the disturbance due to the light gets sufficiently small that we again get the curve P12 shown in Fig. 1–3.
In our experiment we find that it is impossible to arrange the light in such a way that one can tell which hole the electron went through, and at the same time not disturb the pattern. It was suggested by Heisenberg that the then new laws of nature could only be consistent if there were some basic limitation on our experimental capabilities not previously recognized. He proposed, as a general principle, his uncertainty principle, which we can state in terms of our experiment as follows: “It is impossible to design an apparatus to determine which hole the electron passes through, that will not at the same time disturb the electrons enough to destroy the interference pattern.” If an apparatus is capable of determining which hole the electron goes through, it cannot be so delicate that it does not disturb the pattern in an essential way. No one has ever found (or even thought of) a way around the uncertainty principle. So we must assume that it describes a basic characteristic of nature.
The complete theory of quantum mechanics which we now use to describe atoms and, in fact, all matter, depends on the correctness of the uncertainty principle. Since quantum mechanics is such a successful theory, our belief in the uncertainty principle is reinforced. But if a way to “beat” the uncertainty principle were ever discovered, quantum mechanics would give inconsistent results and would have to be discarded as a valid theory of nature.
“Well,” you say, “what about Proposition A? Is it true, or is it not true, that the electron either goes through hole 1 or it goes through hole 2?” The only answer that can be given is that we have found from experiment that there is a certain special way that we have to think in order that we do not get into inconsistencies. What we must say (to avoid making wrong predictions) is the following. If one looks at the holes or, more accurately, if one has a piece of apparatus which is capable of determining whether the electrons go through hole 1 or hole 2, then one can say that it goes either through hole 1 or hole 2. But, when one does not try to tell which way the electron goes, when there is nothing in the experiment to disturb the electrons, then one may not say that an electron goes either through hole 1 or hole 2. If one does say that, and starts to make any deductions from the statement, he will make errors in the analysis. This is the logical tightrope on which we must walk if we wish to describe nature successfully. This is the way Heisenberg stated the uncertainty principle originally: If you make the measurement on any object, and you can determine the x-component of its momentum with an uncertainty Δp, you cannot, at the same time, know its x-position more accurately than Δx≥ℏ/2Δp, where ℏ is a definite fixed number given by nature. It is called the “reduced Planck constant,” and is approximately 1.05×10−34 joule-seconds. The uncertainties in the position and momentum of a particle at any instant must have their product greater than or equal to half the reduced Planck constant. This is a special case of the uncertainty principle that was stated above more generally. The more general statement was that one cannot design equipment in any way to determine which of two alternatives is taken, without, at the same time, destroying the pattern of interference. Now in order to do this it is necessary to know what the momentum of the screen is, before the electron goes through. So when we measure the momentum after the electron goes by, we can figure out how much the plate’s momentum has changed. But remember, according to the uncertainty principle we cannot at the same time know the position of the plate with an arbitrary accuracy. But if we do not know exactly where the plate is, we cannot say precisely where the two holes are. They will be in a different place for every electron that goes through. This means that the center of our interference pattern will have a different location for each electron. The wiggles of the interference pattern will be smeared out. We shall show quantitatively in the next chapter that if we determine the momentum of the plate sufficiently accurately to determine from the recoil measurement which hole was used, then the uncertainty in the x-position of the plate will, according to the uncertainty principle, be enough to shift the pattern observed at the detector up and down in the x-direction about the distance from a maximum to its nearest minimum. Such a random shift is just enough to smear out the pattern so that no interference is observed.
The uncertainty principle “protects” quantum mechanics. Heisenberg recognized that if it were possible to measure the momentum and the position simultaneously with a greater accuracy, the quantum mechanics would collapse. So he proposed that it must be impossible. Then people sat down and tried to figure out ways of doing it, and nobody could figure out a way to measure the position and the momentum of anything—a screen, an electron, a billiard ball, anything—with any greater accuracy. Quantum mechanics maintains its perilous but still correct existence.
Let us show for one particular case that the kind of relation given by Heisenberg must be true in order to keep from getting into trouble. We imagine a modification of the experiment of Fig. 1–3, in which the wall with the holes consists of a plate mounted on rollers so that it can move freely up and down (in the x-direction), as shown in Fig. 1–6. By watching the motion of the plate carefully we can try to tell which hole an electron goes through. Imagine what happens when the detector is placed at x=0. We would expect that an electron which passes through hole 1 must be deflected downward by the plate to reach the detector. Since the vertical component of the electron momentum is changed, the plate must recoil with an equal momentum in the opposite direction. The plate will get an upward kick. If the electron goes through the lower hole, the plate should feel a downward kick. It is clear that for every position of the detector, the momentum received by the plate will have a different value for a traversal via hole 1 than for a traversal via hole 2. So! Without disturbing the electrons at all, but just by watching the plate, we can tell which path the electron used.
We should say right away that you should not try to set up this experiment (as you could have done with the two we have already described). This experiment has never been done in just this way. The trouble is that the apparatus would have to be made on an impossibly small scale to show the effects we are interested in. We are doing a “thought experiment,” which we have chosen because it is easy to think about. We know the results that would be obtained because there are many experiments that have been done, in which the scale and the proportions have been chosen to show the effects we shall describe.
The first thing we notice with our electron experiment is that we hear sharp “clicks” from the detector (that is, from the loudspeaker). And all “clicks” are the same. There are no “half-clicks.”
At those places where the two waves arrive at the detector with a phase difference of π (where they are “out of phase”) the resulting wave motion at the detector will be the difference of the two amplitudes. The waves “interfere destructively,” and we get a low value for the wave intensity. We expect such low values wherever the distance between hole 1 and the detector is different from the distance between hole 2 and the detector by an odd number of half-wavelengths. The low values of I12 in Fig. 1–2 correspond to the places where the two waves interfere destructively.
Now let us measure the wave intensity for various values of x (keeping the wave source operating always in the same way). We get the interesting-looking curve marked I12 in part (c) of the figure.P2 is the probability distribution for bullets that pass through hole 2. Comparing parts (b) and (c) of Fig. 1–1, we find the important result that
Postulate 2: The evolution of an isolated quantum system is described by a unitary matrix acting on the state space of the system. That is, the state 
∣
ψ
⟩
∣ψ⟩ of the system at a time 
t
1
t 
1
​	
  is related to the state 
∣
ψ
′
⟩
∣ψ 
′
 ⟩ at a later time 
t
2
t 
2
​	
  by a unitary matrix, 
U
U: 
∣
ψ
′
⟩
=
U
∣
ψ
⟩
∣ψ 
′
 ⟩=U∣ψ⟩. That matrix
U
U may depend on the times 
t
1
t 
1
​	
  and 
t
2
t 
2
​	
 , but does not depend on the states 
∣
ψ
⟩
∣ψ⟩ and 
∣
ψ
′
⟩
∣ψ 
′
 ⟩.
Our quantum gates demonstrate this postulate in action. So, for instance, the Pauli 
X
X gate, also known as the quantum NOT gate, is an example. Here it is shown in the quantum circuit and matrix representations, as well as the explicit action on states:


So too is the Hadamard gate, 
H
H:


And so on, through the controlled-NOT gate, the Toffoli gate, and all the other quantum gates we met earlier.

Why is it unitary matrices which appear in the second postulate? If you try writing a few matrices down on paper, you quickly find that most matrices aren't unitary, or even close to it. Why can't we have a general matrix in the second postulate? One partial explanation, discussed in depth in the earlier essay, is that unitary matrices are the only matrices which preserve length. If we want the quantum state to remain normalized, then unitary matrices are the only matrices which do the trick, since any other matrix will result in the norm changing. That normalization is in turn connected to the requirement that the probabilities of measurement outcomes sum to one. In this sense, the postulates of quantum mechanics form a tightly interconnected web, with requirements like unitarity from one postulate reflecting requirements elsewhere, like normalization of the state vector, or probabilities summing to one.

How to figure out which unitary transformation is needed to describe any particular physical situation? As you might guess from our discussion of the first postulate, the second postulate is silent on this question. It needs to be figured out case by case. Theories like QED and the standard model supply additional rules specifying the exact (unitary) dynamics of the systems they describe. It's as before: quantum mechanics is a framework, not a complete physical theory in its own right. But being told that the correct way to describe dynamics is using unitary transformations on state space is already an incredibly prescriptive statement. And, as before, the quantum circuit model is a useful source of examples, and working with it is a good way to build intuition.
the early 1960s, quantum physics was regarded as one of the most successful theories of all time. It explained a wide range of phenomena to an unprecedented level of accuracy, from the structure of atoms and the formation of chemical bonds, to how lasers and superconductors worked. For some, it was more than just a theory, providing an all-encompassing framework for understanding the micro-world of elementary particles. However, it turned out that the very foundations of that entire framework were built on shaky ground – and the person who noticed wasn’t a physicist but an up-and-coming philosopher.

The debate that resulted not only opened the door to new ways of thinking about those foundations, but also had tucked away within it, overlooked by all the participants at the time, an entirely different philosophical perspective on quantum physics – one that can be traced back to the phenomenological philosopher Edmund Husserl. The impact of that shift in perspective is only now being fully appreciated, offering an entirely novel understanding of quantum mechanics, one that prompts a complete re-evaluation of the relationship between philosophy and science as a whole.

The philosopher who kick-started that debate was Hilary Putnam, who went on to make groundbreaking advances in philosophy of language and philosophy of mind, as well as in computer science, logic and mathematics. In 1961, he responded to a paper offering a resolution of the so-called Einstein-Podolsky-Rosen (EPR) paradox, which appeared to show that the description of reality offered by quantum mechanics could not be complete. In the course of his argument, Putnam pointed out that there was an even more profound problem that lay at the very heart of the theory, as it was standardly understood, and which had to do with one of the most basic of all scientific procedures: measurement.

That problem can be set out as follows. A crucial element in the formalism of quantum mechanics is a mathematical device known as the ‘wave function’. This is typically taken to represent the state of a given system – such as an atom or an electron – as a superposition of all its possible states. So, consider an electron and the property known as ‘spin’. (This is not really the same as the spin put on a ball in a game of baseball or cricket, but the name has stuck.) Spin comes in two forms, labelled ‘up’ and ‘down’, and so when we use the wave function to represent the spin state of our electron as it travels towards our detector, it is as a non-classical superposition of spin ‘up’ and spin ‘down’. However, when we come to measure that spin, the outcome is always one or the other, either ‘up’ or ‘down’, never a superposition of both. How can we account for the transition from that superposition to a definite outcome when we perform a measurement?

This question forms the basis of what came to be known as the ‘measurement problem’. One influential answer emerged from the mind of one of the greatest mathematicians of all time, János (or ‘John’) von Neumann, who was responsible for many important advances, not only in pure mathematics and physics but also in computer design and game theory. He pointed out that when our spin detector interacts with the electron, the state of that combined system of the detector + electron will also be described by quantum theory as a superposition of possible states. And so will the state of the even larger combined system of the observer’s eye and brain + the detector + electron. However far we extend this chain, anything physical that interacts with the system will be described by the theory as a superposition of all the possible states that combined system could occupy, and so the crucial question above will remain unanswered. Hence, von Neumann concluded, it had to be something non-physical that somehow generates the transition from a superposition to the definite state as recorded on the device and noted by the observer – namely, the observer’s consciousness. (It is this argument that is the source of much of the so-called New Age commentary on quantum mechanics about how reality must somehow be observer-dependent, and so on.)

What bothered Putnam was that if we accept von Neumann’s conclusion, then the theory could not be extended to apply to the entire Universe, because that would require an observer existing beyond the physical universe whose consciousness would collapse the superposition of all the Universe’s possible states into one definite one. Either physicists would have to give up the idea that quantum theory was universally applicable, or the standard account of measurement would have to be abandoned.

Putnam’s short paper, published in the journal Philosophy of Science, happened to be read by Henry Margenau, a former physicist turned philosopher of science, who then alerted the Nobel Prize-winning physicist Eugene Wigner. Together they published a response in which they defended von Neumann’s argument and dismissed Putnam’s concern. The debate then went back and forth over several years, the two sides essentially talking past each other until Abner Shimony decisively entered the fray. The holder of two PhDs, one each in philosophy and physics, and a former student of Wigner himself, Shimony subsequently went on to play a leading role in devising the experimental tests of Bell’s theorem (which builds on the EPR result by ruling out certain attempts to supplement quantum mechanics). He weighed in behind Putnam. The central concern was this: just how does consciousness effect this transition from a superposition to a definite state? With no satisfactory answer forthcoming, it appeared that Putnam and Shimony had won the day, clearing the philosophical ground for alternative approaches such as Hugh Everett’s Many-Worlds interpretation, according to which there is no such transition at all and each element of the superposition is realised as a definite outcome, albeit in a different branch of reality or alternative world.
ather than drawing on von Neumann’s chain argument as presented in his own text, which was quite technical and had only recently been translated into English, both sides in the debate actually cited core passages from what Wigner referred to as a ‘little book’ by two other physicists, Fritz London and Edmond Bauer. Originally published in French in 1939, La théorie de l’observation en mécanique quantique (The Theory of Observation in Quantum Mechanics) formed part of a series of semi-popular expositions of the latest advances in science and technology, covering everything from anthropology to zoology. At just 51 pages long, the pamphlet aimed to set out clearly and accessibly not only the basic framework of the quantum mechanical treatment of measurement, but the role of consciousness in that process. Regarded by both sides as a mere summary of von Neumann’s argument, it was anything but.

Both Bauer and London worked in Paris at the time, the former at the prestigious Collège de France and the latter at the Institut Henri Poincaré. Bauer was an excellent teacher and the first in France to teach the new quantum theory. London, however, was in a different league entirely. He earned his quantum mechanical spurs by showing how the theory could explain chemical bonding, leading his collaborator Walter Heitler to exclaim: ‘Now we can eat chemistry with a spoon!’ London went on to successfully apply the theory to superconductivity with his brother Heinz, and then used it to explain the superfluid behaviour of liquid helium, subsequently publishing a two-volume book on these phenomena that became a classic in the field.
What is phenomenology? It can be summarised as a fundamental enquiry into the correlations between mental acts or experiences, the objects that these acts or experiences are about, and the contents or (where appropriate) meanings of these acts or experiences. Its primary tool is known as the epoché (from the Greek for ‘suspension’), which requires the phenomenological investigator to ‘bracket off’ the world around us, and suppress the ‘natural attitude’ that blithely takes that world to be objective. The idea is to break the hold that such an attitude has on us so that we may uncover the fundamental epistemological and metaphysical presuppositions underpinning it.

It is important to note that this bracketing off does not mean ‘denying the existence of’. Adopting this manoeuvre does not amount to an endorsement of scepticism, nor should it be understood as leading to solipsism. Instead, by using the epoché, we can hold up to scrutiny both the supposedly objective world and that natural attitude, thereby reorienting our understanding of both. What we then discover is that the relationship between our consciousness and the world should be understood as ‘correlative’, in the sense that both exist in a ‘mutually dependent context of being’, as Maximilian Beck put it in 1928. This is not to say that consciousness and the world should be conceived of as existing independently of one another prior to being related, nor that the former somehow creates the latter. Rather, it is the correlations that constitute both consciousness and the world.

There is a great deal more to phenomenology than this, and indeed not everyone agrees with the correlationist interpretation. But it is this view that underpins London and Bauer’s ‘little book’ on measurement in quantum mechanics, which played such a crucial role in the debate over the role of consciousness in that process. Recall that Margenau and Wigner defended the standard view that consciousness somehow produces a definite observation from a quantum superposition, taking London and Bauer to be simply summarising von Neumann’s argument. Putnam and Shimony, on the other hand, questioned that whole approach, pressing the point that it was unclear how consciousness could actually yield such a result. However, both sides in that debate missed the core point of the ‘little book’. London and Bauer actually went beyond von Neumann in adopting a phenomenological perspective on the issue, according to which consciousness plays a constitutive role via the correlation between the observer and the world. They themselves make it clear how they are departing from the standard approach in the introduction:

Without intending to set up a theory of knowledge, although they were guided by a rather questionable philosophy, physicists were so to speak trapped in spite of themselves into discovering that the formalism of quantum mechanics already implies a well-defined theory of the relation between the object and the observer, a relation quite different from that implicit in naive realism, which had seemed, until then, one of the indispensable foundation stones of every natural science.
What London and Bauer are saying here is that quantum mechanics must be understood as not just a theory like any other – that is, as about the world in some sense – but as a theory of knowledge in itself, insofar as it ‘implies a well-defined theory of the relation between the object and the observer’. This represents a crucial difference from classical physics as it is usually understood. From the perspective of quantum mechanics, the relationship between the observer and the object being observed must now be seen as quite different from that which underpins the previous stance of ‘naive realism’, which is typically adopted with regard to classical mechanics and which holds that objects exist entirely independently of all observation and possess measurable properties, whether these are actually measured or not. That view must now be abandoned. The core of London and Bauer’s text then represents an attempt to articulate the nature of that relationship between the observer and the object or system being measured.

London and Bauer radically depart from von Neumann’s argument at a crucial juncture. In setting out the chain of correlations, from detector + system to observer’s body + detector + system, they do not stop at the consciousness of the observer but also include this in the overall quantum superposition. It is this move that expresses in physical terms the phenomenological idea of the ‘mutually dependent context of being’, so that not just the body of the observer but their consciousness is also correlated, quantum mechanically, with the system under investigation.

How do we go from that correlation, manifested through the quantum superposition, to having a definite belief corresponding to our observation of a certain measurement outcome? Here, London and Bauer insist that

it is not a mysterious interaction between the apparatus and the object that produces a new [wave function] for the system during the measurement. It is only the consciousness of an ‘I’ who can separate himself from the former function … and, by virtue of his observation, set up [or, in the original French, ‘constituer’, constitute] a new objectivity in attributing to the object henceforward a new function.
In other words, the transition from a superposition to a definite state is not triggered in some mysterious fashion by the consciousness of the observer and, as a result, Putnam and Shimony’s concern regarding how consciousness can cause a definite state to be produced is simply sidestepped. Instead, what we have is a separation of consciousness from the superposition, leading to a ‘new objectivity’, that is, a definite belief on the part of the observer and a definite state attributed to the system.

How can the observer step outside her own perspective and into that of another?

This separation is effected, as London and Bauer explain, via

a characteristic and quite familiar faculty which we can call the ‘faculty of introspection’. [The observer] can keep track from moment to moment of his own state. By virtue of this ‘immanent knowledge’ he attributes to himself the right to create his own objectivity – that is, to cut the chain of statistical correlations.
And, in a typed note inserted by London in his own copy of the manuscript, he wrote:

Accordingly, we will label this creative action as ‘making objective’. By it the observer establishes his own framework of objectivity and acquires a new piece of information about the object in question.
It is this characteristic and familiar act of reflection that cuts the chain of statistical correlations expressed by quantum theory as a set of nested superpositions, and keeps the twin phenomenological poles of those correlations – namely consciousness and the world – mutually separate. And so, on the one hand, the system is objectified, or ‘made objective’, in the sense of having a definite state attributed to it, and, on the other, the observer acquires a definite belief state through this objectifying act of reflection.

London and Bauer were not unaware of the radical nature of what they were saying. In the final section of their work, they acknowledge that, as a result, it might appear that the idea of scientific objectivity itself was under threat. Indeed, this is a general problem with all such views that deny that states of systems are observer-independent – how can the observer step outside her own perspective and into that of another, and thereby establish what London and Bauer call a ‘community of scientific perception’ about what constitutes the object of the investigation? Their response is to insist that ‘one always has the right to neglect the effect on the apparatus of the “scrutiny” of the observer.’

To understand what they mean here, it is important to realise that the word ‘scrutiny’ in this quote is translated from ‘regard’ in the original French text, where the placing of this term between quote marks in the original text itself indicates its significance. Within phenomenology, this ‘regard-to’ is a fundamental reflective act, which, when directed to something, can be understood in terms of consciousness grasping or seizing upon it. When it comes to mental processes, their existence is, then, guaranteed by that ‘regard’. However, although physical objects are likewise brought within the purview of consciousness by the ‘regard’, their existence is not, of course, guaranteed by it (note that phenomenology does not amount to a form of solipsism).

So, when it comes to the measurement apparatus, operated by the physicist in the ‘natural attitude’, we can neglect the effect on it of this ‘regard-to’. And we can further justify our ‘right’ to do so by appeal to what is now known as quantum decoherence. Although there were indications of the core principle behind this as early as 1929, the framework was clearly set out in the early 1970s. The basic idea is that, when a system interacts with the measurement apparatus, the coherence associated with the superposition appears to be lost among the many more physical degrees of freedom offered by the apparatus compared with that of the system. As a result, even though this process does not, in itself, lead to a definite state, as the superposition is still present, the behaviour of the measurement apparatus may be regarded as classical to all intents and purposes. The ‘scrutiny’ or ‘regard’ of the observer can be ignored (unlike the case when we do consider the transition to a definite state) and a collective scientific perception achieved.

In a lecture given in 1925, shortly before the first papers on the new quantum mechanics appeared, Husserl made it clear that phenomenology needed to be brought down from the abstract heights of philosophical theorising and expressed in concrete terms, stating:

The task that now arises is how to make [the] correlation between constituting subjectivity and constituted objectivity intelligible, not just to prattle about it in empty generality but to clarify it in terms of all the categorial forms of worldliness, in accordance with the universal structures of the world itself.
A little more than 10 years later, in The Crisis of European Sciences and Transcendental Philosophy (1936), his final, magisterial, incomplete work, Husserl decried the way in which the mathematisation of ‘material nature’ had led to its conceptualisation as distinct from consciousness. In order for this split to be healed, he argued, there needed to be a fundamental shift back to ‘the universe of the subjective’ via the adoption of the phenomenological stance. Only then can the results of science in general, and physics in particular, be properly grasped and understood.

Merleau-Ponty argued that the observer should not be placed beyond the reach of the wave function

Unfortunately, Husserl died the year before London and Bauer’s ‘little book’ was published, but if he had read it he might have appreciated how they had, in effect, responded to both of his concerns. By couching the relationship between observer and system within a phenomenological framework, they clarified the correlation between constituting subjectivity and constituted objectivity in terms of that specific ‘categorial [form] of worldliness’ represented by quantum mechanics. Furthermore, London and Bauer showed that, by virtue of embodying that correlative relationship between ourselves and the world, quantum mechanics, conceived of phenomenologically, bridges the psychophysical divide and restores within physics, and indeed science as a whole, ‘the universe of the subjective’.

This restoration of the phenomenological nature of London and Bauer’s text – something that was entirely overlooked in the debate between Putnam and Shimony, on one side, and Margenau and Wigner, on the other – is therefore important firstly for illustrating how that particular philosophical movement was entwined with the development of quantum physics, and secondly for situating this ‘little book’ at an early stage in the evolution of a philosophical approach to that theory that has been largely ignored within the philosophy of physics, at least until recently.

This is not to say that other writers in the phenomenological tradition failed to bring quantum mechanics within their philosophical purview. Gurwitsch and Patrick Heelan also emphasised the phenomenological role of human consciousness in the measurement process, again citing London and Bauer. Maurice Merleau-Ponty, one of the most prominent phenomenological thinkers, similarly engaged with quantum theory while in Paris, and likewise argued that the observer should not be placed beyond the reach of the wave function, but must be included in the description of reality offered by physics. He went on to have a significant influence on subsequent writers, including Michel Bitbol who, together with his collaborators, has developed a form of eco-phenomenology that allies the phenomenological stance with an approach to quantum mechanics known as QBism. Initially developed by the physicist Christopher Fuchs, this similarly adopts a first-person approach that takes the concepts of agent and experience as fundamental, and understands the wave function as representing not the state of the system, but that of that agent when it comes to their possible future experiences.

Recent developments such as these have converged in a series of conferences, in turn resulting in two landmark collections, both edited and with useful introductions by Harald Wiltsche and Philipp Berghofer: Phenomenological Approaches to Physics (2020), which also covers phenomenological approaches to the theory of relativity, and Phenomenology and QBism (2024).

Within the phenomenological framework, then, one of the central problems of quantum mechanics is resolved or, perhaps better, dissolved, through a subtle but crucial shift to understanding it as a theory of knowledge by virtue of embodying our correlative participation in the world. Whether or not you fully agree with such a philosophical stance, it not only adds a hugely stimulating and potentially fruitful dimension to our understanding of one of the most fundamental constituents of modern physics, but also throws new light on the often-overlooked significance of philosophical reflection in these developments.
However, London was not just a brilliant physicist. He was also keenly interested in philosophy from a young age. As a student at the University of Munich, he came to the attention of Alexander Pfänder, professor of philosophy and righthand man of Edmund Husserl, the founder of phenomenology. Indeed, London’s thesis on the nature of scientific theories was published in the leading phenomenological journal of the time, the Jahrbuch für Philosophie und Phänomenologische Forschung (the ‘Yearbook for Philosophy and Phenomenological Research’), which was edited by Pfänder himself. And this was no mere youthful fixation; London maintained his interest in phenomenology throughout his career. While in Paris, he had long discussions about physics and philosophy with his friend Aron Gurwitsch, who, like London, had an academic background in both subjects and went on to help establish phenomenology in the United States.
That debate, as historically important as it was for the further development of the foundations of quantum mechanics, also contained a significant philosophical element that was completely overlooked for many years, and that not only offers an entirely novel response to Putnam and Shimony’s concern, but also opens the door to a fundamentally different understanding of quantum physics. What they didn’t notice was the phenomenological angle.
More broadly: although quantum mechanics reached its final form in the 1920s, physicists spent much of the remainder of the twentieth century figuring out what unitary dynamics, state spaces, and quantum states are needed to describe this or that system. You can't just solve this problem once: optical physicists had to do it for light, atomic physicists for atoms, particle physicists have been doing it for the entire pantheon of particles described in the standard model of particle physics. Still, although there's much more to learn about the application of these two postulates, already they give us a remarkably constraining framework for thinking about what the world is and how it can change.
A point we've glossed over is the use of the term isolated in the first postulate. In particular, the first postulate tells us that every physical system has a state space, but inserts the qualifier isolated when saying which physical systems have a state vector. By isolated we mean a system that's not interacting with any other system. Of course, most physical systems aren't isolated. An atom will interact with its surroundings (for instance, it may be hit by a photon, or perhaps be affected by the charge of a nearby electron). A human being isn't an isolated physical system either – we're constantly being bombarded by light, cosmic rays, and all sorts of other things.
Isaac Newton believed that light is composed of particles, and he had good reason
to think so. All wave motion exibits interference and diﬀraction eﬀects, which are
the signature of any phenomenon involving waves. Newton looked for these eﬀects
by passing light through small holes, but no diﬀraction eﬀects were observed. He
concluded that light is a stream of particles.
One of Newton’s contemporaries, Christian Huygens, was an advocate of the wave
theory of light. Huygens pointed out that the refraction of light could be explained
if light moved at diﬀerent velocities in diﬀerent media, and that Newton’s inability
to find diﬀractive eﬀects could be due simply to the insensitivity of his experiments.
Interference eﬀects are most apparent when wavelengths are comparable to, or larger
than, the size of the holes. If the wavelength of light were very small compared to the
size of the holes used by Newton, interference eﬀects would be very hard to observe.
Huygens turned out to be right. More sensitive optical experiments by Young
(1801) and Fresnel demonstrated the interference and diﬀraction of light, and mea-
surements by Foucault (1850) showed that the speed of light in water was diﬀerent
from the speed of light in air, as required to explain refraction. Then Maxwell, in
1860, by unifying and extending the laws of electricity and magnetism, demonstrated
that electric and magnetic fields would be able to propagate through space as waves,
traveling with a velocity v = 1/√µ0ϵ0, which turned out to equal, within experimen-
tal error, the known velocity of light. Experimental confirmation of the existence of
electromagnetic waves followed shortly after, and by the 1880s the view that light is
a wave motion of the electromagnetic field was universally accepted.
It is a little ironic that following this great triumph of the wave theory of light,
evidence began to accumulate that light is, after all, a stream of particles (or, at least,
light has particle properties which somehow coexist with its wave properties). The
first hint of this behavior came from a study of black-body radiation undertaken by
Max Planck, which marks the historical beginning of quantum theory.
Any object, at any finite temperature, emits electromagnetic radiation at all pos-
sible wavelengths. The emission mechanism is simple: atoms are composed of nega-
tively charged electrons and positively charged nuclei, and upon collision with other
atoms these charges oscillate in some way. According to Maxwell’s theory, oscillating
charges emit (and can also absorb) electromagnetic radiation. So it is no mystery
that if we have a metallic box whose sides are kept at some constant temperature
T , the interior of the box will be filled with electromagnetic radiation, which is con-
stantly being emitted and reabsorbed by the atoms which compose the sides of the
box. There was, however, some mystery in the energy distribution of this radiation
as a function of frequency. Since there can only be an integer number of photons n at any given frequency, each
of energy hf, the energy of the field at that frequency can only be nhf. Planck’s
restriction on energies is thereby explained in a very natural, appealing way.
Except for one little thing. ”Frequency” is a concept which pertains to waves;
yet Einstein’s suggestion is that light is composed of particles. The notion that
the energy of each ”particle” is proportional to the frequency of the electromagnetic
”wave”, which in turn is composed of such ”particles”, seems like an inconsistent mix
of quite diﬀerent concepts. However, inconsistent or not, evidence in support of the
existence of photons continued to accumulate, as in the case of the Compton eﬀect.
2.3 The Compton Eﬀect
Consider an electromagnetic wave incident on an electron at rest. According to clas-
sical electromagnetism, the charged electron will begin to oscillate at the frequency of
the incident wave, and will therefore radiate further electromagnetic waves at exactly
the same frequency as the incident wave. Experiments involving X-rays incident on
free electrons show that this is not the case; the X-rays radiated by the electrons are
a frequencies lower than that of the incident X-rays. Compton explained this eﬀect
in terms of the scattering by electrons of individual photons.
According to special relativity, the relation between energy, momentum, and mass
is given by
E= p2c2 + m2c4 (2.14)
For particles at rest (p= 0), this is just Einstein’s celebrated formula E= mc2
.
For a particle moving at the speed of light, such as a photon, the rest mass m = 0;
otherwise the momentum would be infinite, since momentum p is related to velocity
v via the relativistic expression
p=
mv
(2.15)
1−
v2
c2
Then if, for a photon, m = 0 and E= hf, and given the relation for waves that
v = λf, we derive a relation between photon momentum and wavelength
E
p=
=
c
hf
c
=
h
λ (2.16)
where λ is the wavelength of the electromagnetic wave; in this case X-rays.
Now suppose that a photon of the incident X-ray, moving along the z-axis, strikes
an electron at rest. The photon is scattered at an angle θ relative to the z-axis,
while the electron is scattered at an angle φ, as shown in Fig. [2.4]. If⃗
p1 denotes the
momentum of the incident photon,⃗
p2 denotes the momentum of the scattered photon,
and⃗
pe is the momentum of the scattered electron, then conservation of momentum
tells us that
In general such non-isolated systems don't have their own quantum state! In fact, we saw an example in the earlier essay. 
Of course, it could be that the relation (2.28) is simply a practi-
cal limit on measurement; a particle might have a definite position and momentum
despite our inability to measure those quantities simultaneously. But the diﬃculty
could also be much more profound: if a physical state is simply the mathematical
representation of the outcome of an accurate measurement process (a view which
was advocated in Fig. [1.5] of Lecture 1) and if accurate numbers (x, p) are never
an outcome of any measurement, then perhaps we are mistaken in thinking that a
physical state corresponds to definite values of (x, p). In other words, the origin of
the uncertainty could be due to trying to fit a square peg (the true physical state,
whatever that may be) into a round hole (the set (x, p)). At the very least, if x and
p cannot be measured simultaneously, then there is certainly no experimental proof
that the classical state is the true physical state. This view is obviously a very radical
option; for the moment we only raise it as a possibility, and turn to the mystery of
the stability of the atom.
2.5 The Bohr Atom
Atoms have radii on the order of 10−10 m, and have masses on the order of 10−26 kg.
In 1911, Ernest Rutherford studied the internal structure of atoms by bombarding
gold foil with α-particles from radioactive Cesium. By studying the scattering of
the α-particles by the gold atoms (a topic we will turn to at the end of the course),
Rutherford found that almost all the mass of the atom is concentrated in a positively
charged nucleus, of radius on the order of 10−15 m, i.e. 100,000 times smaller than
the radius of the atom itself. The nucleus is surrounded by much lighter, negatively
charged electrons, which collectively account for less than 1/2000th of the total mass
of the atom. Atomic structure was pictured as analogous to the solar system, with the
nucleus playing the role of the sun, orbited by much lighter electrons (the ”planets”),
bound in their orbits by Coulomb attraction to the nucleus.
However, orbital motion is a form of accellerated motion, and electrons are charged
particles. According to electromagnetic theory, an accellerating charged particle ra-
diates electromagnetic waves. As electrons move in their orbits, they should be con-
stantly radiating energy in the form of electromagnetic waves, and as the electrons
lose energy, they should spiral into the nucleus; a process which would take only a
fraction (about 10−10) of a second. By this reasoning, atoms should be about the size
of nuclei, but in fact they are roughly 100,000 times larger. So what accounts for the
stability of the electron orbits; why don’t electrons spiral into the nucleus?
Another mystery of atomic structure was the existence of spectral lines. If a gas
is placed in a discharge tube, with a suﬃciently large voltage diﬀerence maintained
at opposite ends of the tube, the gas glows. But, unlike thermal radiation (which
occurs due to random collisions among atoms) the light emitted from the discharge
tube is not spread diﬀusely over a broad range of frequencies, but is composed instead
of discrete, very definite wavelengths. Matter in the solid state consists of atoms in a regular (”crystalline”) array of
some kind, and the atomic structure of solids is determined by X-ray diﬀraction. X-
rays, being a form of wave motion, reflect oﬀ the atoms in the array, and interfere to
form a pattern which can be calculated from the principles of physical optics, given a
knowlege of the structure of the array, and the wavelength of the X-rays. The inverse
problem, finding the structure of the array given the X-ray interference pattern, is
the subject of X-ray crystallography.
In 1927 Davisson and Germer, in an eﬀort to check De Broglie’s hypothesis that
electrons are associated with wave motion, directed a beam of electrons at the surface
of crystalline nickel. The electrons were reflected at various angles, and it was found
that the intensity of the reflected electrons, as a function of reflection angle, was
identical to the interference pattern that would be formed if the beam were instead
composed of X-rays. Assuming that the electron beam was indeed some sort of
wave, the wavelength could be determined from the intensity pattern of the reflected
electrons. The wavelength λ was found to equal, within experimental error, the
de Broglie prediction λ= h/p, where p is the electron momentum, determined by
accellerating the incident beam of electrons across a known potential V. Apart from
brilliantly confirming the existence of ”de Broglie waves”, this is an experiment with
extraordinary and profound implications. To discuss these implications, it is useful
to consider an experimental setup less complicated than that used by Davisson and
Germer, in which the electrons are directed at an inpenetrable barrier containing two,
very narrow, slits. First, however, we need an expression for the wavefunction of de
Broglie waves. When a conclusion (eq. (3.35) in this case) turns out to be false, and the reasoning
which led to the conclusion is correct, then there must be something wrong with
the premises. The premises in this case are that the electrons in the beam travel
independently of one another, and that each electron in the beam passed through
either slit A or slit B. More generally, we have assumed that the electron, as a
”pointlike particle”, follows a trajectory which takes it through one of the two slits
on its way to the screen.
It is easy to check whether the interference eﬀect is due to some collective inter-
action between electrons in the beam. We can use a beam of such low intensity that
the electrons go through the barrier one by one. It takes a little longer to accumulate
the data needed to compute PA(y), PB (y), and P (y), but the results are ultimately
the same. So this is not the explanation.
Then perhaps we should stop thinking of the electron as a point-like object, and
start thinking of it as literally a de Broglie wave? A wave, after all, can pass through
both slits; that is the origin of interference. The problem with this idea is that de
Broglie waves expand. At slits A and B, the wave is localized just in the narrow
region of the open slits, but as the wave propagates towards the screen it expands,
typically spanning (in a real electron diﬀraction experiment) a region on the order
of 10 cm across. Then one would detect ”parts” of an electron. Likewise, a wave of such
dimensions should leave a diﬀuse glow on a photographic plate, or a thick cylindrical
track through a cloud chamber. None of this is ever observed. No matter how big
the de Broglie wave of a single electron becomes (as big as a baseball, as big as a
house...), only a single ”click” of a geiger counter is heard, only a sharp ”dot” on a
photographic plate is recorded. An electron is not, literally, a wave. As far as can be
determined from measurements of electron position, electrons are discrete, point-like
objects.
If electrons are pointlike objects, then one could imagine (with the help of a pow-
erful microscope) actually observing the barrier as the electrons reach it, to determine
if they go through slit A, or slit B, or somehow through both. If one would perform
such an experiment,2 the result is that each electron is indeed observed to pass either
through slit A or slit B (never both), but then the interference pattern is wiped out!
Instead, one finds the uniform distribution of eq. (3.35). Thus if an electron is forced
(essentially by observation) to go through one or the other of the two slits, the inter-
ference eﬀect is lost. Interference is regained only if the electron is not constrained to
a trajectory which has passed, with certainty, through one or the other slit.
We are left with the uncomfortable conclusion that electrons are pointlike objects
which do not follow definite trajectories through space. This sounds like a paradox:
how then can electrons get from one point to another? It is actually not a paradox,
but it does require a considerable readjustment of our concept of the physical state
of electrons (not to mention atoms, molecules, and everything else that makes up the
physical Universe). This will be the subject of the next lecture. If in fact we could determine the position and momentum to an accuracy greater
than (6.25), then the particle would be left in a physical state with ∆x∆p <¯ h/2. But
according to eq. (6.24) there are no such physical states. Therefore, measurements of
that kind are impossible.
The Uncertainty Principle is an unavoidable consequence of quantum theory, and
if one could design a measurement process which would be more accurate than the
bound (6.25), then quantum theory would be wrong. We have already discussed one
attempt to measure x and p simultaneously, in the example of the Heisenberg mi-
croscope. In that case it was found that the photon composition of light, combined
with the Rayleigh criterion of resolution, results in ∆x∆p ≈¯ h, which is in agree-
ment with the Uncertainty Principle. There have been other ingenious proposals for
measurements which would violate the Uncertainty Principle, especially due to Al-
bert Einstein in his discussions with Niels Bohr.1 A careful study of these proposals
always reveals a flaw. In much the same way that the existence of perpetual motion
machines is ruled out by the Second Law of Thermodynamics, the existence of an
apparatus which would give a precise determination of position and momentum is in
conflict with the nature of physical states in quantum mechanics.
However, the example of Heisenberg’s microscope often leads to a misunderstand-
ing that the Uncertainty Principle is simply saying that ”the observation disturbs
what is being observed.” It is true that an observation usually changes the physical
state of the observed system. But it is not true that this is full content of the Un-
certainty principle. If position-momentum uncertainty were only a matter of light
disturbing the observed particle, then we would be free to imagine that a particle
really has a definite position and definite momentum at every moment in time, but
that the physical properties of light prevent their accurate simultaneous determina-
tion. This interpretation is wrong, because if a particle had a definite position and
momentum at every moment of time, then the particle would follow a definite tra-
jectory. We have already seen that the assumption that particles follow trajectories
is inconsistent with electron interference. The Heisenberg principle is best under-
stood, not as a slogan ”the observation disturbs what is observed,” but rather as a
consequence of the nature of physical states in quantum mechanics, which cannot be
simultaneously eigenstates of position and momentum. Despite the stationarity, this energy eigenstate is obviously just the limit of a non-
stationary situation, in which an incoming wavepacket is scattered backwards by an
infinite potential. During the (long) interval in which the incoming wavepacket (with
∆p very small) reflects from the end of the tube, the wavefunction near the end of the
tube looks very much like the energy eigenstate (8.10). In fact, in the ∆p → 0 limit,
we can easily identify the part of the eigenstate that corresponds to the incoming
wavepacket (φinc) and the part which corresponds to the scattered wavepacket (φref ).
This is a general feature of unbound states: an unbound stationary state can be
viewed as the limit of a dynamical situation, in which an incoming wavepacket of
a very precise momentum is scattered by a potential. Part of the unbound state is
identified as the incoming wave, other parts represent the scattered waves.
As a second example, consider a particle wavepacket incident on the potential well
of Fig. [8.1]. The incoming wavepacket is shown in Fig. [8.10a]. Upon encountering
the potential, the wavepacket splits into a reflected wavepacket, moving backwards
towards x = −∞, and a transmitted wavepacket which has passed through the well
and is moving on towards x = +∞, as shown in Fig. [8.10c].1 At some intermediate
time, the incoming and reflected wavepackets overlap, and as we take ∆p → 0 for the
incoming wave, the incoming and reflected waves overlap over a very large region. We live in a world which is almost, but not quite, symmetric. The Earth is round,
nearly, and moves in an orbit around the Sun which is circular, almost. Our galaxy
looks roughly like a spiral. Human beings and most animals have a left-right symme-
try; the left-hand side looks the same as the right-hand side, more or less. Starfish
have a pentagonal symmetry, table salt has a cubic symmetry, and in general Nature
abounds in geometrical shapes of various kinds.
The symmetry which is apparent in large structures (animals, planets, galaxies...)
is even more evident at the very small scales that are the domain of quantum physics.
Symmetries appear in the arrangement of atoms in solids, in the arrangement of
electrons in atoms, and in the structure of the nucleus. Symmetry principles are
especially important for understanding the variety and behavior of the elementary
particles, out of which all larger structures are built. Stars die. Their long lives are interesting and complex, occasionally culminating
in fantastic explosions (supernovas) that can briefly outshine entire galaxies. The
details are the subject matter of some other course. For us, it is suﬃcient to know
that every ”living” star balances its immense gravitational force, which tends to crush
all the atoms of the star inward to a single point, with an equally enormous outward
pressure, due to the heat produced by nuclear fusion. Eventually, any star will exhaust
its nuclear fuel, and then the gravitational force, unopposed, crushes the atoms of the
star to a fantastic density. In the course of collapse, for stars greater than about one
solar mass, the atomic electrons are absorbed by protons in the nuclei, via the process
e− + p → n + ν (16.1)
and the massless neutrinos ν, due to their weak interaction with all other matter,
escape into space. At this stage, all of the particles composing the star are neutrons,
the density of the star approximates that of atomic nuclei, and the dead star is known
as a ”neutron star.” For stars with masses less than about four solar masses, that is
the end of the story: the cold dead star remains as a neutron star until the end of
time. But this brings up the question: what force can there possibly be, within the
cold neutron star, that is capable of opposing the mighty gravitational force, which
would otherwise crush all the matter of the star to a single point?
It seems incredible that all this astrophysical drama should have anything at all
to do with the apparently more mundane question of why some materials conduct
electricity, and some don’t. Nevertheless, the physics of dead stars, and that of quite
ordinary solids, are related in certain unexpected ways. Both neutron stars, and the
electrons in cold metals, are examples of what are known as degenerate F ermi
gases. We begin by taking up the question of how is it possible for certain solids to
conduct electricity. A crystalline solid is a regular array of atoms, and, at first sight, conduction of
electricity is a mystery: if electrons are bound to atoms, how is possible for them to
move through the solid under the influence of a small electric field? The answer is
that in a crystal, not all of the electrons are actually bound to the atoms; in fact,
some of the electrons in the metal behave more like a gas of free particles, albeit with
some peculiar characteristics which are due to the exclusion principle.
To understand how electrons in a crystal can act as a gas, it is useful to solve for
the electron energy eigenstates in a highly idealized model of a solid, known as the
Kronig-Penny model, which makes the following simplifications:
S1. The solid is one-dimensional, rather than three-dimensional. The N atoms are
spaced a distance a from one another. In order that there are no special eﬀects
at the boundaries, we consider a solid has no boundary at all, by arranging the
atoms in a circle as shown in Fig. [16.1].
S2. Instead of a Coulomb potential, the potential of the n-th atom is represented
by a delta-function attractive potential well
Vn(x) =−gδ(x− xn) (16.2)
where xn is the position of the n-th atom.
S3. Interactions between electrons in the 1-dimensional solid are ignored.
Obviously, these are pretty drastic simplifications. The important feature of this
model, which it shares with realistic solids, is that the electrons are moving in a
periodic potential. For purposes of understanding the existence of conductivity, it
is the periodicity of the potential, not its precise shape (or even its dimensionality)
which is the crucial feature.
Arranging the atoms in a circle, as in Fig. [16.1], means that the position variable
is periodic, like an angle. Just as θ + 2π is the same angle as θ, so the position x + L
is the same position as x, where
L= N a (16.3)
is the length of the solid. Let the position of the n-th particle be xn = na, n =
0, 1, ..., N− 1, the potential then has the form. Thus there can a maximum of two
electrons (spin up and spin down) at any allowed energy in an energy band.
At the lowest possible temperature (T= 0 K), the electrons’ configuration is the
lowest possible energy consistent with the Exclusion Principle. A perfect Insulator
is a crystal in which the electrons completely fill one or more energy bands, and there
is a gap in energy from the most energetic electron to the next unoccupied energy
level. In a Conductor, the highest energy band containing electrons is only partially
filled.
In an applied electric field the electrons in a crystal will tend to accellerate, and
increase their energy. But...they can only increase their energy if there are (nearby)
higher energy states available, for electrons to occupy. If there are no nearby higher
energy states, as in an insulator, no current will flow (unless the applied field is so
enormous that electrons can ”jump” across the energy gap). In a conductor, there
are an enormous number of nearby energy states for electrons to move into. Electrons
are therefore free to accellerate, and a current flows through the material.
The actual physics of conduction, in a real solid, is of course far more complex
than this little calculation would indicate. Still, the Kronig-Penny model does a
remarkable job of isolating the essential eﬀect, namely, the formation of separated
energy bands, which is due to the periodicity of the potential.
16.2 The Free Electron Gas
In the Kronig-Penney model, the electron wavefunctions have a free-particle form in
the interval between the atoms; there is just a discontinuity in slope at precisely the
position of the atoms. In passing to the three-dimensional case, we’ll simplify the
situation just a bit more, by ignoring even the discontinuity in slope. The electron
wavefunctions are then entirely of the free particle form, with only some boundary
conditions that need to be imposed at the surface of the solid. Tossing away the
atomic potential means losing the energy gaps; there is only one ”band,” whose
energies are determined entirely by the boundary conditions. For some purposes
(such as thermodynamics of solids, or computing the bulk modulus), this is not such
a terrible approximation.
We consider the case of N electrons in a cubical solid of length L on a side. Since
the electrons are constrained to stay within the solid, but we are otherwise ignoring
atomic potentials and inter-electron forces, the problem maps directly into a gas of
non-interacting electrons in a cubical box. According to the principles of quantum mechanics, physical states are normalized
vectors in Hilbert space, and the sum of two vectors is another vector. If that vector
is normalized, it is a physical state too. So the wavefunction (22.3) is, from this point
of view, entirely kosher: it is a physically allowed state of the two-particle system. But
then if we ask: what is the state of particle 1, when the two-particle system is in state
(22.3), we discover that we can’t answer the question! Particle 1 is not in state ψA,
nor is it in state ψC ; neither is it in a linear combination such as cψA(x1) + dψC (x1).
The fact is, the state of particle 1 has become entangled with that of particle 2; it is
impossible to specify the state of either particle separately, even if the two particles
are non-interacting, and very far apart.
The fact that the quantum states of composite systems can be inseparable (or
entangled) in this way was first noticed by Einstein, Podolsky, and Rosen (”EPR”)
in 1933. The consequences of this entanglement are truly mind-bending, even by the
generous mind-bending standards of quantum mechanics.
22.1 The EPR Paradox
Einstein, Podolsky and Rosen were of the opinion that quantum mechanics is in some
way incomplete, because it leads to (what they considered to be) a paradox in certain
circumstances. To illustrate their point, we consider the following experiment (Fig.
22.1). Every time a button is pushed, two spin 1/2 particles are ejected, in opposite
directions, each particle eventually passing through a detector which measures the
spin of the particle along a given axis. The detectors can be rotated, so that the spin
along any axis, in particular along the x- and z-axes, can be measured. After many
trials, the following result is reported:
A. Both detectors are set to measure spin along the z-axis. Whenever the spin of the
particle on the left is found to be spin up, and this occurs 50% of the time, the
spin of the particle on the right is found to be spin down. Likewise, whenever
the particle on the left is found to be spin down, which is the other 50% of the
time, the particle on the right is found to be spin up.
This means that by measuring the z-component of the spin of the particle on the
left, we can determine the z-component of the spin of the particle on the right, without
actually interacting with the particle on the right. Suppose, in a particular run, that
the particle which moves to the left (which we’ll now call ”L”) is found to be spin-
up. With observable O2 and state Ψ chosen in this way, then by following the standard
rules of quantum theory we can compute the probabilities for finding outcomes GG,
RR, RG, and GR, for switch settings 12, 21, 22, and 11. These probabilities are
shown in Fig. 22.3 They are in complete agreement with the asserted results (A), (B)
and (C) above.
In this way, Bell’s Theorem is proven. There are physical states in quantum
mechanics which lead to predictions that can never be reproduced by a local hidden
variables theory. These physical states are always of the ”entangled” form, in which
it is impossible to deduce the state of either particle separately.
Of course, the example constructed here, although suﬃcient to prove theorem,
is rather special. Bell’s theorem, in more generality, states that if the measured
values of various observables violate certain inequalities, then those results cannot be
explained by any local hidden variables theory. This brings us to an experimental
question: Maybe quantum mechanics is wrong! Can we actually observe, in the
laboratory, results which violate the Bell inequalities (i.e. cannot be explained by
local hidden variable theories)? The relevant experiments were performed by Aspect and his collaborators in the
1970s. The two particles were two photons produced by the decay of Positronium
(an electron-positron bound state). All of the quantum-mechanical predictions were
confirmed. The mysterious non-local behavior of quantum theory, in which a mea-
surement of particle 1 somehow causes the distant particle 2 to jump into a state of
definite spin, cannot be explained by a local hidden variables theory. Defined in this way, the states {ϕn(x2)} do not have to be orthogonal, they do not
even have to be diﬀerent states. If all the {ϕn(x2)} are the same state, then Ψ(x1, x2)
is separable. If at least some of the {ϕn(x2)} are diﬀerent, then Ψ(x1, x2) is entangled.
Now we make the measurement of observable A on particle 1, and one of the
eigenvalues, λ= λk say, is obtained. This means that particle 1 has jumped into
the state φk(x1). But, if the intial state is entangled, it also means that particle 2
has jumped into state ϕk(x2), which it was not in before. In other words, the entire
two-particle state Ψ(x1, x2) has ”jumped” into state φk(x1)ϕk(x2), even if particles
1 and 2 are far apart, and a measurement is performed only on particle 1. This
certainly seems like a non-local influence, and Bell’s Theorem prevents the ”cheap”
explanation that particle 2 was really in state ϕk(x2) (perhaps supplemented by some
”hidden” variables) all along.
Its natural to suppose that if a measurement at detector 1 causes something to
happen to the distant particle 2, then this eﬀect could be used to send messages, in-
stantaneously, between the observer at detector 1 and the observer at detector 2. But
if instantaneous (or in general, faster than light) communication were possible, then
according to special relativity it should also be possible to send messages backwards
in time. This would have some interesting practical applications. A typical scenario
is as follows:
The four accomplices had planned it for years, after coming into pos-
session of two pairs of super-secret - and highly illegal - quantum radios.
Now for the payoﬀ. John, the ringleader, stands at the betting window of
a major racetrack, while his wife, Mary, waits impatiently on the planet
Mars. The third participant, Rajiv, is piloting a rocketship which is head-
ing towards Earth at a very substantial fraction of the speed of light. His
partner, F atima, is on another rocketship, moving parallel to Rajiv and
at equal speed towards Mars. The objective of all these preparations is
for John to learn the name of the winning horse at the Kentucky Derby,
before the race is actually run.
The worldlines of all four participants are indicated on a spacetime diagram shown
in Fig. 22.4. At point A in Fig. 22.4, the race has just ended.
”Niels Bohr”, a long shot paying 200 to 1, has come in first. As Rajiv’s
rocket streaks overhead, John signals the name of the winning horse to
him. Rajiv then uses his quantum radio to send this information instan-
taneously to F atima. When Rajiv signals the name of the winning horse to F atima, she re-
ceives this information at spacetime point B’, just as her rocket is passing
Mars. F atima relays the important information to Mary, and then Mary
contacts John, again by quantum radio, and transfers the message instan-
taneously (in the John-Mary rest frame) to Earth. The message reaches
John at spacetime point B, several minutes before the race is to begin.
John places a very large bet on ”Niels Bohr” to win, and ...
Lets leave the story there. Could entangled states be used to send instantaneous
messages in this way, even in principle? To answer this question, we again consider the
apparatus shown in Fig. 22.2. John is at detector 1. Mary is at detector 2, but has an
arsenal of laboratory equipment available to her, and can measure any observable she
likes. Is there some observation which she could perform on the right-hand particles,
which would tell her
I. whether John has turned on his detector; or
II. if John’s detector is on, whether the switch is on setting 1 or setting 2? Entangled states are the norm, not the exception, in quantum mechanics. Generally
speaking, when any two systems come into interaction, the resulting state of the
combined system will be entangled. In this technical sense of quantum inseparability,
the poet was right: no man is an island. No electron is an island either.
As we have seen, the existence of entangled states means that a measurement
of one system can cause the second system, perhaps very distant from the first, to
jump into one or another quantum state. It is time to return to the EPR criticism:
”Isn’t this situation paradoxical? Doesn’t it imply that something must be wrong
with quantum theory?”
Now first of all, as pointed out by Niels Bohr in his response to EPR, this situation
is not really a paradox. The non-locality pointed out by EPR is certainly very, very
strange. But quantum non-locality is not actually a paradox in the sense of being a
logical contradiction.
”Well then, doesn’t this non-local behavior violate theory of Relativity?” Accord-
ing to relativity theory, after all, no signal can propagate faster than the speed of
light, so how can the state of particle 2 change instantaneously, in the laboratory
reference frame, upon measurement of particle 1? Here we must be careful - the
relativistic prohibition is against information propagating faster than the speed of
light. If such propagation were possible, it would also be possible to send messages
backwards in time, which would raise many other (authentic!) paradoxes. we have just seen that the observer at detector 2 can never, by simply observing one
of the particles, conclude anything about the settings of detector 1. Non-locality is
not a violation of relativity in the sense of information transfer.
At this point, it is worth noting is that non-locality is not really unique to entan-
gled states; some kind of non-local eﬀect occurs in almost any measurement. Con-
sider, for example a single particle, whose wavepacket ψ(x) is distributed uniformly
in a large, dark room, filled with particle detectors. At a given time t= 0 the detec-
tors are suddenly all switched on and, just as suddenly, the position of the particle
is measured. This means that the wavepacket of the particle, which was spread all
over the room at t < 0, suddenly collapses to a narrow gaussian in the region of one
of the detectors at t= 0. This is known as the ”collapse of the wavefunction”, and it
also seems to be a form of non-local behavior. If we think of the particle wavepacket
as some sort of fluid, by what mechanism does it instantaneously collapse?
Now, in volume 1 of these notes I have urged you to avoid thinking of the wavefunc-
tion as representing some sort of material fluid. There are just too many diﬃculties
inherent in that kind of picture. Rather, one should think of Hilbert Space, rather
than ordinary space, as being the arena of dynamics, where the state (vector) of a
system evolves continuously, according to the Schrodinger equation, by rotation. This
change of arena alleviates a lot of the problems associated with non-locality. On the
other hand, I also said that a measurement causes the state to ”jump,” probabilisti-
cally, into one of a number of eigenstates of the quantity being measured.
Its time to face up to the real problem, the hard problem: By what process does
a measurement cause a state to suddenly jump to one of a set of states? Can this
behavior itself be explained quantum mechanically? This very puzzling issue has not
been settled to this day. It is known as the Problem of Measurement. The simplest answer was given by von Neumann, who urges us to follow the chain
of events into the brain of the observer. The detector is in a superposition of ”red
light/green light” states, and it emits photons in a superposition of the corresponding
frequencies. The photons reach the retina of the observer, and certain neurons are
left in a superposition of excited/un-excited states. The message from the retina
travels to the cerebral cortex; very large collections of neurons are now in quantum
superpositions, and the brain remains in such a superposition until, at some point, a
sensation occurs. At the instant of conscious perception, the observer, the detector,
and even the particle, jump into one or the other of the ”up/down, red/green” states.
What von Neumann is suggesting is that human consciousness causes the wave-
function to ”collapse,” with a certain probablity, into one or another of the possible
neural states; the collapse of the wavefunction occurs due to the awareness of the
observer. It follows, since the Schrodinger equation can never cause a wavefunction
to collapse (i.e. cause a pure state to go to a mixture), that the mental function
described as ”awareness” or ”conciousness” cannot be described by the Schrodinger
equation; it is not a physical process in the usual sense.
The notion that there is something special about conciousness, and that it cannot
be explained by the dynamical laws that govern all other physical processes, is anath-
ema to most scientists. It is reminiscent of vitalism; a theory which held that one
must look beyond the usual laws of physics and chemistry to explain the processes
which occur in living organisms. This theory was, of course, long ago discredited by
spectacular advances in molecular biology.
Still, von Neumann’s idea should not be rejected out of hand. Philosophers have
argued for centuries about the so-called mind/body problem, and there exist sophisti-
cated arguments to the eﬀect that, e.g., a computer following a complicated algorithm
to simulate some aspect of human behavior can never actually ”understand” what it
is doing.1 In the absence of any well-established ”Theory of Consciousness,” it is not
entirely obvious that awareness can be explained entirely in terms of the dynamics
of molecules obeying the Schrodinger equation. von Neumann argues that mental
processes simply cannot be explained in this way, due to the absence of superimposed
mental states. His argument, although obviously radical, cannot be immediately
dismissed. Rather, the wavefunction is a compact
representation of the observer’s information about the observables of a given object,
and merely describes the possible outcome of a series of measurements. Put another
way, the wavefunction does not refer to ”physical reality” per se, in the absence of an
observer; it serves only to predict and correllate measurements. If there is no observer,
then no meaning can be attached to a quantum-mechanical wavefunction. From this
point of view, the ”collapse” of the wavefunction does not describe a new physical
process; it is just a change in the information available to the observer, obtained by
measurement.
The main criticism that can be leveled at Bohr’s interpretation is that it becomes
meaningless to speak of the physical state of an object in the absence of an observer.
How, then, can we describe quantum-mechanical processes that may have occured
in nature prior to the evolution of human beings, or events which, for one reason
or another, may have escaped human scrutiny? In classical physics, every object in
the Universe has a physical state, a definite position and momentum, regardless of
whether or not human beings are around to measure that state. Not so for quantum
mechanics, at least in the Bohr interpretation. It is impossible, in this view, to
imagine that any object is in any definite quantum state, without at least an implicit
reference to the Observer.
The fact that human observers are somehow an essential feature of the quantum-
mechanical description of nature is, for some, a very troubling aspect of Bohr’s view.
In general, the Copenhagen interpretation has something of the flavor of logical pos-
itivism, a philosophy which holds that the purpose of science is to correllate mea-
surements, rather than describe some ”objective” reality. ”Objective reality,” in the
positivist view, is a meaningless concept, and science should be formulated entirely
in terms of quantities which can be measured directly. This view, in the hands of
Mach and others, had considerable influence on physics in the early 20th century, and
certainly the Copenhagen interpretation show traces of this influence.
It is a little distressing to think that science is not about Nature, but only about
correllating measurements. Still, the consistency of the Copenhagen interpretation,
and its ability to evade puzzles connected with the apparent non-local ”collapse” of
entangled wavefunctions, should not be underestimated. To the extent that there is
an ”oﬃcial” interpretation of quantum theory, it would be the Copenhagen view. In classical physics the Euler-Lagrange equations are derived from the condition that
the action S[x(t)] should be stationary. These second order equations are equiva-
lent to the first order Hamilton equations of motion, which are obtained by taking
appropriate derivatives of the Hamiltonian function H[q, p]. Now the Hamiltonian
is a quantity that we have encountered frequently in these pages. But what about
the action? It would be surprising if something so fundamental to classical physics
had no part at all to play in quantum theory. In fact, the action has the central
role in an alternative approach to quantum mechanics known as the ”path-integral
formulation.”
Lets start with the concept of a propagator. Given the wavefunction ψ(x, t) at
time t, the wavefunction at any later time t + T can be expressed as
ψ(x, t + T ) = dy GT (x, y)ψ(y, t) (24.1)
where GT (x, y) is known as the propagator, and of course it depends on the Hamilto-
nian of theory. In fact, given a time-independent Hamiltonian with eigenstates
Hφn(x) = Enφn(x) (24.2)
its easy to see that
GT (x, y) =
φn(x)φ∗
n(y)e−iEnT (24.3)
n
Richard Feynman, in 1948, discovered a very beautiful expression for the propagator
in terms of an integral over all paths that the particle can follow, from point y at
time t, to point x at time t + T , with an integrand
eiS[x(t)]/¯ h (24.4)
As we will see, his expression can be taken as a new way of quantizing a mechanical
system, equivalent to the ”canonical” approach based on exchanging observables for
operators. Having derived the path-integral from the Schrodinger equation, one can of course
go in the other direction, i.e. derive the Schrodinger equation starting from the
concept of an integration over paths. We have seen that path-integrals with gaussian
integrands can be evaluated exactly; integrands with non-gaussian terms can often
be evaluated approximately by a perturbation technique. We have also seen that
path-integrals lead to the usual form of the momentum operator. Logically, the path-
integral approach is an alternative to canonical quantization based on commutators;
either method can be used to quantize a classical theory. Why then, have we spent the whole year following the Schrodinger equation ap-
proach? Why not begin with path-integral, and solve bound-state and scattering
problems by that method? In fact, such an approach to teaching non-relativistic
quantum mechanics can be and has been followed, by Feynman himself. The results
are enshrined in a textbook by Feynman and Hibbs. However, no other elementary
textbook, and not many instructors, have followed Feynman’s example. The reason
is simply that, in non-relativistic quantum mechanics, the path-integral is a rather
cumbersome procedure for solving problems, as compared to the Schrodinger equa-
tion.
In relativistic quantum field theory, however, the situation is diﬀerent: for very
many problems it is the path-integral technique which is easier to use, and better
adapted than operator methods to the thorny technicalities that are encountered. As
a bonus, the path-integral formulation is naturally suited to various non-perturbative
approximation methods, such as the Monte Carlo procedure, in cases where pertur-
bation theory cannot be applied. Finally, there are interesting and deep connections
between quantum field theory, based on the Feynam path integral, and statistical
mechanics, based on the analysis of a partition function. But this is a long story, to
be taught in another, more advanced, course.
Introduction and Review 


that are not visible to the eye, including radio waves, microwaves, 
x-rays, and gamma rays. These are the “colors” of light that do not 
happen to fall within the narrow violet-to-red range of the rainbow 
that we can see. 


self-check B 

At the turn of the 20th century, a strange new phenomenon was discov- 
ered in vacuum tubes: mysterious rays of unknown origin and nature. 
These rays are the same as the ones that shoot from the back of your 
TV’s picture tube and hit the front to make the picture. Physicists in 
1895 didn’t have the faintest idea what the rays were, so they simply 
named them “cathode rays,” after the name for the electrical contact 
from which they sprang. A fierce debate raged, complete with national- 
istic overtones, over whether the rays were a form of light or of matter. 
What would they have had to do in order to settle the issue? > 
Answer, p. 563 


Many physical phenomena are not themselves light or matter, 
but are properties of light or matter or interactions between light 
and matter. For instance, motion is a property of all light and some 
matter, but it is not itself light or matter. The pressure that keeps 
a bicycle tire blown up is an interaction between the air and the 
tire. Pressure is not a form of matter in and of itself. It is as 
much a property of the tire as of the air. Analogously, sisterhood 
and employment are relationships among people but are not people 
themselves. 


Some things that appear weightless actually do have weight, and 
so qualify as matter. Air has weight, and is thus a form of matter 
even though a cubic inch of air weighs less than a grain of sand. A 
helium balloon has weight, but is kept from falling by the force of the 
surrounding more dense air, which pushes up on it. Astronauts in 
orbit around the Earth have weight, and are falling along a curved 
arc, but they are moving so fast that the curved arc of their fall 
is broad enough to carry them all the way around the Earth in a 
circle. They perceive themselves as being weightless because their 
space capsule is falling along with them, and the floor therefore does 
not push up on their feet. 


Optional Topic: Modern Changes in the Definition of Light and 
Matter 

Einstein predicted as a consequence of his theory of relativity that light 
would after all be affected by gravity, although the effect would be ex- 
tremely weak under normal conditions. His prediction was borne out 
by observations of the bending of light rays from stars as they passed 
close to the sun on their way to the Earth. Einstein’s theory also implied 
the existence of black holes, stars so massive and compact that their 
intense gravity would not even allow light to escape. (These days there 
is strong evidence that black holes exist.) 


Einstein’s interpretation was that light doesn’t really have mass, but 
that energy is affected by gravity just like mass is. The energy in a light 


Section 0.2 





c/ This telescope picture shows 
two images of the same distant 
object, an exotic, very luminous 
object called a quasar. This is 
interpreted as evidence that a 
massive, dark object, possibly 
a black hole, happens to be 
between us and it. Light rays that 
would otherwise have missed the 
earth on either side have been 
bent by the dark object’s gravity 
so that they reach us. The actual 
direction to the quasar is presum- 
ably in the center of the image, 
but the light along that central line 
doesn’t get to us because it is 
absorbed by the dark object. The 
quasar is known by its catalog 
number, MG1131+0456, or more 
informally as Einstein’s Ring. 


What is physics? 19 


molecule 


neutrons 
lave eo) melo) ars) 





? 
20 Chapter 0 
d / Reductionism. 


beam is equivalent to a certain amount of mass, given by the famous 
equation E = mc?, where c is the speed of light. Because the speed 
of light is such a big number, a large amount of energy is equivalent to 
only a very small amount of mass, so the gravitational force on a light 
ray can be ignored for most practical purposes. 


There is however a more satisfactory and fundamental distinction 
between light and matter, which should be understandable to you if you 
have had a chemistry course. In chemistry, one learns that electrons 
obey the Pauli exclusion principle, which forbids more than one electron 
from occupying the same orbital if they have the same spin. The Pauli 
exclusion principle is obeyed by the subatomic particles of which matter 
is composed, but disobeyed by the particles, called photons, of which a 
beam of light is made. 


Einstein’s theory of relativity is discussed more fully in book 6 of this 
series. 


The boundary between physics and the other sciences is not 
always clear. For instance, chemists study atoms and molecules, 
which are what matter is built from, and there are some scientists 
who would be equally willing to call themselves physical chemists 
or chemical physicists. It might seem that the distinction between 
physics and biology would be clearer, since physics seems to deal 
with inanimate objects. In fact, almost all physicists would agree 
that the basic laws of physics that apply to molecules in a test tube 
work equally well for the combination of molecules that constitutes 
a bacterium. (Some might believe that something more happens in 
the minds of humans, or even those of cats and dogs.) What differ- 
entiates physics from biology is that many of the scientific theories 
that describe living things, while ultimately resulting from the fun- 
damental laws of physics, cannot be rigorously derived from physical 
principles. 


Isolated systems and reductionism 


To avoid having to study everything at once, scientists isolate the 
things they are trying to study. For instance, a physicist who wants 
to study the motion of a rotating gyroscope would probably prefer 
that it be isolated from vibrations and air currents. Even in biology, 
where field work is indispensable for understanding how living things 
relate to their entire environment, it is interesting to note the vital 
historical role played by Darwin’s study of the Galapagos Islands, 
which were conveniently isolated from the rest of the world. Any 
part of the universe that is considered apart from the rest can be 
called a “system.” 


Physics has had some of its greatest successes by carrying this 
process of isolation to extremes, subdividing the universe into smaller 
and smaller parts. Matter can be divided into atoms, and the be- 
havior of individual atoms can be studied. Atoms can be split apart 


Introduction and Review 


into their constituent neutrons, protons and electrons. Protons and 
neutrons appear to be made out of even smaller particles called 
quarks, and there have even been some claims of experimental ev- 
idence that quarks have smaller parts inside them. This method 
of splitting things into smaller and smaller parts and studying how 
those parts influence each other is called reductionism. The hope is 
that the seemingly complex rules governing the larger units can be 
better understood in terms of simpler rules governing the smaller 
units. To appreciate what reductionism has done for science, it is 
only necessary to examine a 19th-century chemistry textbook. At 
that time, the existence of atoms was still doubted by some, elec- 
trons were not even suspected to exist, and almost nothing was 
understood of what basic rules governed the way atoms interacted 
with each other in chemical reactions. Students had to memorize 
long lists of chemicals and their reactions, and there was no way to 
understand any of it systematically. Today, the student only needs 
to remember a small set of rules about how atoms interact, for in- 
stance that atoms of one element cannot be converted into another 
via chemical reactions, or that atoms from the right side of the pe- 
riodic table tend to form strong bonds with atoms from the left 
side. 


Discussion questions 


A I’ve suggested replacing the ordinary dictionary definition of light 
with a more technical, more precise one that involves weightlessness. It’s 
still possible, though, that the stuff a lightbulb makes, ordinarily called 
“light,” does have some small amount of weight. Suggest an experiment 
to attempt to measure whether it does. 


B_ _Heat is weightless (i.e., an object becomes no heavier when heated), 
and can travel across an empty room from the fireplace to your skin, 
where it influences you by heating you. Should heat therefore be con- 
sidered a form of light by our definition? Why or why not? 


C Similarly, should sound be considered a form of light? 


0.3 How to learn physics 


For as knowledges are now delivered, there is a kind of con- 
tract of error between the deliverer and the receiver; for he 
that delivereth knowledge desireth to deliver it in such a form 
as may be best believed, and not as may be best examined; 
and he that receiveth knowledge desireth rather present sat- 
isfaction than expectant inquiry. 


Francis Bacon 


Many students approach a science course with the idea that they 
can succeed by memorizing the formulas, so that when a problem 


Section 0.3 How to learn physics 


21 


22 


Chapter 0 


is assigned on the homework or an exam, they will be able to plug 
numbers in to the formula and get a numerical result on their cal- 
culator. Wrong! That’s not what learning science is about! There 
is a big difference between memorizing formulas and understanding 
concepts. To start with, different formulas may apply in different 
situations. One equation might represent a definition, which is al- 
ways true. Another might be a very specific equation for the speed 
of an object sliding down an inclined plane, which would not be true 
if the object was a rock drifting down to the bottom of the ocean. 
If you don’t work to understand physics on a conceptual level, you 
won’t know which formulas can be used when. 


Most students taking college science courses for the first time 
also have very little experience with interpreting the meaning of an 
equation. Consider the equation w = A/h relating the width of a 
rectangle to its height and area. A student who has not developed 
skill at interpretation might view this as yet another equation to 
memorize and plug in to when needed. A slightly more savvy stu- 
dent might realize that it is simply the familiar formula A = wh 
in a different form. When asked whether a rectangle would have 
a greater or smaller width than another with the same area but 
a smaller height, the unsophisticated student might be at a loss, 
not having any numbers to plug in on a calculator. The more ex- 
perienced student would know how to reason about an equation 
involving division — if h is smaller, and A stays the same, then w 
must be bigger. Often, students fail to recognize a sequence of equa- 
tions as a derivation leading to a final result, so they think all the 
intermediate steps are equally important formulas that they should 
memorize. 


When learning any subject at all, it is important to become as 
actively involved as possible, rather than trying to read through 
all the information quickly without thinking about it. It is a good 
idea to read and think about the questions posed at the end of each 
section of these notes as you encounter them, so that you know you 
have understood what you were reading. 


Many students’ difficulties in physics boil down mainly to diffi- 
culties with math. Suppose you feel confident that you have enough 
mathematical preparation to succeed in this course, but you are 
having trouble with a few specific things. In some areas, the brief 
review given in this chapter may be sufficient, but in other areas 
it probably will not. Once you identify the areas of math in which 
you are having problems, get help in those areas. Don’t limp along 
through the whole course with a vague feeling of dread about some- 
thing like scientific notation. The problem will not go away if you 
ignore it. The same applies to essential mathematical skills that you 
are learning in this course for the first time, such as vector addition. 


Sometimes students tell me they keep trying to understand a 


Introduction and Review 


certain topic in the book, and it just doesn’t make sense. The worst 
thing you can possibly do in that situation is to keep on staring 
at the same page. Every textbook explains certain things badly — 
even mine! — so the best thing to do in this situation is to look 
at a different book. Instead of college textbooks aimed at the same 
mathematical level as the course you’re taking, you may in some 
cases find that high school books or books at a lower math level 
give clearer explanations. 


Finally, when reviewing for an exam, don’t simply read back 
over the text and your lecture notes. Instead, try to use an active 
method of reviewing, for instance by discussing some of the discus- 
sion questions with another student, or doing homework problems 
you hadn’t done the first time. 


0.4 Self-evaluation 


The introductory part of a book like this is hard to write, because 
every student arrives at this starting point with a different prepara- 
tion. One student may have grown up outside the U.S. and so may 
be completely comfortable with the metric system, but may have 
had an algebra course in which the instructor passed too quickly 
over scientific notation. Another student may have already taken 
calculus, but may have never learned the metric system. The fol- 
lowing self-evaluation is a checklist to help you figure out what you 
need to study to be prepared for the rest of the course. 





If you disagree with this state- | you should study this section: 
ment... 





I am familiar with the basic metric | section 0.5 Basic of the Metric Sys- 
units of meters, kilograms, and sec- | tem 

onds, and the most common metric 
prefixes: milli- (m), kilo- (k), and 





centi- (c). 
I know about the newton, a unit of | section 0.6 The newton, the Metric 
force Unit of Force 





I am familiar with these less com- | section 0.7 Less Common Metric 
mon metric prefixes: mega- (M), | Prefixes 
micro- (4), and nano- (n). 





I am comfortable with scientific no- | section 0.8 Scientific Notation 
tation. 





I can confidently do metric conver- | section 0.9 Conversions 
sions. 





I understand the purpose and use of | section 0.10 Significant Figures 
significant figures. 














It wouldn’t hurt you to skim the sections you think you already 
know about, and to do the self-checks in those sections. 


Section 0.4 Self-evaluation 


0.5 Basics of the metric system 


The metric system 


Every country in the world besides the U.S. uses a system of 
units known in English as the “metric system.?” This system is 
entirely decimal, thanks to the same eminently logical people who 
brought about the French Revolution. In deference to France, the 
system’s official name is the Systeme International, or SI, meaning 
International System. The system uses a single, consistent set of 
Greek and Latin prefixes that modify the basic units. Each prefix 
stands for a power of ten, and has an abbreviation that can be 
combined with the symbol for the unit. For instance, the meter is 
a unit of distance. The prefix kilo- stands for 10°, so a kilometer, 1 
km, is a thousand meters. 


The basic units of the SI are the meter for distance, the second 
for time, and the kilogram (not the gram) for mass. 


The following are the most common metric prefixes. You should 
memorize them. 


prefix meaning example 
kilo k 103 60 kg =a person’s mass 
centi- c 107? 28cm = height of a piece of paper 
milli- m 107? lms = time for one vibration of a guitar 


string playing the note D 


The prefix centi-, meaning 10~?, is only used in the centimeter; 
a hundredth of a gram would not be written as 1 cg but as 10 mg. 
The centi- prefix can be easily remembered because a cent is 10~? 


66) 


dollars. The official SI abbreviation for seconds is “s” (not “sec”) 


66 


and grams are “g” (not “gm”). 


The second 


When I stated briefly above that the second was a unit of time, it 
may not have occurred to you that this was not much of a definition. 
We can make a dictionary-style definition of a term like “time,” or 
give a general description like Isaac Newton’s: “Absolute, true, and 
mathematical time, of itself, and from its own nature, flows equably 
without relation to anything external...” Newton’s characterization 
sounds impressive, but physicists today would consider it useless as 
a definition of time. Today, the physical sciences are based on oper- 
ational definitions, which means definitions that spell out the actual 
steps (operations) required to measure something numerically. 


In an era when our toasters, pens, and coffee pots tell us the 
time, it is far from obvious to most people what is the fundamental 
operational definition of time. Until recently, the hour, minute, and 
second were defined operationally in terms of the time required for 





Liberia and Myanmar have not legally adopted metric units, but use them 
in everyday life. 


Chapter 0 Introduction and Review 


the earth to rotate about its axis. Unfortunately, the Earth’s ro- 
tation is slowing down slightly, and by 1967 this was becoming an 
issue in scientific experiments requiring precise time measurements. 
The second was therefore redefined as the time required for a cer- 
tain number of vibrations of the light waves emitted by a cesium 
atoms in a lamp constructed like a familiar neon sign but with the 
neon replaced by cesium. The new definition not only promises to 
stay constant indefinitely, but for scientists is a more convenient 
way of calibrating a clock than having to carry out astronomical 
measurements. 


self-check C 
What is a possible operational definition of how strong apersonis? p> 
Answer, p. 563 


The meter 


The French originally defined the meter as 10~7 times the dis- 
tance from the equator to the north pole, as measured through Paris 
(of course). Even if the definition was operational, the operation of 
traveling to the north pole and laying a surveying chain behind you 
was not one that most working scientists wanted to carry out. Fairly 
soon, a standard was created in the form of a metal bar with two 
scratches on it. This was replaced by an atomic standard in 1960, 
and finally in 1983 by the current definition, which is that the speed 
of light has a defined value in units of m/s. 


The kilogram 


The third base unit of the SI is the kilogram, a unit of mass. 
Mass is intended to be a measure of the amount of a substance, 
but that is not an operational definition. Bathroom scales work by 
measuring our planet’s gravitational attraction for the object being 
weighed, but using that type of scale to define mass operationally 
would be undesirable because gravity varies in strength from place 
to place on the earth. The kilogram was for a long time defined 
by a physical artifact (figure f), but in 2019 it was redefined by 
giving a defined value to Planck’s constant (p. 970), which plays a 
fundamental role in the description of the atomic world. 


Combinations of metric units 


Just about anything you want to measure can be measured with 
some combination of meters, kilograms, and seconds. Speed can be 
measured in m/s, volume in m°, and density in kg/m®. Part of what 
makes the SI great is this basic simplicity. No more funny units like 
a cord of wood, a bolt of cloth, or a jigger of whiskey. No more 
liquid and dry measure. Just a simple, consistent set of units. The 
SI measures put together from meters, kilograms, and seconds make 
up the mks system. For example, the mks unit of speed is m/s, not 
km/hr. 





e/The original definition of 


the meter. 





CE ae OR 


f/A duplicate of the Paris 
kilogram, maintained at the Dan- 
ish National Metrology Institute. 
As of 2019, the kilogram is no 
longer defined in terms of a 
physical standard. 


Section 0.5 Basics of the metric system 25 


26 


Chapter 0 





Checking units 


A useful technique for finding mistakes in one’s algebra is to 


analyze the units associated with the variables. 


Checking units example 1 
> Jae starts from the formula V = 3Ah for the volume of a cone, 
where A is the area of its base, and h is its height. He wants to 
find an equation that will tell him how tall a conical tent has to be 
in order to have a certain volume, given its radius. His algebra 
goes like this: 





| V=—Ah 
3 
2 A=nr- 
3 V= anr?h 
2 
Tr 
paay 


Is his algebra correct? If not, find the mistake. 


> Line 4 is supposed to be an equation for the height, so the units 
of the expression on the right-hand side had better equal meters. 
The pi and the 3 are unitless, so we can ignore them. In terms of 
units, line 4 becomes 


m2 1 
nee. 
me om 


This is false, so there must be a mistake in the algebra. The units 
of lines 1, 2, and 3 check out, so the mistake must be in the step 
from line 3 to line 4. In fact the result should have been 


3V 
h= mre. 


Now the units check: m = m?/m?. 


Discussion question 


A Isaac Newton wrote, “...the natural days are truly unequal, though 
they are commonly considered as equal, and used for a measure of 
time. . . It may be that there is no such thing as an equable motion, whereby 
time may be accurately measured. All motions may be accelerated or re- 
tarded...” Newton was right. Even the modern definition of the second 
in terms of light emitted by cesium atoms is subject to variation. For in- 
stance, magnetic fields could cause the cesium atoms to emit light with 
a slightly different rate of vibration. What makes us think, though, that a 
pendulum clock is more accurate than a sundial, or that a cesium atom 
is a more accurate timekeeper than a pendulum clock? That is, how can 
one test experimentally how the accuracies of different time standards 
compare? 


Introduction and Review 


0.6 The Newton, the metric unit of force 


A force is a push or a pull, or more generally anything that can 
change an object’s speed or direction of motion. A force is required 
to start a car moving, to slow down a baseball player sliding in to 
home base, or to make an airplane turn. (Forces may fail to change 
an object’s motion if they are canceled by other forces, e.g., the 
force of gravity pulling you down right now is being canceled by the 
force of the chair pushing up on you.) The metric unit of force is 
the Newton, defined as the force which, if applied for one second, 
will cause a 1-kilogram object starting from rest to reach a speed of 
1 m/s. Later chapters will discuss the force concept in more detail. 
In fact, this entire book is about the relationship between force and 
motion. 


In section 0.5, I gave a gravitational definition of mass, but by 
defining a numerical scale of force, we can also turn around and de- 
fine a scale of mass without reference to gravity. For instance, if a 
force of two Newtons is required to accelerate a certain object from 
rest to 1 m/s in 1 s, then that object must have a mass of 2 kg. 
From this point of view, mass characterizes an object’s resistance 
to a change in its motion, which we call inertia or inertial mass. 
Although there is no fundamental reason why an object’s resistance 
to a change in its motion must be related to how strongly gravity 
affects it, careful and precise experiments have shown that the in- 
ertial definition and the gravitational definition of mass are highly 
consistent for a variety of objects. It therefore doesn’t really matter 
for any practical purpose which definition one adopts. 


Discussion question 


A Spending a long time in weightlessness is unhealthy. One of the 
most important negative effects experienced by astronauts is a loss of 
muscle and bone mass. Since an ordinary scale won’t work for an astro- 
naut in orbit, what is a possible way of monitoring this change in mass? 
(Measuring the astronaut’s waist or biceps with a measuring tape is not 
good enough, because it doesn't tell anything about bone mass, or about 
the replacement of muscle with fat.) 


0.7 Less common metric prefixes 


The following are three metric prefixes which, while less common 
than the ones discussed previously, are well worth memorizing. 


prefix meaning example 
mega- M_ 10° 6.4Mm = radius of the earth 
micro- yp 107° 10 pm = size of a white blood cell 
nano- n_ 107% 0.154 nm = distance between carbon 


nuclei in an ethane molecule 


Note that the abbreviation for micro is the Greek letter mu, ju 
— a common mistake is to confuse it with m (milli) or M (mega). 


pee little 


10 -9 nano — nuns 





10 6 micro <—[ mix 


10-3 oni No 


10 3 kilo 


10 6 mega «| MUQS. 


g/This is a mnemonic. to 
help you remember the most im- 
portant metric prefixes. The word 
“little” is to remind you that the 
list starts with the prefixes used 
for small quantities and builds 
upward. The exponent changes 
by 3, except that of course that 
we do not need a special prefix 
for 10°, which equals one. 


Section 0.6 The Newton, the metric unit of force 27 


28 


Chapter 0 


There are other prefixes even less common, used for extremely 
large and small quantities. For instance, 1 femtometer = 107! m is 
a convenient unit of distance in nuclear physics, and 1 gigabyte = 
10° bytes is used for computers’ hard disks. The international com- 
mittee that makes decisions about the SI has recently even added 
some new prefixes that sound like jokes, e.g., 1 yoctogram = 10724 g 
is about half the mass of a proton. In the immediate future, how- 
ever, you’re unlikely to see prefixes like “yocto-” and “zepto-” used 
except perhaps in trivia contests at science-fiction conventions or 
other geekfests. 


self-check D 

Suppose you could slow down time so that according to your perception, 
a beam of light would move across a room at the speed of a slow walk. 
If you perceived a nanosecond as if it was a second, how would you 
perceive a microsecond? > Answer, p. 564 


d 


0.8 Scientific notation 


Most of the interesting phenomena in our universe are not on the 
human scale. It would take about 1,000,000,000,000,000,000,000 
bacteria to equal the mass of a human body. When the physicist 
Thomas Young discovered that light was a wave, it was back in the 
bad old days before scientific notation, and he was obliged to write 
that the time required for one vibration of the wave was 1/500 of 
a millionth of a millionth of a second. Scientific notation is a less 
awkward way to write very large and very small numbers such as 
these. Here’s a quick review. 


Scientific notation means writing a number in terms of a product 
of something from 1 to 10 and something else that is a power of ten. 
For instance, 

32 = 32% 10! 
320 = 3.2 x 10? 
3200 = 3.2 x 10° 


Each number is ten times bigger than the previous one. 


Since 10! is ten times smaller than 10? , it makes sense to use 
the notation 10° to stand for one, the number that is in turn ten 
times smaller than 10! . Continuing on, we can write 10! to stand 
for 0.1, the number ten times smaller than 10° . Negative exponents 
are used for small numbers: 


3.2 = 3.2 x 10° 
0.32 = 3.2 x 107! 
0.032 = 3.2 x 10~ 


Introduction and Review 


A common source of confusion is the notation used on the dis- 
plays of many calculators. Examples: 


3.2 x 10° (written notation) 
3.2E+6 (notation on some calculators) 
32° (notation on some other calculators) 


The last example is particularly unfortunate, because 3.2° really 

stands for the number 3.2 x 3.2 « 3.2 x 3.2 x 3.2 x 3.2 = 1074, a 

totally different number from 3.2 x 10° = 3200000. The calculator 

notation should never be used in writing. It’s just a way for the 

manufacturer to save money by making a simpler display. 
self-check E 


A student learns that 104 bacteria, standing in line to register for classes 
at Paramecium Community College, would form a queue of this size: 


ee 
The student concludes that 10? bacteria would form a line of this length: 


Why is the student incorrect? > Answer, p. 564 


0.9 Conversions 


Conversions are one of the three essential mathematical skills, sum- 
marized on pp.545-546, that you need for success in this course. 


I suggest you avoid memorizing lots of conversion factors be- 
tween SI units and U.S. units, but two that do come in handy are: 


1 inch = 2.54 cm 


An object with a weight on Earth of 2.2 pounds-force has a 
mass of 1 kg. 


The first one is the present definition of the inch, so it’s exact. The 
second one is not exact, but is good enough for most purposes. (U.S. 
units of force and mass are confusing, so it’s a good thing they’re 
not used in science. In U.S. units, the unit of force is the pound- 
force, and the best unit to use for mass is the slug, which is about 
14.6 kg.) 


More important than memorizing conversion factors is under- 
standing the right method for doing conversions. Even within the 
SI, you may need to convert, say, from grams to kilograms. Differ- 
ent people have different ways of thinking about conversions, but 
the method I’ll describe here is systematic and easy to understand. 
The idea is that if 1 kg and 1000 g represent the same mass, then 


Section 0.9 Conversions 


29 


30 


Chapter 0 


we can consider a fraction like 
10° g 
1 kg 





to be a way of expressing the number one. This may bother you. For 
instance, if you type 1000/1 into your calculator, you will get 1000, 
not one. Again, different people have different ways of thinking 
about it, but the justification is that it helps us to do conversions, 
and it works! Now if we want to convert 0.7 kg to units of grams, 
we can multiply kg by the number one: 





10° ¢ 
0.7 kg x 
. 1 kg 
If yow’re willing to treat symbols such as “kg” as if they were vari- 
ables as used in algebra (which they’re really not), you can then 
cancel the kg on top with the kg on the bottom, resulting in 
LO? 


0.7 ke x BY aul 





To convert grams to kilograms, you would simply flip the fraction 
upside down. 


One advantage of this method is that it can easily be applied to 
a series of conversions. For instance, to convert one year to units of 
seconds, 


,, 365 days | 24 hours | 60 min 60s 
1 year 1 day lheur  1lmin — 
= 3.15 « 10" 6. 





Should that exponent be positive, or negative? 


A common mistake is to write the conversion fraction incorrectly. 
For instance the fraction 


10° ke 
eS 





(incorrect) 


does not equal one, because 10? kg is the mass of a car, and 1 g is 
the mass of a raisin. One correct way of setting up the conversion 
factor would be 
1073 kg 
lg 
You can usually detect such a mistake if you take the time to check 
your answer and see if it is reasonable. 


(correct). 


If common sense doesn’t rule out either a positive or a negative 
exponent, here’s another way to make sure you get it right. There 
are big prefixes and small prefixes: 


Introduction and Review 


big prefixes: k M 
small prefixes: m wp n 


(It’s not hard to keep straight which are which, since “mega” and 
“micro” are evocative, and it’s easy to remember that a kilometer 
is bigger than a meter and a millimeter is smaller.) In the example 
above, we want the top of the fraction to be the same as the bottom. 
Since & is a big prefix, we need to compensate by putting a small 
number like 107? in front of it, not a big number like 10%. 


> Solved problem: a simple conversion page 36, problem 6 


> Solved problem: the geometric mean page 37, problem 8 
Discussion question 


A Each of the following conversions contains an error. In each case, 
explain what the error is. 


(a) 1000 kg x agg = 19 








b) 50m x $% =0.5cm 


(b) 
(c) “Nano” is 10-9, so there are 10-9 nm in a meter. 
(d) “Micro” is 10-®, so 1 kg is 10® ug. 


0.10 Significant figures 


The international governing body for football (“soccer” in the US) 
says the ball should have a circumference of 68 to 70 cm. Taking the 
middle of this range and dividing by a gives a diameter of approx- 
imately 21.96338214668155633610595934540698196 cm. The digits 
after the first few are completely meaningless. Since the circumfer- 
ence could have varied by about a centimeter in either direction, the 
diameter is fuzzy by something like a third of a centimeter. We say 
that the additional, random digits are not significant figures. If you 
write down a number with a lot of gratuitous insignificant figures, 
it shows a lack of scientific literacy and imples to other people a 
greater precision than you really have. 


As a rule of thumb, the result of a calculation has as many 
significant figures, or “sig figs,” as the least accurate piece of data 
that went in. In the example with the soccer ball, it didn’t do us any 
good to know z to dozens of digits, because the bottleneck in the 
precision of the result was the figure for the circumference, which 
was two sig figs. The result is 22 cm. The rule of thumb works best 
for multiplication and division. 


For calculations involving multiplication and division, a given 
fractional or “percent” error in one of the inputs causes the same 
fractional error in the output. The number of digits in a number 


Section 0.10 


Significant figures 


31 


32 


Chapter 0 


provides a rough measure of its possible fractional error. These are 
called significant figures or “sig figs.” Examples: 























3.14 3 sig figs 

3.1 2 sig figs 

0.03 1 sig fig, because the zeroes are just placeholders 

3.0 x 10! | 2 sig figs 

30 could be 1 or 2 sig figs, since we can’t tell if the 
0 is a placeholder or a real sig fig 








In such calculations, your result should not have more than the 
number of sig figs in the least accurate piece of data you started 
with. 


Sig figs in the area of a triangle example 2 
> A triangle has an area of 6.45 m? and a base with a width of 
4.0138 m. Find its height. 


> The area is related to the base and height by A = bh/2. 
_2A 


b 
= 3.21391200358762 m_ (calculator output) 


=3.21m 


h 


The given data were 3 sig figs and 5 sig figs. We're limited by the 
less accurate piece of data, so the final result is 3 sig figs. The 
additional digits on the calculator don’t mean anything, and if we 
communicated them to another person, we would create the false 
impression of having determined h with more precision than we 
really obtained. 


self-check F 
The following quote is taken from an editorial by Norimitsu Onishi in the 
New York Times, August 18, 2002. 


Consider Nigeria. Everyone agrees it is Africa’s most populous 
nation. But what is its population? The United Nations says 
114 million; the State Department, 120 million. The World Bank 
says 126.9 million, while the Central Intelligence Agency puts it 
at 126,635,626. 


What should bother you about this? > Answer, p. 564 


Dealing correctly with significant figures can save you time! Of- 
ten, students copy down numbers from their calculators with eight 
significant figures of precision, then type them back in for a later 
calculation. That’s a waste of time, unless your original data had 
that kind of incredible precision. 

self-check G 


How many significant figures are there in each of the following mea- 
surements? 


Introduction and Review 


(1) 9.937 m 
(2) 4.05 
(3) 0.0000000000000037 kg > Answer, p. 564 


The rules about significant figures are only rules of thumb, and 
are not a substitute for careful thinking. For instance, $20.00 + 
$0.05 is $20.05. It need not and should not be rounded off to $20. 
In general, the sig fig rules work best for multiplication and division, 
and we sometimes also apply them when doing a complicated calcu- 
lation that involves many types of operations. For simple addition 
and subtraction, it makes more sense to maintain a fixed number of 
digits after the decimal point. 


When in doubt, don’t use the sig fig rules at all. Instead, in- 
tentionally change one piece of your initial data by the maximum 
amount by which you think it could have been off, and recalculate 
the final result. The digits on the end that are completely reshuffled 
are the ones that are meaningless, and should be omitted. 


A nonlinear function example 3 
> How many sig figs are there in sin 88.7°? 


> We're using a sine function, which isn’t addition, subtraction, 
multiplication, or division. It would be reasonable to guess that 
since the input angle had 3 sig figs, so would the output. But if 
this was an important calculation and we really needed to know, 
we would do the following: 


sin 88.7° = 0.999742609322698 
sin 88.8° = 0.999780683474846 


Surprisingly, the result appears to have as many as 5 sig figs, not 
just 3: 
sin 88.7° = 0.99974, 


where the final 4 is uncertain but may have some significance. 
The unexpectedly high precision of the result is because the sine 
function is nearing its maximum at 90 degrees, where the graph 
flattens out and becomes insensitive to the input angle. 


0.11 A note about diagrams 


A quick note about diagrams. Often when you solve a problem, 
the best way to get started and organize your thoughts is by draw- 
ing a diagram. For an artist, it’s desirable to be able to draw a 
recognizable, realistic, perspective picture of a tomato, like the one 
at the top of figure h. But in science and engineering, we usually 
don’t draw solid figures in perspective, because that would make it 
difficult to label distances and angles. Usually we want views or 
cross-sections that project the object into its planes of symmetry, 
as in the line drawings in the figure. 





horizontal 
cross-section 


h/A_ diagram of 


Section 0.11 A note about diagrams 


vertical 


cross-section 


a 


tomato. 


33 


Summary 


Selected vocabulary 


matter 


defi- 


operational 
nition 


Systeme Interna- 
tional 


Anything that is affected by gravity. 
Anything that can travel from one place to an- 
other through empty space and can influence 
matter, but is not affected by gravity. 

A definition that states what operations 
should be carried out to measure the thing be- 
ing defined. 

A fancy name for the metric system. 


The use of metric units based on the meter, 


kilogram, and second. Example: meters per 
second is the mks unit of speed, not cm/s or 
km/hr. 

A numerical measure of how difficult it is to 
change an object’s motion. 

Digits that contribute to the accuracy of a 


significant figures 


measurement. 
Notation 
1 ee meter, the metric distance unit 
KO fe BBs ot kilogram, the metric unit of mass 
Sebati aoe 2 second, the metric unit of time 
Me ie ba ee the metric prefix mega-, 10° 
Kes os we et eas the metric prefix kilo-, 10° 
Ti? vies eis ae the metric prefix milli-, 107° 
fis sn ho eA: the metric prefix micro-, 10~° 
TS iS speek ee the metric prefix nano-, 1079 
Summary 


Physics is the use of the scientific method to study the behavior 
of light and matter. The scientific method requires a cycle of the- 
ory and experiment, theories with both predictive and explanatory 
value, and reproducible experiments. 


The metric system is a simple, consistent framework for measure- 
ment built out of the meter, the kilogram, and the second plus a set 
of prefixes denoting powers of ten. The most systematic method for 
doing conversions is shown in the following example: 

-3 

= = 0.378 

ms 





10 
370 ms x 


Mass is a measure of the amount of a substance. Mass can be 
defined gravitationally, by comparing an object to a standard mass 
on a double-pan balance, or in terms of inertia, by comparing the 
effect of a force on an object to the effect of the same force on a 
standard mass. The two definitions are found experimentally to 
be proportional to each other to a high degree of precision, so we 


Introduction and Review 


usually refer simply to “mass,” without bothering to specify which 
type. 


A force is that which can change the motion of an object. The 
metric unit of force is the Newton, defined as the force required to 
accelerate a standard 1-kg mass from rest to a speed of 1 m/s in 1 
S. 


Scientific notation means, for example, writing 3.2 x 10° rather 
than 320000. 


Writing numbers with the correct number of significant figures 
correctly communicates how accurate they are. As a rule of thumb, 
the final result of a calculation is no more accurate than, and should 
have no more significant figures than, the least accurate piece of 
data. 


Summary 


36 


Chapter 0 


Problems 
Key 


Vv A computerized answer check is available online. 
Jf A problem that requires calculus. 
x A difficult problem. 


1 Correct use of a calculator: (a) Calculate SS SEI on a cal- 
culator. [Self-check: The most common mistake results in 97555.40.] 
Vv 


(b) Which would be more like the price of a TV, and which would 
be more like the price of a house, $3.5 x 10° or $3.5°? 


2 Compute the following things. If they don’t make sense be- 
cause of units, say so. 

(a) 3cm + 5 cm 

(b) 1.11 m + 22 cm 

(c) 120 miles + 2.0 hours 

(d) 120 miles / 2.0 hours 


3 Your backyard has brick walls on both ends. You measure a 
distance of 23.4 m from the inside of one wall to the inside of the 
other. Each wall is 29.4 cm thick. How far is it from the outside 
of one wall to the outside of the other? Pay attention to significant 
figures. 


4 The speed of light is 3.0 x 10° m/s. Convert this to furlongs 
per fortnight. A furlong is 220 yards, and a fortnight is 14 days. An 
inch is 2.54 cm. Vv 


5 Express each of the following quantities in micrograms: 
(a) 10 mg, (b) 10* g, (c) 10 kg, (d) 100 x 10° g, (e) 1000 ng. v 


6 Convert 134 mg to units of kg, writing your answer in scientific 
notation. > Solution, p. 547 


7 In the last century, the average age of the onset of puberty for 
girls has decreased by several years. Urban folklore has it that this 
is because of hormones fed to beef cattle, but it is more likely to be 
because modern girls have more body fat on the average and pos- 
sibly because of estrogen-mimicking chemicals in the environment 
from the breakdown of pesticides. A hamburger from a hormone- 
implanted steer has about 0.2 ng of estrogen (about double the 
amount of natural beef). A serving of peas contains about 300 
ng of estrogen. An adult woman produces about 0.5 mg of estrogen 
per day (note the different unit!). (a) How many hamburgers would 
a girl have to eat in one day to consume as much estrogen as an 
adult woman’s daily production? (b) How many servings of peas? 


Introduction and Review 


8 The usual definition of the mean (average) of two numbers a 
and b is (a+6)/2. This is called the arithmetic mean. The geometric 
mean, however, is defined as (ab)!/? (i.e., the square root of ab). For 
the sake of definiteness, let’s say both numbers have units of mass. 
(a) Compute the arithmetic mean of two numbers that have units 
of grams. Then convert the numbers to units of kilograms and 
recompute their mean. Is the answer consistent? (b) Do the same 
for the geometric mean. (c) If a and b both have units of grams, 
what should we call the units of ab? Does your answer make sense 
when you take the square root? (d) Suppose someone proposes to 
you a third kind of mean, called the superduper mean, defined as 
(ab)'/3. Is this reasonable? > Solution, p. 547 


9 In an article on the SARS epidemic, the May 7, 2003 New 
York Times discusses conflicting estimates of the disease’s incuba- 
tion period (the average time that elapses from infection to the first 
symptoms). “The study estimated it to be 6.4 days. But other sta- 
tistical calculations ... showed that the incubation period could be 
as long as 14.22 days.” What’s wrong here? 


10 The photo shows the corner of a bag of pretzels. What’s 
wrong here? 


11 The distance to the horizon is given by the expression V2rh, 

where r is the radius of the Earth, and h is the observer’s height 

above the Earth’s surface. (This can be proved using the Pythagorean 
theorem.) Show that the units of this expression make sense. Don’t 

try to prove the result, just check its units. (See example 1 on p. 

26 for an example of how to do this.) 


12 (a) Based on the definitions of the sine, cosine, and tangent, 
what units must they have? (b) A cute formula from trigonometry 
lets you find any angle of a triangle if you know the lengths of 
its sides. Using the notation shown in the figure, and letting s = 
(a+b+c)/2 be half the perimeter, we have 


—b\(s— 
tan A/2 = (s — b)(s =o) 
s(s — a) 
Show that the units of this equation make sense. In other words, 
check that the units of the right-hand side are the same as your 
answer to part a of the question. > Solution, p. 547 


13 A 2002 paper by Steegmann et al. uses data from modern 
human groups like the Inuit to argue that Neanderthals in Ice Age 
Europe had to eat up “to 4,480 kcal per day to support strenuous 
winter foraging and cold resistance costs.” What’s wrong here? 


NET WT. 3 1/2 oz. (99.2 g) 


Problem 10. 


Problem 12. 


Problems 





37 


38 


Exercise 0: Models and idealization 
Equipment: 

coffee filters 

ramps (one per group) 

balls of various sizes 

sticky tape 

vacuum pump and “guinea and feather” apparatus (one) 


The motion of falling objects has been recognized since ancient times as an important piece of 
physics, but the motion is inconveniently fast, so in our everyday experience it can be hard to 
tell exactly what objects are doing when they fall. In this exercise you will use several techniques 
to get around this problem and study the motion. Your goal is to construct a scientific model of 
falling. A model means an explanation that makes testable predictions. Often models contain 
simplifications or idealizations that make them easier to work with, even though they are not 
strictly realistic. 


1. One method of making falling easier to observe is to use objects like feathers that we know 
from everyday experience will not fall as fast. You will use coffee filters, in stacks of various 
sizes, to test the following two hypotheses and see which one is true, or whether neither is true: 


Hypothesis 1A: When an object is dropped, it rapidly speeds up to a certain natural falling 
speed, and then continues to fall at that speed. The falling speed is proportional to the object’s 
weight. (A proportionality is not just a statement that if one thing gets bigger, the other does 
too. It says that if one becomes three times bigger, the other also gets three times bigger, etc.) 


Hypothesis 1B: Different objects fall the same way, regardless of weight. 
Test these hypotheses and discuss your results with your instructor. 


2. A second way to slow down the action is to let a ball roll down a ramp. The steeper the 
ramp, the closer to free fall. Based on your experience in part 1, write a hypothesis about what 
will happen when you race a heavier ball against a lighter ball down the same ramp, starting 
them both from rest. 


Hypothesis: 
Show your hypothesis to your instructor, and then test it. 


You have probably found that falling was more complicated than you thought! Is there more 
than one factor that affects the motion of a falling object? Can you imagine certain idealized 
situations that are simpler? Try to agree verbally with your group on an informal model of 
falling that can make predictions about the experiments described in parts 3 and 4. 


3. You have three balls: a standard “comparison ball” of medium weight, a light ball, and a 
heavy ball. Suppose you stand on a chair and (a) drop the light ball side by side with the 
comparison ball, then (b) drop the heavy ball side by side with the comparison ball, then (c) 
join the light and heavy balls together with sticky tape and drop them side by side with the 
comparison ball. 


Use your model to make a prediction:__________________________ 


Test your prediction. 


Chapter 0 Introduction and Review 


4. Your instructor will pump nearly all the air out of a chamber containing a feather and a 
heavier object, then let them fall side by side in the chamber. 


Use your model to make a prediction: 


Exercise 0: Models and idealization 


39 


40 


Chapter 0 


Introduction and Review 





Chapter 1 
Scaling and Estimation 


1.1 Introduction 


Why can’t an insect be the size of a dog? Some skinny stretched- 
out cells in your spinal cord are a meter tall — why does nature 
display no single cells that are not just a meter tall, but a meter 
wide, and a meter thick as well? Believe it or not, these are questions 
that can be answered fairly easily without knowing much more about 
physics than you already do. The only mathematical technique you 
really need is the humble conversion, applied to area and volume. 


Area and volume 


Area can be defined by saying that we can copy the shape of 
interest onto graph paper with 1 cm x 1 cm squares and count the 
number of squares inside. Fractions of squares can be estimated by 
eye. We then say the area equals the number of squares, in units of 
square cm. Although this might seem less “pure” than computing 
areas using formulae like A = rr? for a circle or A = wh/2 for a 
triangle, those formulae are not useful as definitions of area because 
they cannot be applied to irregularly shaped areas. 


2 


Units of square cm are more commonly written as cm“ in science. 


Of course, the unit of measurement symbolized by “cm” is not an 


Life would be very different if you 
were the size of an insect. 





a/Amoebas this size are 
seldom encountered. 


41 


b / Visualizing 


conversions 


of 


area and volume using traditional 


U.S. units. 


42 


Chapter 1 


algebra symbol standing for a number that can be literally multiplied 
by itself. But it is advantageous to write the units of area that way 
and treat the units as if they were algebra symbols. For instance, 
if you have a rectangle with an area of 6m? and a width of 2 m, 
then calculating its length as (6 m?)/(2 m) = 3 m gives a result 
that makes sense both numerically and in terms of units. This 
algebra-style treatment of the units also ensures that our methods 
of converting units work out correctly. For instance, if we accept 


the fraction 
100 cm 


1m 





as a valid way of writing the number one, then one times one equals 
one, so we should also say that one can be represented by 


100 cm ‘i 100 cm 
1m 1m 





’ 


which is the same as 
10000 cm? 


1 m2 
That means the conversion factor from square meters to square cen- 
timeters is a factor of 10+, i.e., a square meter has 104 square cen- 
timeters in it. 


All of the above can be easily applied to volume as well, using 
one-cubic-centimeter blocks instead of squares on graph paper. 


To many people, it seems hard to believe that a square meter 
equals 10000 square centimeters, or that a cubic meter equals a 
million cubic centimeters — they think it would make more sense if 
there were 100 cm? in 1 m?, and 100 cm? in 1 m?, but that would be 
incorrect. The examples shown in figure b aim to make the correct 
answer more believable, using the traditional U.S. units of feet and 
yards. (One foot is 12 inches, and one yard is three feet.) 


=a a Ah YO 
13 


1ft2 lyd2=9ft 2 
1yd3=27ft 3 


self-check A 

Based on figure b, convince yourself that there are 9 ft? ina square yard, 
and 27 ft? in a cubic yard, then demonstrate the same thing symbolically 
(i.e., with the method using fractions that equal one). > Answer, p. 
564 


Scaling and Estimation 


> Solved problem: converting mm? to cm? page 59, problem 10 


> Solved problem: scaling a liter page 60, problem 19 


Discussion question 


A How many square centimeters are there in a square inch? (1 inch = 
2.54 cm) First find an approximate answer by making a drawing, then de- 
rive the conversion factor more accurately using the symbolic method. 


c / Galileo Galilei (1564-1642) was a Renaissance Italian who brought the 
scientific method to bear on physics, creating the modern version of the 
science. Coming from a noble but very poor family, Galileo had to drop 
out of medical school at the University of Pisa when he ran out of money. 
Eventually becoming a lecturer in mathematics at the same school, he 
began a career as a notorious troublemaker by writing a burlesque ridi- 
culing the university’s regulations — he was forced to resign, but found a 
new teaching position at Padua. He invented the pendulum clock, inves- 
tigated the motion of falling bodies, and discovered the moons of Jupiter. 
The thrust of his life’s work was to discredit Aristotle’s physics by con- 
fronting it with contradictory experiments, a program that paved the way 
for Newton’s discovery of the relationship between force and motion. In 
chapter 3 we’ll come to the story of Galileo’s ultimate fate at the hands of 
the Church. 





1.2 Scaling of area and volume 


Great fleas have lesser fleas 
Upon their backs to bite ’em. 
And lesser fleas have lesser still, 
And so ad infinitum. 


Jonathan Swift 


Now how do these conversions of area and volume relate to the 
questions I posed about sizes of living things? Well, imagine that 
you are shrunk like Alice in Wonderland to the size of an insect. 
One way of thinking about the change of scale is that what used 
to look like a centimeter now looks like perhaps a meter to you, 
because you’re so much smaller. If area and volume scaled according 
to most people’s intuitive, incorrect expectations, with 1 m? being 
the same as 100 cm?, then there would be no particular reason 
why nature should behave any differently on your new, reduced 
scale. But nature does behave differently now that you’re small. 
For instance, you will find that you can walk on water, and jump 
to many times your own height. The physicist Galileo Galilei had 
the basic insight that the scaling of area and volume determines 
how natural phenomena behave differently on different scales. He 
first reasoned about mechanical structures, but later extended his 
insights to living things, taking the then-radical point of view that at 
the fundamental level, a living organism should follow the same laws 


Section 1.2 Scaling of area and volume 43 


et 


d/The small boat holds up 
just fine. 


e/A larger boat built with 
the same proportions as_ the 
small one will collapse under its 
own weight. 


f/A boat this large needs to 
have timbers that are thicker 
compared to its size. 


44 Chapter 1 


of nature as a machine. We will follow his lead by first discussing 
machines and then living things. 


Galileo on the behavior of nature on large and small scales 


One of the world’s most famous pieces of scientific writing is 
Galileo’s Dialogues Concerning the Two New Sciences. Galileo was 
an entertaining writer who wanted to explain things clearly to laypeo- 
ple, and he livened up his work by casting it in the form of a dialogue 
among three people. Salviati is really Galileo’s alter ego. Simplicio 
is the stupid character, and one of the reasons Galileo got in trouble 
with the Church was that there were rumors that Simplicio repre- 
sented the Pope. Sagredo is the earnest and intelligent student, with 
whom the reader is supposed to identify. (The following excerpts 
are from the 1914 translation by Crew and de Salvio.) 


SAGREDO: Yes, that is what | mean; and | refer especially to 
his last assertion which | have always regarded as false... ; 
namely, that in speaking of these and other similar machines 
one cannot argue from the small to the large, because many 
devices which succeed on a small scale do not work on a 
large scale. Now, since mechanics has its foundations in ge- 
ometry, where mere size [ is unimportant], | do not see that 
the properties of circles, triangles, cylinders, cones and other 
solid figures will change with their size. If, therefore, a large 
machine be constructed in such a way that its parts bear to 
one another the same ratio as in a smaller one, and if the 
smaller is sufficiently strong for the purpose for which it is 
designed, | do not see why the larger should not be able to 
withstand any severe and destructive tests to which it may be 
subjected. 


Salviati contradicts Sagredo: 


SALVIATI: ...Please observe, gentlemen, how facts which 
at first seem improbable will, even on scant explanation, drop 
the cloak which has hidden them and stand forth in naked and 
simple beauty. Who does not know that a horse falling from a 
height of three or four cubits will break his bones, while a dog 
falling from the same height or a cat from a height of eight 
or ten cubits will suffer no injury? Equally harmless would be 
the fall of a grasshopper from a tower or the fall of an ant from 
the distance of the moon. 


The point Galileo is making here is that small things are sturdier 
in proportion to their size. There are a lot of objections that could be 
raised, however. After all, what does it really mean for something to 
be “strong”, to be “strong in proportion to its size,” or to be strong 
“out of proportion to its size?” Galileo hasn’t given operational 
definitions of things like “strength,” i.e., definitions that spell out 
how to measure them numerically. 


Scaling and Estimation 


Also, a cat is shaped differently from a horse — an enlarged 
photograph of a cat would not be mistaken for a horse, even if the 
photo-doctoring experts at the National Inquirer made it look like a 
person was riding on its back. A grasshopper is not even a mammal, 
and it has an exoskeleton instead of an internal skeleton. The whole 
argument would be a lot more convincing if we could do some iso- 
lation of variables, a scientific term that means to change only one 
thing at a time, isolating it from the other variables that might have 
an effect. If size is the variable whose effect we’re interested in see- 
ing, then we don’t really want to compare things that are different 
in size but also different in other ways. 


SALVIATI: — ...we asked the reason why [shipbuilders] em- 
ployed stocks, scaffolding, and bracing of larger dimensions 
for launching a big vessel than they do for a small one; and 
[an old man] answered that they did this in order to avoid the 
danger of the ship parting under its own heavy weight, a dan- 
ger to which small boats are not subject? 


After this entertaining but not scientifically rigorous beginning, 
Galileo starts to do something worthwhile by modern standards. 
He simplifies everything by considering the strength of a wooden 
plank. The variables involved can then be narrowed down to the 
type of wood, the width, the thickness, and the length. He also 
gives an operational definition of what it means for the plank to 
have a certain strength “in proportion to its size,” by introducing 
the concept of a plank that is the longest one that would not snap 
under its own weight if supported at one end. If you increased 
its length by the slightest amount, without increasing its width or 
thickness, it would break. He says that if one plank is the same 
shape as another but a different size, appearing like a reduced or 
enlarged photograph of the other, then the planks would be strong 
“in proportion to their sizes” if both were just barely able to support 
their own weight. 










Dae (aS 
eircom ost 








g/Galileo discusses planks 
made of wood, but the concept 
may be easier to imagine with 
clay. All three clay rods in the 
figure were originally the same 
shape. The medium-sized one 
was twice the height, twice the 
length, and twice the width of 
the small one, and similarly the 
large one was twice as big as 
the medium one in all its linear 
dimensions. The big one has 
four times the linear dimensions 
of the small one, 16 times the 
cross-sectional area when cut 
perpendicular to the page, and 
64 times the volume. That means 
that the big one has 64 times the 
weight to support, but only 16 
times the strength compared to 
the smallest one. 


h/1. This plank is as long as it 
can be without collapsing under 
its own weight. If it was a hun- 
dredth of an inch longer, it would 
collapse. 2. This plank is made 
out of the same kind of wood. It is 
twice as thick, twice as long, and 
twice as wide. It will collapse un- 
der its own weight. 


Section 1.2 Scaling of area and volume 45 


Also, Galileo is doing something that would be frowned on in 
modern science: he is mixing experiments whose results he has ac- 
tually observed (building boats of different sizes), with experiments 
that he could not possibly have done (dropping an ant from the 
height of the moon). He now relates how he has done actual ex- 
periments with such planks, and found that, according to this op- 
erational definition, they are not strong in proportion to their sizes. 
The larger one breaks. He makes sure to tell the reader how impor- 
tant the result is, via Sagredo’s astonished response: 


SAGREDO: My brain already reels. My mind, like a cloud 
momentarily illuminated by a lightning flash, is for an instant 
filled with an unusual light, which now beckons to me and 
which now suddenly mingles and obscures strange, crude 
ideas. From what you have said it appears to me impossible 
to build two similar structures of the same material, but of 
different sizes and have them proportionately strong. 


In other words, this specific experiment, using things like wooden 
planks that have no intrinsic scientific interest, has very wide impli- 
cations because it points out a general principle, that nature acts 
differently on different scales. 


To finish the discussion, Galileo gives an explanation. He says 
that the strength of a plank (defined as, say, the weight of the heav- 
iest boulder you could put on the end without breaking it) is pro- 
portional to its cross-sectional area, that is, the surface area of the 
fresh wood that would be exposed if you sawed through it in the 
middle. Its weight, however, is proportional to its volume.! 


How do the volume and cross-sectional area of the longer plank 
compare with those of the shorter plank? We have already seen, 
while discussing conversions of the units of area and volume, that 
these quantities don’t act the way most people naively expect. You 
might think that the volume and area of the longer plank would both 
be doubled compared to the shorter plank, so they would increase 
in proportion to each other, and the longer plank would be equally 
able to support its weight. You would be wrong, but Galileo knows 
that this is a common misconception, so he has Salviati address the 
point specifically: 


SALVIATI: — ... Take, for example, a cube two inches on a 
side so that each face has an area of four square inches 
and the total area, i.e., the sum of the six faces, amounts 
to twenty-four square inches; now imagine this cube to be 
sawed through three times [with cuts in three perpendicular 
planes] so as to divide it into eight smaller cubes, each one 
inch on the side, each face one inch square, and the total 





‘Galileo makes a slightly more complicated argument, taking into account 
the effect of leverage (torque). The result I’m referring to comes out the same 
regardless of this effect. 


Scaling and Estimation 


surface of each cube six square inches instead of twenty- 
four in the case of the larger cube. It is evident therefore, 
that the surface of the little cube is only one-fourth that of 
the larger, namely, the ratio of six to twenty-four; but the vol- 
ume of the solid cube itself is only one-eighth; the volume, 
and hence also the weight, diminishes therefore much more 
rapidly than the surface... You see, therefore, Simplicio, that 
| was not mistaken when ...1 said that the surface of a small 
solid is comparatively greater than that of a large one. 


The same reasoning applies to the planks. Even though they 
are not cubes, the large one could be sawed into eight small ones, 
each with half the length, half the thickness, and half the width. 
The small plank, therefore, has more surface area in proportion to 
its weight, and is therefore able to support its own weight while the 
large one breaks. 


Scaling of area and volume for irregularly shaped objects 


You probably are not going to believe Galileo’s claim that this 
has deep implications for all of nature unless you can be convinced 
that the same is true for any shape. Every drawing you’ve seen so 
far has been of squares, rectangles, and rectangular solids. Clearly 
the reasoning about sawing things up into smaller pieces would not 
prove anything about, say, an egg, which cannot be cut up into eight 
smaller egg-shaped objects with half the length. 


Is it always true that something half the size has one quarter 
the surface area and one eighth the volume, even if it has an irreg- 
ular shape? Take the example of a child’s violin. Violins are made 
for small children in smaller size to accomodate their small bodies. 
Figure i shows a full-size violin, along with two violins made with 
half and 3/4 of the normal length.? Let’s study the surface area of 
the front panels of the three violins. 


Consider the square in the interior of the panel of the full-size 
violin. In the 3/4-size violin, its height and width are both smaller 
by a factor of 3/4, so the area of the corresponding, smaller square 
becomes 3/4 x 3/4 = 9/16 of the original area, not 3/4 of the original 
area. Similarly, the corresponding square on the smallest violin has 
half the height and half the width of the original one, so its area is 
1/4 the original area, not half. 


The same reasoning works for parts of the panel near the edge, 
such as the part that only partially fills in the other square. The 
entire square scales down the same as a square in the interior, and 
in each violin the same fraction (about 70%) of the square is full, so 
the contribution of this part to the total area scales down just the 
same. 





?The customary terms “half-size” and “3/4-size” actually don’t describe the 
sizes in any accurate way. They’re really just standard, arbitrary marketing 
labels. 





i/The area of a_ shape is 
proportional to the square of its 
linear dimensions, even if the 
shape is irregular. 


Section 1.2 Scaling of area and volume 47 





j/The muffin comes out of 
the oven too hot to eat. Breaking 
it up into four pieces increases 
its surface area while keeping 
the total volume the same. It 


cools faster because of the 
greater surface-to-volume ratio. 
In general, smaller things have 
greater surface-to-volume ratios, 
but in this example there is no 
easy way to compute the effect 
exactly, because the small pieces 
aren't the same shape as the 
original muffin. 


48 Chapter 1 


Since any small square region or any small region covering part 
of a square scales down like a square object, the entire surface area 
of an irregularly shaped object changes in the same manner as the 
surface area of a square: scaling it down by 3/4 reduces the area by 
a factor of 9/16, and so on. 


In general, we can see that any time there are two objects with 
the same shape, but different linear dimensions (i.e., one looks like a 
reduced photo of the other), the ratio of their areas equals the ratio 
of the squares of their linear dimensions: 


Ay (li\? 

a) 
Note that it doesn’t matter where we choose to measure the linear 
size, L, of an object. In the case of the violins, for instance, it could 
have been measured vertically, horizontally, diagonally, or even from 
the bottom of the left fhole to the middle of the right f-hole. We 
just have to measure it in a consistent way on each violin. Since all 


the parts are assumed to shrink or expand in the same manner, the 
ratio L,/L» is independent of the choice of measurement. 


It is also important to realize that it is completely unnecessary 
to have a formula for the area of a violin. It is only possible to 
derive simple formulas for the areas of certain shapes like circles, 
rectangles, triangles and so on, but that is no impediment to the 
type of reasoning we are using. 


Sometimes it is inconvenient to write all the equations in terms 
of ratios, especially when more than two objects are being compared. 
A more compact way of rewriting the previous equation is 


Ax L?. 


The symbol “x” means “is proportional to.” Scientists and engi- 
neers often speak about such relationships verbally using the phrases 


“scales like” or “goes like,” for instance “area goes like length squared. 


All of the above reasoning works just as well in the case of vol- 
ume. Volume goes like length cubed: 


Va L. 


self-check B 

When a car or truck travels over a road, there is wear and tear on the 
road surface, which incurs a cost. Studies show that the cost C per kilo- 
meter of travel is related to the weight per axle w by C x w’. Translate 
this into a statement about ratios. > Answer, p. 564 


If different objects are made of the same material with the same 
density, p = m/V, then their masses, m = pV, are proportional to 
L°. (The symbol for density is p, the lower-case Greek letter “rho.” ) 


Scaling and Estimation 


” 


An important point is that all of the above reasoning about 
scaling only applies to objects that are the same shape. For instance, 
a piece of paper is larger than a pencil, but has a much greater 
surface-to-volume ratio. 


Scaling of the area of a triangle example 1 
> In figure k, the larger triangle has sides twice as long. How 
many times greater is its area? 


Correct solution #1: Area scales in proportion to the square of the 
linear dimensions, so the larger triangle has four times more area 
(22 = 4). 


Correct solution #2: You could cut the larger triangle into four of 
the smaller size, as shown in fig. (b), so its area is four times 
greater. (This solution is correct, but it would not work for a shape 
like a circle, which can’t be cut up into smaller circles.) 


Correct solution #3: The area of a triangle is given by 


A = bh/2, where b is the base and h is the height. The areas of 
the triangles are 


Ay = bihy /2 
Ap = bohp /2 
= (2b;)(2h1)/2 
= 2b; hy 
A2/Ay = (2b; hy) /(b; hy /2) 
=4 


(Although this solution is correct, it is a lot more work than solution 
#1, and it can only be used in this case because a triangle is a 
simple geometric shape, and we happen to know a formula for its 
area.) 


Correct solution #4: The area of a triangle is A = bh/2. The 
comparison of the areas will come out the same as long as the 
ratios of the linear sizes of the triangles is as specified, so let’s 
just say 6; = 1.00 m and bo = 2.00 m. The heights are then also 
hy = 1.00 m and ho = 2.00 m, giving areas A; = 0.50 m? and 
Ao = 2.00 m2, so A2/Aj = 4.00. 


(The solution is correct, but it wouldn’t work with a shape for 
whose area we don’t have a formula. Also, the numerical cal- 
culation might make the answer of 4.00 appear inexact, whereas 
solution #1 makes it clear that it is exactly 4.) 


Incorrect solution: The area of a triangle is A = bh/2, and if you 
plug in b = 2.00 m and h = 2.00 m, you get A = 2.00 m2, so 
the bigger triangle has 2.00 times more area. (This solution is 
incorrect because no comparison has been made with the smaller 
triangle.) 


k/Example 1. The big trian- 
gle has four times more area than 
the little one. 


1/A tricky way of solving ex- 
ample 1, explained in solution #2. 


Section 1.2 Scaling of area and volume 49 


© 


m/Example 2. The big sphere 
has 125 times more volume than 


the little one. 


n/Example_ 3. 





The 48-point 


“S” has 1.78 times more area 
than the 36-point “S.” 


50 


Scaling of the volume of a sphere example 2 
> In figure m, the larger sphere has a radius that is five times 
greater. How many times greater is its volume? 


Correct solution #1: Volume scales like the third power of the 
linear size, so the larger sphere has a volume that is 125 times 
greater (5° = 125). 


Correct solution #2: The volume of a sphere is V = (4/3)zr?, so 


V; a an} 
4 
= an(5n)° 
500 _ 3 
ae ai 
4 
Vo/V4 = (AP) / (3x?) = 125 
Incorrect solution: The volume of a sphere is V = (4/3)zr°, so 
4 
4 
Vo = 372 
4 
= an} 
20 4 
Vo/V1 = (Fx) / (3-4) =5 


(The solution is incorrect because (5r;)° is not the same as 57.) 


Scaling of a more complex shape example 3 
> The first letter “S” in figure n is in a 36-point font, the second in 
48-point. How many times more ink is required to make the larger 
“S’? (Points are a unit of length used in typography.) 


Correct solution: The amount of ink depends on the area to be 
covered with ink, and area is proportional to the square of the 
linear dimensions, so the amount of ink required for the second 
“S” is greater by a factor of (48 /36)° = 1.78. 


Incorrect solution: The length of the curve of the second “S” is 
longer by a factor of 48/36 = 1.33, so 1.33 times more ink is 
required. 


(The solution is wrong because it assumes incorrectly that the 
width of the curve is the same in both cases. Actually both the 


Chapter 1 Scaling and Estimation 


width and the length of the curve are greater by a factor of 48/36, 
so the area is greater by a factor of (48/36)? = 1.78.) 


Reasoning about ratios and proportionalities is one of the three 
essential mathematical skills, summarized on pp.545-546, that you 
need for success in this course. 


> Solved problem: a telescope gathers light page 59, problem 11 


> Solved problem: distance from an earthquake page 59, problem 12 


Discussion questions 


A __ A toy fire engine is 1/30 the size of the real one, but is constructed 
from the same metal with the same proportions. How many times smaller 
is its weight? How many times less red paint would be needed to paint 
it? 

B Galileo spends a lot of time in his dialog discussing what really 
happens when things break. He discusses everything in terms of Aristo- 
tle’s now-discredited explanation that things are hard to break, because 
if something breaks, there has to be a gap between the two halves with 
nothing in between, at least initially. Nature, according to Aristotle, “ab- 
hors a vacuum,” i.e., nature doesn't “like” empty space to exist. Of course, 
air will rush into the gap immediately, but at the very moment of breaking, 
Aristotle imagined a vacuum in the gap. Is Aristotle’s explanation of why 
it is hard to break things an experimentally testable statement? If so, how 
could it be tested experimentally? 


1.3 x Scaling applied to biology 
Organisms of different sizes with the same shape 


The left-hand panel in figure o shows the approximate valid- 
ity of the proportionality m o L? for cockroaches (redrawn from 
McMahon and Bonner). The scatter of the points around the curve 
indicates that some cockroaches are proportioned slightly differently 
from others, but in general the data seem well described by m « L?. 
That means that the largest cockroaches the experimenter could 
raise (is there a 4-H prize?) had roughly the same shape as the 
smallest ones. 


Another relationship that should exist for animals of different 
sizes shaped in the same way is that between surface area and 
body mass. If all the animals have the same average density, then 
body mass should be proportional to the cube of the animal’s lin- 
ear size, m x L?, while surface area should vary proportionately to 
L?. Therefore, the animals’ surface areas should be proportional to 
m2/3_ As shown in the right-hand panel of figure o, this relationship 
appears to hold quite well for the dwarf siren, a type of salamander. 
Notice how the curve bends over, meaning that the surface area does 
not increase as quickly as body mass, e.g., a salamander with eight 


Section 1.3 x Scaling applied to biology 


1000 


body mass (mg) 


750 


500 


250 


Body mass, m, versus leg 
length, L, for the cockroach 
Periplaneta americana 
The data points rep- 
resent individual 
specimens, and the 

curve is a fit to the 

data of the form 


m=kL 3 where k is 
a constant. 





1 2 


length of leg segment (mm) 


o / Geometrical scaling of animals. 


52 


Chapter 1 


1000 





800 
a 
5 
S 800 Surface 
a area versus 
8 body mass for 
5 dwarf sirens, a 
2) 
400 species of sala- 
mander ( Pseudo- 
branchus striatus __). 
The data points 
200 represent individual 
specimens, and the curve is 
afitofthe formA=km = 2/3. 
0 
3 0 500 1000 


body mass (g) 


times more body mass will have only four times more surface area. 


This behavior of the ratio of surface area to mass (or, equiv- 
alently, the ratio of surface area to volume) has important conse- 
quences for mammals, which must maintain a constant body tem- 
perature. It would make sense for the rate of heat loss through the 
animal’s skin to be proportional to its surface area, so we should 
expect small animals, having large ratios of surface area to volume, 
to need to produce a great deal of heat in comparison to their size to 
avoid dying from low body temperature. This expectation is borne 
out by the data of the left-hand panel of figure p, showing the rate 
of oxygen consumption of guinea pigs as a function of their body 
mass. Neither an animal’s heat production nor its surface area is 
convenient to measure, but in order to produce heat, the animal 
must metabolize oxygen, so oxygen consumption is a good indicator 
of the rate of heat production. Since surface area is proportional to 
m2/3, the proportionality of the rate of oxygen consumption to m2/3 
is consistent with the idea that the animal needs to produce heat at a 
rate in proportion to its surface area. Although the smaller animals 


Scaling and Estimation 


Diameter versus length 
of the third lumbar 
vertebrae of adult 
African Bovidae 
(antelopes and oxen). 
The smallest animal 
represented is the 
cat-sized Gunther's 
dik-dik, and the 
largest is the 

850-kg giant 

eland. The 

solid curve is 

a fit of the 


diameter (cm) 


Rate of oxygen 
consumption versus 
body mass for guinea 
pigs at rest. The 
curve is a fit of the 


form (rate)=km 2/3 


oxygen consumption (mL/min) 





0.0 0.2 0.4 0.6 0.8 1.0 0 
body mass (kg) 


p / Scaling of animals’ bodies related to metabolic rate and skeletal strength. 


metabolize less oxygen and produce less heat in absolute terms, the 
amount of food and oxygen they must consume is greater in propor- 
tion to their own mass. The Etruscan pigmy shrew, weighing in at 
2 grams as an adult, is at about the lower size limit for mammals. 
It must eat continually, consuming many times its body weight each 
day to survive. 


Changes in shape to accommodate changes in size 


Large mammals, such as elephants, have a small ratio of surface 
area to volume, and have problems getting rid of their heat fast 
enough. An elephant cannot simply eat small enough amounts to 
keep from producing excessive heat, because cells need to have a 
certain minimum metabolic rate to run their internal machinery. 
Hence the elephant’s large ears, which add to its surface area and 
help it to cool itself. Previously, we have seen several examples 
of data within a given species that were consistent with a fixed 
shape, scaled up and down in the cases of individual specimens. The 
elephant’s ears are an example of a change in shape necessitated by 
a change in scale. 


and the dashed 
line is a linear 
fit. (After 
McMahon and 
Bonner, 1983.) 





length (cm) 


Section 1.3 x Scaling applied to biology 


q / Galileo’s 

showing how 
bones must be greater in diam- 
eter compared to their lengths. 


54 





original drawing, 
larger animals’ 


Large animals also must be able to support their own weight. 
Returning to the example of the strengths of planks of different 
sizes, we can see that if the strength of the plank depends on area 
while its weight depends on volume, then the ratio of strength to 
weight goes as follows: 


strength/weight « A/V « 1/L. 


Thus, the ability of objects to support their own weights decreases 
inversely in proportion to their linear dimensions. If an object is to 
be just barely able to support its own weight, then a larger version 
will have to be proportioned differently, with a different shape. 


Since the data on the cockroaches seemed to be consistent with 
roughly similar shapes within the species, it appears that the abil- 
ity to support its own weight was not the tightest design constraint 
that Nature was working under when she designed them. For large 
animals, structural strength is important. Galileo was the first to 
quantify this reasoning and to explain why, for instance, a large an- 
imal must have bones that are thicker in proportion to their length. 
Consider a roughly cylindrical bone such as a leg bone or a vertebra. 
The length of the bone, LD, is dictated by the overall linear size of the 
animal, since the animal’s skeleton must reach the animal’s whole 
length. We expect the animal’s mass to scale as L?, so the strength 
of the bone must also scale as L?. Strength is proportional to cross- 
sectional area, as with the wooden planks, so if the diameter of the 
bone is d, then 


a «x L? 


d x L3/?, 

If the shape stayed the same regardless of size, then all linear di- 
mensions, including d and L, would be proportional to one another. 
If our reasoning holds, then the fact that d is proportional to L®/?, 
not L, implies a change in proportions of the bone. As shown in the 
right-hand panel of figure p, the vertebrae of African Bovidae follow 
the rule d x L?/? fairly well. The vertebrae of the giant eland are 
as chunky as a coffee mug, while those of a Gunther’s dik-dik are as 
slender as the cap of a pen. 


Discussion questions 


A Single-celled animals must passively absorb nutrients and oxygen 
from their surroundings, unlike humans who have lungs to pump air in and 
out and a heart to distribute the oxygenated blood throughout their bodies. 
Even the cells composing the bodies of multicellular animals must absorb 
oxygen from a nearby capillary through their surfaces. Based on these 
facts, explain why cells are always microscopic in size. 


B The reasoning of the previous question would seem to be contra- 
dicted by the fact that human nerve cells in the spinal cord can be as 
much as a meter long, although their widths are still very small. Why is 
this possible? 


Chapter 1 Scaling and Estimation 


1.4 Order-of-magnitude estimates 


It is the mark of an instructed mind to rest satisfied with the 
degree of precision that the nature of the subject permits and 
not to seek an exactness where only an approximation of the 
truth is possible. 


Aristotle 


It is a common misconception that science must be exact. For 
instance, in the Star Trek TV series, it would often happen that 
Captain Kirk would ask Mr. Spock, “Spock, we’re in a pretty bad 
situation. What do you think are our chances of getting out of 
here?” The scientific Mr. Spock would answer with something like, 
“Captain, I estimate the odds as 237.345 to one.” In reality, he 
could not have estimated the odds with six significant figures of 
accuracy, but nevertheless one of the hallmarks of a person with a 
good education in science is the ability to make estimates that are 
likely to be at least somewhere in the right ballpark. In many such 
situations, it is often only necessary to get an answer that is off by no 
more than a factor of ten in either direction. Since things that differ 
by a factor of ten are said to differ by one order of magnitude, such 
an estimate is called an order-of-magnitude estimate. The tilde, 
~, is used to indicate that things are only of the same order of 
magnitude, but not exactly equal, as in 


odds of survival ~ 100 to one. 


The tilde can also be used in front of an individual number to em- 
phasize that the number is only of the right order of magnitude. 


Although making order-of-magnitude estimates seems simple and 
natural to experienced scientists, it’s a mode of reasoning that is 
completely unfamiliar to most college students. Some of the typical 
mental steps can be illustrated in the following example. 


Cost of transporting tomatoes (incorrect solution) example 4 
> Roughly what percentage of the price of a tomato comes from 
the cost of transporting it in a truck? 


> The following incorrect solution illustrates one of the main ways 
you can go wrong in order-of-magnitude estimates. 


Incorrect solution: Let’s say the trucker needs to make a $400 
profit on the trip. Taking into account her benefits, the cost of gas, 
and maintenance and payments on the truck, let’s say the total 
cost is more like $2000. I’d guess about 5000 tomatoes would fit 
in the back of the truck, so the extra cost per tomato is 40 cents. 
That means the cost of transporting one tomato is comparable to 
the cost of the tomato itself. Transportation really adds a lot to the 
cost of produce, | guess. 


The problem is that the human brain is not very good at esti- 
mating area or volume, so it turns out the estimate of 5000 tomatoes 


Section 1.4 Order-of-magnitude estimates 


55 





r/Can you guess how many 
jelly beans are in the jar? If you 
try to guess directly, you will 
almost certainly underestimate. 
The right way to do it is to esti- 
mate the linear dimensions, then 
get the volume indirectly. See 
problem 26, p. 62. 





s/Consider a_ spherical cow. 


fitting in the truck is way off. That’s why people have a hard time 
at those contests where you are supposed to estimate the number of 
jellybeans in a big jar. Another example is that most people think 
their families use about 10 gallons of water per day, but in reality 
the average is about 300 gallons per day. When estimating area 
or volume, you are much better off estimating linear dimensions, 
and computing volume from the linear dimensions. Here’s a better 
solution to the problem about the tomato truck: 


Cost of transporting tomatoes (correct solution) example 5 
As in the previous solution, say the cost of the trip is $2000. The 
dimensions of the bin are probably 4m x 2m x 1 m, for a vol- 
ume of 8 m?. Since the whole thing is just an order-of-magnitude 
estimate, let’s round that off to the nearest power of ten, 10 m°. 
The shape of a tomato is complicated, and | don’t know any for- 
mula for the volume of a tomato shape, but since this is just an 
estimate, let’s pretend that a tomato is a cube, 0.05m x 0.05m x 
0.05 m, for a volume of 1.25 x 10-4 m®. Since this is just a rough 
estimate, let's round that to 10-4m%. We can find the total num- 
ber of tomatoes by dividing the volume of the bin by the volume 
of one tomato: 10 m°/10-* m® = 10° tomatoes. The transporta- 
tion cost per tomato is $2000/10° tomatoes=$0.02/tomato. That 
means that transportation really doesn’t contribute very much to 
the cost of a tomato. 


Approximating the shape of a tomato as a cube is an example of 
another general strategy for making order-of-magnitude estimates. 
A similar situation would occur if you were trying to estimate how 
many m? of leather could be produced from a herd of ten thousand 
cattle. There is no point in trying to take into account the shape of 
the cows’ bodies. A reasonable plan of attack might be to consider 
a spherical cow. Probably a cow has roughly the same surface area 
as a sphere with a radius of about 1 m, which would be 47(1 m)?. 
Using the well-known facts that pi equals three, and four times three 
equals about ten, we can guess that a cow has a surface area of about 
10 m?, so the herd as a whole might yield 10° m? of leather. 


Estimating mass indirectly example 6 
Usually the best way to estimate mass is to estimate linear di- 
mensions, then use those to infer volume, and then get the mass 
based on the volume. For example, Amphicoelias, shown in the 
figure, may have been the largest land animal ever to live. Fossils 
tell us the linear dimensions of an animal, but we can only indi- 
rectly guess its mass. Given the length scale in the figure, let’s 
estimate the mass of an Amphicoelias. 


Its torso looks like it can be approximated by a rectangular box 
with dimensions 10 mx 5 mx 3m, giving about 2 x 10° m°. Living 
things are mostly made of water, so we assume the animal to 
have the density of water, 1 g/cm®, which converts to 10? kg/m®. 


56 Chapter 1 Scaling and Estimation 


This gives a mass of about 2 x 10° kg, or 200 metric tons. 





The following list summarizes the strategies for getting a good 
order-of-magnitude estimate. 


1. Don’t even attempt more than one significant figure of preci- 
sion. 


2. Don’t guess area, volume, or mass directly. Guess linear di- 
mensions and get area, volume, or mass from them. 


3. When dealing with areas or volumes of objects with complex 
shapes, idealize them as if they were some simpler shape, a 
cube or a sphere, for example. 


4. Check your final answer to see if it is reasonable. If you esti- 
mate that a herd of ten thousand cattle would yield 0.01 m? 
of leather, then you have probably made a mistake with con- 
version factors somewhere. 


Section 1.4 Order-of-magnitude estimates 


57 


58 


Chapter 1 


Summary 


Notation 

OG. MegSodA emacs dus is proportional to 

BOE Ses tN ne Bae ed on the order of, is on the order of 
Summary 


Nature behaves differently on large and small scales. Galileo 
showed that this results fundamentally from the way area and vol- 
ume scale. Area scales as the second power of length, A « L?, while 
volume scales as length to the third power, V « L?. 


An order of magnitude estimate is one in which we do not at- 
tempt or expect an exact answer. The main reason why the unini- 
tiated have trouble with order-of-magnitude estimates is that the 
human brain does not intuitively make accurate estimates of area 
and volume. Estimates of area and volume should be approached 
by first estimating linear dimensions, which one’s brain has a feel 
for. 


Scaling and Estimation 


Problems 
Key 


VA computerized answer check is available online. 
J A problem that requires calculus. 
x A difficult problem. 


1 How many cubic inches are there in a cubic foot? The answer 
is not 12. Vv 
2 Assume a dog’s brain is twice as great in diameter as a cat’s, 


but each animal’s brain cells are the same size and their brains are 
the same shape. In addition to being a far better companion and 
much nicer to come home to, how many times more brain cells does 
a dog have than a cat? The answer is not 2. 

2 


3 The population density of Los Angeles is about 4000 people/km*. 


That of San Francisco is about 6000 people/km?. How many times 
farther away is the average person’s nearest neighbor in LA than in 
San Francisco? The answer is not 1.5. Vv 


4 A hunting dog’s nose has about 10 square inches of active 
surface. How is this possible, since the dog’s nose is only about 1 in 
x lin x 1 in = 1 in®? After all, 10 is greater than 1, so how can it 
fit’? 


5 Estimate the number of blades of grass on a football field. 


6 In a computer memory chip, each bit of information (a 0 or 
a 1) is stored in a single tiny circuit etched onto the surface of a 
silicon chip. The circuits cover the surface of the chip like lots in a 
housing development. A typical chip stores 64 Mb (megabytes) of 
data, where a byte is 8 bits. Estimate (a) the area of each circuit, 
and (b) its linear size. 


7 Suppose someone built a gigantic apartment building, mea- 
suring 10 km x 10 km at the base. Estimate how tall the building 
would have to be to have space in it for the entire world’s population 
to live. 


8 A hamburger chain advertises that it has sold 10 billion Bongo 
Burgers. Estimate the total mass of feed required to raise the cows 
used to make the burgers. 


9 Estimate the volume of a human body, in cm’. 


10 How many cm? is 1 mm?? > Solution, p. 547 


11 Compare the light-gathering powers of a 3-cm-diameter tele- 
scope and a 30-cm telescope. > Solution, p. 547 


12 One step on the Richter scale corresponds to a factor of 100 
in terms of the energy absorbed by something on the surface of the 
Earth, e.g., a house. For instance, a 9.3-magnitude quake would 
release 100 times more energy than an 8.3. The energy spreads out 


Problems 


59 


from the epicenter as a wave, and for the sake of this problem we’ll 
assume we’re dealing with seismic waves that spread out in three 
dimensions, so that we can visualize them as hemispheres spreading 
out under the surface of the earth. If a certain 7.6-magnitude earth- 
quake and a certain 5.6-magnitude earthquake produce the same 
amount of vibration where I live, compare the distances from my 
house to the two epicenters. > Solution, p. 548 


13 In Europe, a piece of paper of the standard size, called A4, 
is a little narrower and taller than its American counterpart. The 
ratio of the height to the width is the square root of 2, and this has 
some useful properties. For instance, if you cut an A4 sheet from left 
to right, you get two smaller sheets that have the same proportions. 
You can even buy sheets of this smaller size, and they’re called A5. 
There is a whole series of sizes related in this way, all with the same 
proportions. (a) Compare an A5 sheet to an A4 in terms of area and 
linear size. (b) The series of paper sizes starts from an AO sheet, 
which has an area of one square meter. Suppose we had a series 
of boxes defined in a similar way: the BO box has a volume of one 
cubic meter, two B1 boxes fit exactly inside an BO box, and so on. 
What would be the dimensions of a BO box? Vv 


14 Estimate the mass of one of the hairs in Albert Einstein’s 
moustache, in units of kg. 


15 According to folklore, every time you take a breath, you are 
inhaling some of the atoms exhaled in Caesar’s last words. Is this 
true? If so, how many? 





Albert Einstein, and his mous- 


tache, problem 14. 16 The Earth’s surface is about 70% water. Mars’s diameter is 
about half the Earth’s, but it has no surface water. Compare the 
land areas of the two planets. v 


17 The traditional Martini glass is shaped like a cone with 
the point at the bottom. Suppose you make a Martini by pouring 
vermouth into the glass to a depth of 3 cm, and then adding gin 
to bring the depth to 6 cm. What are the proportions of gin and 
vermouth? > Solution, p. 548 


18 The central portion of a CD is taken up by the hole and some 
surrounding clear plastic, and this area is unavailable for storing 
data. The radius of the central circle is about 35% of the outer 
radius of the data-storing area. What percentage of the CD’s area 
is therefore lost? Vv 





Problem 19. 19 The one-liter cube in the photo has been marked off into 
smaller cubes, with linear dimensions one tenth those of the big 
one. What is the volume of each of the small cubes? 

> Solution, p. 548 


60 Chapter 1 Scaling and Estimation 


20 [This problem is now problem 0-12 on p. 37.] 


21 Estimate the number of man-hours required for building the 
Great Wall of China. > Solution, p. 548 


22 (a) Using the microscope photo in the figure, estimate the 
mass of a one cell of the E. coli bacterium, which is one of the 
most common ones in the human intestine. Note the scale at the 
lower right corner, which is 1 wm. Each of the tubular objects in 
the column is one cell. (b) The feces in the human intestine are 
mostly bacteria (some dead, some alive), of which EL. coli is a large 
and typical component. Estimate the number of bacteria in your 
intestines, and compare with the number of human cells in your 
body, which is believed to be roughly on the order of 10!%. (c) 
Interpreting your result from part b, what does this tell you about 
the size of a typical human cell compared to the size of a typical 
bacterial cell? 


23 A taxon (plural taxa) is a group of living things. For ex- 
ample, Homo sapiens and Homo neanderthalensis are both taxa — 
specifically, they are two different species within the genus Homo. 
Surveys by botanists show that the number of plant taxa native 
to a given contiguous land area A is usually approximately propor- 
tional to A‘/3. (a) There are 70 different species of lupine native 
to Southern California, which has an area of about 200,000 km?. 
The San Gabriel Mountains cover about 1,600 km?. Suppose that 
you wanted to learn to identify all the species of lupine in the San 
Gabriels. Approximately how many species would you have to fa- 
miliarize yourself with? > Answer, p. 569 V 
(b) What is the interpretation of the fact that the exponent, 1/3, is 
less than one? 


Problem 22. 


Problems 





61 


24 X-ray images aren’t only used with human subjects but also, 
for example, on insects and flowers. In 2003, a team of researchers 
at Argonne National Laboratory used x-ray imagery to find for the 
first time that insects, although they do not have lungs, do not 
necessarily breathe completely passively, as had been believed pre- 
viously; many insects rapidly compress and expand their trachea, 
head, and thorax in order to force air in and out of their bodies. 
One difference between x-raying a human and an insect is that if a 
medical x-ray machine was used on an insect, virtually 100% of the 
x-rays would pass through its body, and there would be no contrast 
in the image produced. Less penetrating x-rays of lower energies 
have to be used. For comparison, a typical human body mass is 
about 70 kg, whereas a typical ant is about 10 mg. Estimate the 
ratio of the thicknesses of tissue that must be penetrated by x-rays 
in one case compared to the other. v 


25 Radio was first commercialized around 1920, and ever since 
then, radio signals from our planet have been spreading out across 
our galaxy. It is possible that alien civilizations could detect these 
signals and learn that there is life on earth. In the 90 years that the 
signals have been spreading at the speed of light, they have created 
a sphere with a radius of 90 light-years. To show an idea of the 
size of this sphere, I’ve indicated it in the figure as a tiny white 
circle on an image of a spiral galaxy seen edge on. (We don’t have 
similar photos of our own Milky Way galaxy, because we can’t see 
Problem 25. it from the outside.) So far we haven’t received answering signals 
from aliens within this sphere, but as time goes on, the sphere will 
expand as suggested by the dashed outline, reaching more and more 
stars that might harbor extraterrestrial life. Approximately what 
year will it be when the sphere has expanded to fill a volume 100 
times greater than the volume it fills today in 2010? Vv 





26 Estimate the number of jellybeans in figure r on p. 56. 
> Solution, p. 548 


27 At the grocery store you will see oranges packed neatly in 
stacks. Suppose we want to pack spheres as densely as possible, 
so that the greatest possible fraction of the space is filled by the 
spheres themselves, not by empty space. Let’s call this fraction f. 
Mathematicians have proved that the best possible result is f ~ 
0.7405, which requires a systematic pattern of stacking. If you buy 
ball bearings or golf balls, however, the seller is probably not going 
to go to the trouble of stacking them neatly. Instead they will 
probably pour the balls into a box and vibrate the box vigorously 
for a while to make them settle. This results in a random packing. 
The closest random packing has f ¥ 0.64. Suppose that golf balls, 
with a standard diameter of 4.27 cm, are sold in bulk with the 
Problem 27. closest random packing. What is the diameter of the largest ball 
that could be sold in boxes of the same size, packed systematically, 
so that there would be the same number of balls per box? v 
62 Chapter 1. Scaling and Estimation 
28 Plutonium-239 is one of a small number of important long- 
lived forms of high-level radioactive nuclear waste. The world’s 
waste stockpiles have about 10° metric tons of plutonium. Drinking 
water is considered safe by U.S. government standards if it contains 
less than 2 x 107!3 g/cm? of plutonium. The amount of radioac- 
tivity to which you were exposed by drinking such water on a daily 
basis would be very small compared to the natural background radi- 
ation that you are exposed to every year. Suppose that the world’s 
inventory of plutonium-239 were ground up into an extremely fine 
dust and then dispersed over the world’s oceans, thereby becoming 
mixed uniformly into the world’s water supplies over time. Esti- 
mate the resulting concentration of plutonium, and compare with 
the government standard. 
Problems 
63 
Exercise 1: Scaling applied to leaves 
Equipment: 
leaves of three sizes, having roughly similar proportions of length, width, and thickness balance 
Each group will have one leaf, and should measure its surface area and volume, and determine 
its surface-to-volume ratio. For consistency, every group should use units of cm? and cm?, and 
should only find the area of one side of the leaf. The area can be found by tracing the area of 
the leaf on graph paper and counting squares. The volume can be found by weighing the leaf 
and assuming that its density is 1 g/cm? (the density of water). What implications do your results have for the plants’ abilities to survive in different environments? 
The idea is that each of the algebra symbols with an arrow writ- 
ten on top, called a vector, is actually an abbreviation for three 
different numbers, the x, y, and z components. The three compo- 
nents are referred to as the components of the vector, e.g., Fi, is the 
x component of the vector F’. The notation with an arrow on top 
is good for handwritten equations, but is unattractive in a printed 
book, so books use boldface, F, to represent vectors. After this 
point, I’ll use boldface for vectors throughout this book. 


Quantities can be classified as vectors or scalars. In a phrase like 
“a to the northeast,” it makes sense to fill in the blank with 
“force” or “velocity,” which are vectors, but not with “mass” or 
“time,” which are scalars. Any nonzero vector has both a direction 
and an amount. The amount is called its magnitude. The notation 
for the magnitude of a vector A is |AJ, like the absolute value sign 
used with scalars. 


Often, as in example (b), we wish to use the vector notation to 
represent adding up all the x components to get a total x component, 
etc. The plus sign is used between two vectors to indicate this type 
of component-by-component addition. Of course, vectors are really 
triplets of numbers, not numbers, so this is not the same as the use 
of the plus sign with individual numbers. But since we don’t want to 
have to invent new words and symbols for this operation on vectors, 
we use the same old plus sign, and the same old addition-related 
words like “add,” “sum,” and “total.” Combining vectors this way 
is called vector addition. 


Similarly, the minus sign in example (a) was used to indicate 
negating each of the vector’s three components individually. The 
equals sign is used to mean that all three components of the vector 
on the left side of an equation are the same as the corresponding 
components on the right. 


Example (c) shows how we abuse the division symbol in a similar 
manner. When we write the vector Av divided by the scalar At, 
we mean the new vector formed by dividing each one of the velocity 
components by At. 


It’s not hard to imagine a variety of operations that would com- 
bine vectors with vectors or vectors with scalars, but only four of 
them are required in order to express Newton’s laws: 


Chapter 7 Vectors 


operation definition 

vector + vector Add component by component to 
make a new set of three numbers. 

vector — vector Subtract component by component 
to make a new set of three numbers. 


vector - scalar Multiply each component of the vec- 
tor by the scalar. 
vector /scalar Divide each component of the vector 


by the scalar. 


As an example of an operation that is not useful for physics, there 
just aren’t any useful physics applications for dividing a vector by 
another vector component by component. In optional section 7.5, 
we discuss in more detail the fundamental reasons why some vector 
operations are useful and others useless. 


We can do algebra with vectors, or with a mixture of vectors 
and scalars in the same equation. Basically all the normal rules of 
algebra apply, but if you’re not sure if a certain step is valid, you 
should simply translate it into three component-based equations and 
see if it works. 


Order of addition example 1 
> If we are adding two force vectors, F + G, is it valid to assume 
as in ordinary algebra that F + Gis the same as G+ F? 


> To tell if this algebra rule also applies to vectors, we simply 
translate the vector notation into ordinary algebra notation. In 
terms of ordinary numbers, the components of the vector F + G 
would be Fy + Gy, Fy + Gy, and Fz + Gz, which are certainly the 
same three numbers as Gy + Fy, Gy + Fy, and Gz+ Fz. Yes, F+G 
is the same as G+F. 


It is useful to define a symbol r for the vector whose components 
are x, y, and z, and a symbol Ar made out of Az, Ay, and Az. 


Although this may all seem a little formidable, keep in mind that 
it amounts to nothing more than a way of abbreviating equations! 
Also, to keep things from getting too confusing the remainder of this 
chapter focuses mainly on the Ar vector, which is relatively easy to 
visualize. 


self-check A 

Translate the equations vy = Ax/At, v, = Ay/At, and vz = Az/At for 
motion with constant velocity into a single equation in vector notation. 
> Answer, p. 566 


Section 7.1 


Vector notation 


205 


nl 


x component 
(positive) 


y 
y component 
(negative) 
x 


b/The x and y components 
of a vector can be thought of as 
the shadows it casts onto the x 
and y axes. 


Q 


c / Self-check B. 





d/A_ playing card returns to 
its original state when rotated by 
180 degrees. 


Drawing vectors as arrows 


A vector in two dimensions can be easily visualized by drawing 
an arrow whose length represents its magnitude and whose direction 
represents its direction. The « component of a vector can then be 
visualized as the length of the shadow it would cast in a beam of 
light projected onto the x axis, and similarly for the y component. 
Shadows with arrowheads pointing back against the direction of the 
positive axis correspond to negative components. 


In this type of diagram, the negative of a vector is the vector 
with the same magnitude but in the opposite direction. Multiplying 
a vector by a scalar is represented by lengthening the arrow by that 
factor, and similarly for division. 


self-check B 

Given vector Q represented by an arrow in figure c, draw arrows repre- 
senting the vectors 1.5Q and —Q. > Answer, p. 
566 


This leads to a way of defining vectors and scalars that reflects 
how physicists think in general about these things: 


definition of vectors and scalars 
A general type of measurement (force, velocity, ...) is a vector if it 
can be drawn as an arrow so that rotating the paper produces the 
same result as rotating the actual quantity. A type of quantity that 
never changes at all under rotation is a scalar. 


For example, a force reverses itself under a 180-degree rotation, 
but a mass doesn’t. We could have defined a vector as something 
that had both a magnitude and a direction, but that would have left 
out zero vectors, which don’t have a direction. A zero vector is a 
legitimate vector, because it behaves the same way under rotations 
as a zero-length arrow, which is simply a dot. 


A remark for those who enjoy brain-teasers: not everything is 
a vector or a scalar. An American football is distorted compared 
to a sphere, and we can measure the orientation and amount of 
that distortion quantitatively. The distortion is not a vector, since 
a 180-degree rotation brings it back to its original state. Something 
similar happens with playing cards, figure d. For some subatomic 
particles, such as electrons, 360 degrees isn’t even enough; a 720- 
degree rotation is needed to put them back the way they were! 


Discussion questions 


A You drive to your friend’s house. How does the magnitude of your Ar 
vector compare with the distance you've added to the car’s odometer? 


206 Chapter 7 Vectors 


7.2 Calculations with magnitude and direction 


If you ask someone where Las Vegas is compared to Los Angeles, 
they are unlikely to say that the Ax is 290 km and the Ay is 230 
km, in a coordinate system where the positive x axis is east and the 
y axis points north. They will probably say instead that it’s 370 
km to the northeast. If they were being precise, they might give the 
direction as 38° counterclockwise from east. In two dimensions, we 
can always specify a vector’s direction like this, using a single angle. 
A magnitude plus an angle suffice to specify everything about the 
vector. The following two examples show how we use trigonometry 
and the Pythagorean theorem to go back and forth between the x—y 
and magnitude-angle descriptions of vectors. 


Finding magnitude and angle from components example 2 
> Given that the Ar vector from LA to Las Vegas has Ax = 290 km 
and Ay = 230 km, how would we find the magnitude and direction 
of Ar? 


> We find the magnitude of Ar from the Pythagorean theorem: 


|Ar| = ,/Ax2 + Ay? 


= 370 km 





We know all three sides of the triangle, so the angle @ can be 
found using any of the inverse trig functions. For example, we 
know the opposite and adjacent sides, so 


_1 AY 
1 —— 
8 = tan ea 
= 38°. 
Finding components from magnitude and angle example 3 


> Given that the straight-line distance from Los Angeles to Las 
Vegas is 370 km, and that the angle 0 in the figure is 38°, how 
can the x and y components of the Ar vector be found? 


> The sine and cosine of 0 relate the given information to the 
information we wish to find: 


AX 

cos 8 = —_— 
|Ar| 

; Ay 
sin@ = —— 
|Ar| 


Solving for the unknowns gives 


Ax = |Ar| cos 8 

= 290 km and 
Ay = |Ar|sin@ 

= 230 km. 





“YS ax 


Los 
Angeles 


Las V egas 








e / Examples 2 and 3. 


Section 7.2 Calculations with magnitude and direction 


207 


Los 
Angeles 





Ax 
(negative) 


f / Example 4. 


208 


San Diego 


The following example shows the correct handling of the plus 
and minus signs, which is usually the main cause of mistakes. 


Negative components example 4 
> San Diego is 120 km east and 150 km south of Los Angeles. An 
airplane pilot is setting course from San Diego to Los Angeles. At 
what angle should she set her course, measured counterclock- 
wise from east, as shown in the figure? 


> If we make the traditional choice of coordinate axes, with x 
pointing to the right and y pointing up on the map, then her Ax is 
negative, because her final x value is less than her initial x value. 
Her Ay is positive, so we have 


Ax = —120 km 
Ay = 150 km. 


If we work by analogy with example 2, we get 


_1 Ay 

7 (SY 

8 =tan Ax 
= tan~'(—1.25) 
= —51°. 


According to the usual way of defining angles in trigonometry, 
a negative result means an angle that lies clockwise from the x 
axis, which would have her heading for the Baja California. What 
went wrong? The answer is that when you ask your calculator to 
take the arctangent of a number, there are always two valid pos- 
sibilities differing by 180°. That is, there are two possible angles 
whose tangents equal -1.25: 


tan 129° = —1.25 
tan —51° = —1.25 


You calculator doesn’t Know which is the correct one, so it just 
picks one. In this case, the one it picked was the wrong one, and 
it was up to you to add 180°to it to find the right answer. 


Chapter 7 Vectors 





‘A shortcut example 5 
> A split second after nine o’clock, the hour hand on a clock dial 
has moved clockwise past the nine-o’clock position by some im- 
perceptibly small angle @. Let positive x be to the right and posi- 
tive y up. If the hand, with length 2, is represented by a Ar vector g / Example 5. 
going from the dial’s center to the tip of the hand, find this vector’s 

AX. 





> The following shortcut is the easiest way to work out examples 
like these, in which a vector’s direction is known relative to one 
of the axes. We can tell that Ar will have a large, negative x 
component and a small, positive y. Since Ax < 0, there are 
really only two logical possibilities: either Ax = —£cos >, or Ax = 
—fsind. Because ¢ is small, cos ¢ is large and sind is small. 
We conclude that Ax = —fcos . 


A typical application of this technique to force vectors is given in 
example 6 on p. 226. 


Discussion question 


A _ Inexample 4, we dealt with components that were negative. Does it 
make sense to classify vectors as positive and negative? 


Section 7.2 Calculations with magnitude and direction 209 


Las Vegas 








oY 
Los 
Angeles 
San Diego 
h/ Example 6. 


A+B 
A 
A 

al 7A 

B 
i/ Vectors can be added graph- 
ically by placing them tip to tail, 
and then drawing a vector from 


the tail of the first vector to the tip 
of the second vector. 


7.3. Techniques for adding vectors 


Vector addition is one of the three essential mathematical skills, 
summarized on pp.545-546, that you need for success in this course. 


Addition of vectors given their components 


The easiest type of vector addition is when you are in possession 
of the components, and want to find the components of their sum. 


Adding components example 6 
> Given the Ax and Ay values from the previous examples, find 
the Ax and Ay from San Diego to Las Vegas. 


> 


AXtotal = AX1 + Axo 
= —120 km+ 290 km 
= 170 km 


AYtotal = AY1 + Aye 
= 150 km + 230 km 
= 380 


Note how the signs of the x components take care of the west- 
ward and eastward motions, which partially cancel. 


Addition of vectors given their magnitudes and directions 


In this case, you must first translate the magnitudes and di- 
rections into components, and the add the components. In our San 
Diego-Los Angeles-Las Vegas example, we can simply string together 
the preceding examples; this is done on p. 546. 


Graphical addition of vectors 


Often the easiest way to add vectors is by making a scale drawing 
on a piece of paper. This is known as graphical addition, as opposed 
to the analytic techniques discussed previously. (It has nothing to 
do with x — y graphs or graph paper. “Graphical” here simply 
means drawing. It comes from the Greek verb “grapho,” to write, 
like related English words including “graphic.” ) 


210 Chapter 7 Vectors 


LA to Vegas, graphically example 7 
> Given the magnitudes and angles of the Ar vectors from San 
Diego to Los Angeles and from Los Angeles to Las Vegas, find 
the magnitude and angle of the Ar vector from San Diego to Las 
Vegas. 


> Using a protractor and a ruler, we make a careful scale draw- 
ing, as shown in figure j. The protractor can be conveniently 
aligned with the blue rules on the notebook paper. A scale of 
1 mm -> 2 km was chosen for this solution because it was as big 
as possible (for accuracy) without being so big that the drawing 
wouldn't fit on the page. With a ruler, we measure the distance 
from San Diego to Las Vegas to be 206 mm, which corresponds 
to 412 km. With a protractor, we measure the angle 0 to be 65°. A If you’re doing graphical addition of vectors, does it matter which 
vector you start with and which vector you start from the other vector’s 
tip? 

B If you add a vector with magnitude 1 to a vector of magnitude 2, 
what magnitudes are possible for the vector sum? 


Cc Which of these examples of vector addition are correct, and which 
are incorrect? 


7.4 x Unit vector notation 


When we want to specify a vector by its components, it can be cum- 
bersome to have to write the algebra symbol for each component: 


Az = 290 km, Ay = 230 km 
A more compact notation is to write 
Ar = (290 km)x + (230 km)y, 


where the vectors x, y, and Z, called the unit vectors, are defined 
as the vectors that have magnitude equal to 1 and directions lying 
along the x, y, and z axes. In speech, they are referred to as “x-hat” 
and so on. 


A slightly different, and harder to remember, version of this 
notation is unfortunately more prevalent. In this version, the unit 
vectors are called i, j, and k: 


Ar = (290 km)i + (230 km)j. 


Chapter 7 Vectors 


7.5 x Rotational invariance 


Let’s take a closer look at why certain vector operations are use- 
ful and others are not. Consider the operation of multiplying two 
vectors component by component to produce a third vector: 


Ry — P,Q 
Ry = PyQy 
R, = PQ: 


As a simple example, we choose vectors P and Q to have length 
1, and make them perpendicular to each other, as shown in figure 
k/1. If we compute the result of our new vector operation using the 
coordinate system in k/2, we find: 


Ry =0 
Res 
R,=0. 


The x component is zero because P, = 0, the y component is zero 
because @, = 0, and the z component is of course zero because 
both vectors are in the x — y plane. However, if we carry out the 
same operations in coordinate system k/3, rotated 45 degrees with 
respect to the previous one, we find 


Ry, = 1/2 
Ry =—1/2 
Ry= 0. 


The operation’s result depends on what coordinate system we use, 
and since the two versions of R. have different lengths (one being zero 
and the other nonzero), they don’t just represent the same answer 
expressed in two different coordinate systems. Such an operation 
will never be useful in physics, because experiments show physics 
works the same regardless of which way we orient the laboratory 
building! The useful vector operations, such as addition and scalar 
multiplication, are rotationally invariant, i.e., come out the same 
regardless of the orientation of the coordinate system. 


Calibrating an electronic compass example 8 
Some smart phones and GPS units contain electronic compasses 
that can sense the direction of the earth’s magnetic field vector, 
notated B. Because all vectors work according to the same rules, 
you don’t need to know anything special about magnetism in or- 
der to understand this example. Unlike a traditional compass that 
uses a magnetized needle on a bearing, an electronic compass 
has no moving parts. It contains two sensors oriented perpendic- 
ular to one another, and each sensor is only sensitive to the com- 
ponent of the earth’s field that lies along its own axis. Because a 


/ 


3 
k / Component-by-component 
multiplication of the vectors in 1 


would produce different vectors 
in coordinate systems 2 and 3. 


Section 7.5 x Rotational invariance 213 


214 


Chapter 7 Vectors 


choice of coordinates is arbitrary, we can take one of these sen- 
sors as defining the x axis and the other the y. Given the two 
components B, and By, the device’s computer chip can compute 
the angle of magnetic north relative to its sensors, tan~'(By/B,). 


All compasses are vulnerable to errors because of nearby mag- 
netic materials, and in particular it may happen that some part 
of the compass’s own housing becomes magnetized. In an elec- 
tronic compass, rotational invariance provides a convenient way 
of calibrating away such effects by having the user rotate the de- 
vice in a horizontal circle. 


Suppose that when the compass is oriented in a certain way, it 
measures B, = 1.00 and By = 0.00 (in certain units). We then 
expect that when it is rotated 90 degrees clockwise, the sensors 
will detect B, = 0.00 and By = 1.00. 


But imagine instead that we get By = 0.20 and By = 0.80. This 
would violate rotational invariance, since rotating the coordinate 
system is supposed to give a different description of the same 
vector. The magnitude appears to have changed from 1.00 to 
V0.202 + 0.802 = 0.82, and a vector can’t change its magnitude 
just because you rotate it. The compass’s computer chip figures 
out that some effect, possibly a slight magnetization of its hous- 
ing, must be adding an erroneous 0.2 units to all the By readings, 
because subtracting this amount from all the B, values gives vec- 
tors that have the same magnitude, satisfying rotational invari- 
ance. 


Summary 
Selected vocabulary 


vector. ...... a quantity that has both an amount (magni- 
tude) and a direction in space 

magnitude .... the “amount” associated with a vector 

scalar ....... a quantity that has no direction in space, only 
an amount 

Notation 

RS erinaiis ek Sa a vector with components A;, A,, and A, 

A ssravigube aide ie tt handwritten notation for a vector 

AE) i oe wate the magnitude of vector A 

1 siege A the vector whose components are x, y, and z 

PNT oo hastens the vector whose components are Ax, Ay, and 
Az 

Ky Vew ae Saks (optional topic) unit vectors; the vectors with 
magnitude 1 lying along the x, y, and z axes 

Ty fg ioe ted Ak a harder to remember notation for the unit 
vectors 


Other terminology and notation 
displacement vec- a name for the symbol Ar 


TOPS Se eee 
speed ....... the magnitude of the velocity vector, i.e., the 
velocity stripped of any information about its 
direction 
Summary 


A vector is a quantity that has both a magnitude (amount) and 
a direction in space, as opposed to a scalar, which has no direction. 
The vector notation amounts simply to an abbreviation for writing 
the vector’s three components. 


In two dimensions, a vector can be represented either by its two 
components or by its magnitude and direction. The two ways of 
describing a vector can be related by trigonometry. 


The two main operations on vectors are addition of a vector to 
a vector, and multiplication of a vector by a scalar. 


Vector addition means adding the components of two vectors 
to form the components of a new vector. In graphical terms, this 
corresponds to drawing the vectors as two arrows laid tip-to-tail and 
drawing the sum vector from the tail of the first vector to the tip 
of the second one. Vector subtraction is performed by negating the 
vector to be subtracted and then adding. 


Multiplying a vector by a scalar means multiplying each of its 
components by the scalar to create a new vector. Division by a 
scalar is defined similarly. 


Summary 


215 


Problems 
Key 


Vv A computerized answer check is available online. 
J A problem that requires calculus. 
x A difficult problem. 


1 The figure shows vectors A and B. Graphically calculate 
the following, as in figure i on p. 210, self-check C on p. 211, and 


A self-check B on p. 206. 
A+B, A—B, B— A, —2B, A — 2B 
<e—___ : 
No numbers are involved. 
B 
Problem 1. 


2 Phnom Penh is 470 km east and 250 km south of Bangkok. 
Hanoi is 60 km east and 1030 km north of Phnom Penh. 

(a) Choose a coordinate system, and translate these data into Ax 
and Ay values with the proper plus and minus signs. 

(b) Find the components of the Ar vector pointing from Bangkok 
to Hanoi. Vv 


3 If you walk 35 km at an angle 25° counterclockwise from east, 
and then 22 km at 230° counterclockwise from east, find the distance 
and direction from your starting point to your destination. v 


4 A machinist is drilling holes in a piece of aluminum according 
to the plan shown in the figure. She starts with the top hole, then 
moves to the one on the left, and then to the one on the right. Since 
this is a high-precision job, she finishes by moving in the direction 
and at the angle that should take her back to the top hole, and 
checks that she ends up in the same place. What are the distance 
Problem 4. and direction from the right-hand hole to the top one? v 





216 Chapter 7 Vectors 


5 Suppose someone proposes a new operation in which a vector 
A and a scalar B are added together to make a new vector C like 
this: 


C,=Az+B 
Cy=Ay+B 
C,=A,+B 





Prove that this operation won’t be useful in physics, because it’s 
not rotationally invariant. 


Problems 


217 


218 Chapter 7 Vectors 


a: 2 6. 2. 6 eb | 
| | 


i2 13 14 15 36 17 15 ip ae 
| @ 





yu E MORSE IN Motion. 
/ / 


MUYBRIDGE 


Chapter 8 
Vectors and Motion 


In 1872, capitalist and former California governor Leland Stanford 
asked photographer Eadweard Muybridge if he would work for him 
on a project to settle a $25,000 bet (a princely sum at that time). 
Stanford’s friends were convinced that a trotting horse always had 
at least one foot on the ground, but Stanford claimed that there was 
a moment during each cycle of the motion when all four feet were 
in the air. The human eye was simply not fast enough to settle the 
question. In 1878, Muybridge finally succeeded in producing what 
amounted to a motion picture of the horse, showing conclusively 
that all four feet did leave the ground at one point. (Muybridge was 
a colorful figure in San Francisco history, and his acquittal for the 
murder of his wife’s lover was considered the trial of the century in 
California. ) 


The losers of the bet had probably been influenced by Aris- 
totelian reasoning, for instance the expectation that a leaping horse 
would lose horizontal velocity while in the air with no force to push 
it forward, so that it would be more efficient for the horse to run 
without leaping. But even for students who have converted whole- 


a 


219 





a/The racing greyhound’s 
velocity vector is in the direction 
of its motion, i.e., tangent to its 
curved path. 








y 


b/ Example 1. 


heartedly to Newtonianism, the relationship between force and ac- 
celeration leads to some conceptual difficulties, the main one being 
a problem with the true but seemingly absurd statement that an 
object can have an acceleration vector whose direction is not the 
same as the direction of motion. The horse, for instance, has nearly 
constant horizontal velocity, so its a, is zero. But as anyone can tell 
you who has ridden a galloping horse, the horse accelerates up and 
down. The horse’s acceleration vector therefore changes back and 
forth between the up and down directions, but is never in the same 
direction as the horse’s motion. In this chapter, we will examine 
more carefully the properties of the velocity, acceleration, and force 
vectors. No new principles are introduced, but an attempt is made 
to tie things together and show examples of the power of the vector 
formulation of Newton’s laws. 


8.1 The velocity vector 
For motion with constant velocity, the velocity vector is 
v = Ar/At. 


The Ar vector points in the direction of the motion, and dividing 
it by the scalar At only changes its length, not its direction, so the 
velocity vector points in the same direction as the motion. When the 
velocity is not constant, i.e., when the x —t, y—t, and z—t graphs 
are not all linear, we use the slope-of-the-tangent-line approach to 
define the components v;, vy, and vz, from which we assemble the 
velocity vector. Even when the velocity vector is not constant, it 
still points along the direction of motion. 


[only for constant velocity] 


Vector addition is the correct way to generalize the one-dimensional 
concept of adding velocities in relative motion, as shown in the fol- 
lowing example: 





‘Velocity vectors in relative motion example 1 
> You wish to cross a river and arrive at a dock that is directly 
across from you, but the river’s current will tend to carry you 
downstream. To compensate, you must steer the boat at an an- 
gle. Find the angle 0, given the magnitude, |Vy,|, of the water’s 
velocity relative to the land, and the maximum speed, |Vgy, of 
which the boat is capable relative to the water. 


> The boat’s velocity relative to the land equals the vector sum of 
its velocity with respect to the water and the water’s velocity with 
respect to the land, 


VeL = Veaw + Vw. 
If the boat is to travel straight across the river, i.e., along the y 


axis, then we need to have Vg,,x = 0. This x component equals 
the sum of the x components of the other two vectors, 


VeL,x = VBw,x + VWLxs 


220 Chapter 8 Vectors and Motion 


or 


O= —|Vew| sin 0 + \Vw_l- 


Solving for 8, we find sin 8 = |Vwz|/|Vgw|, so 





a Seine Vw 
lVew| 
> Solved problem: Annie Oakley page 234, problem 8 
Discussion questions 
A Is it possible for an airplane to maintain a constant velocity vector 


but not a constant |v|? How about the opposite — a constant |v| but not a 
constant velocity vector? Explain. 


B New York and Rome are at about the same latitude, so the earth’s 
rotation carries them both around nearly the same circle. Do the two cities 
have the same velocity vector (relative to the center of the earth)? If not, 
is there any way for two cities to have the same velocity vector? 





The acceleration vector 


When the acceleration is constant, we can define the acceleration 
vector as 


a= Av/At, [only for constant acceleration] 
which can be written in terms of initial and final velocities as 
a= (vy — vy) /At. [only for constant acceleration] 


Otherwise, we can use the type of graphical definition described in 
section 8.1 for the velocity vector. 


Now there are two ways in which we could have a nonzero accel- 
eration. Either the magnitude or the direction of the velocity vector 
could change. This can be visualized with arrow diagrams as shown 
in figures c and d. Both the magnitude and direction can change 
simultaneously, as when a car accelerates while turning. Only when 
the magnitude of the velocity changes while its direction stays con- 
stant do we have a Av vector and an acceleration vector along the 
same line as the motion. 


self-check A 

(1) In figure c, is the object speeding up, or slowing down? (2) What 
would the diagram look like if v; was the same as v;? (3) Describe how 
the Av vector is different depending on whether an object is speeding 
up or slowing down. > Answer, p. 566 





c/A_ change 


in the magni- 


tude of the velocity vector implies 


an acceleration. 





d/A_ change 
of the velocity 


in the direction 
vector also pro- 


duces a nonzero Av vector, and 
thus a nonzero. acceleration 


vector, Av/At. 


Section 8.2 The acceleration vector 


221 


The acceleration vector points in the direction that an accelerom- 
eter would point, as in figure e. 


e/The car has just swerved to 
the right. The air freshener hang- 
ing from the rear-view mirror acts 
as an accelerometer, showing 
that the acceleration vector is to 
the right. 





self-check B 
In projectile motion, what direction does the acceleration vector have? 
> Answer, p. 566 


velocity acceleration force 








™ < + 
\ . . 
| = « 
< < 
x < « 
| = + 
< 
A +] = 
f / Example 2. 
‘Rappelling example 2 


In figure f, the rappeller’s velocity has long periods of gradual 
change interspersed with short periods of rapid change. These 
correspond to periods of small acceleration and force, and peri- 
ods of large acceleration and force. 


222 Chapter 8 Vectors and Motion 

















Av points down —— ee Av points up 


g / Example 3. 





‘The galloping horse example 3 
Figure g on page 223 shows outlines traced from the first, third, 
fifth, seventh, and ninth frames in Muybridge’s series of pho- 
tographs of the galloping horse. The estimated location of the 
horse’s center of mass is shown with a circle, which bobs above 
and below the horizontal dashed line. 


If we don’t care about calculating velocities and accelerations in 
any particular system of units, then we can pretend that the time 
between frames is one unit. The horse’s velocity vector as it 
moves from one point to the next can then be found simply by 
drawing an arrow to connect one position of the center of mass to 
the next. This produces a series of velocity vectors which alter- 
nate between pointing above and below horizontal. 


The Av vector is the vector which we would have to add onto one 
velocity vector in order to get the next velocity vector in the series. 
The Av vector alternates between pointing down (around the time 
when the horse is in the air, B) and up (around the time when the 
horse has two feet on the ground, D). 


Section 8.2 The acceleration vector 








223 








h/ Example 4. 


224 


Discussion questions 


A When acar accelerates, why does a bob hanging from the rearview 
mirror swing toward the back of the car? Is it because a force throws it 
backward? If so, what force? Similarly, describe what happens in the 
other cases described above. 


B Superman is guiding a crippled spaceship into port. The ship’s 
engines are not working. If Superman suddenly changes the direction of 
his force on the ship, does the ship’s velocity vector change suddenly? Its 
acceleration vector? Its direction of motion? 





The force vector and simple machines 


Force is relatively easy to intuit as a vector. The force vector points 
in the direction in which it is trying to accelerate the object it is 
acting on. 


Since force vectors are so much easier to visualize than accel- 
eration vectors, it is often helpful to first find the direction of the 
(total) force vector acting on an object, and then use that to find 
the direction of the acceleration vector. Newton’s second law tells 
us that the two must be in the same direction. 


A component of a force vector example 4 
Figure h, redrawn from a classic 1920 textbook, shows a boy 
pulling another child on a sled. His force has both a horizontal 
component and a vertical one, but only the horizontal one accel- 
erates the sled. (The vertical component just partially cancels the 
force of gravity, causing a decrease in the normal force between 
the runners and the snow.) There are two triangles in the figure. 
One triangle’s hypotenuse is the rope, and the other's is the mag- 
nitude of the force. These triangles are similar, so their internal 
angles are all the same, but they are not the same triangle. One 
is a distance triangle, with sides measured in meters, the other 
a force triangle, with sides in newtons. In both cases, the hori- 
zontal leg is 93% as long as the hypotenuse. It does not make 
sense, however, to compare the sizes of the triangles — the force 
triangle is not smaller in any meaningful sense. 


Chapter 8 Vectors and Motion 


Pushing a block up a ramp example 5 
> Figure i shows a block being pushed up a frictionless ramp at 
constant speed by an externally applied force Fy4. How much 
force is required, in terms of the block’s mass, m, and the angle 
of the ramp, 0? 


> We analyze the forces on the block and introduce notation for 
the other forces besides F,: 





force acting on block 3rd-law partner 

ramp’s normal force on block, | block’s normal force on ramp, 
Fy, a v 
external object's force on | block’s force on external ob- 
block (type irrelevant), Fa « | ject (type irrelevant), y 
planet earth’s gravitational | block’s gravitational force on 
force on block, Fy | earth, + 























Because the block is being pushed up at constant speed, it has 
zero acceleration, and the total force on it must be zero. From 
figure j, we find 


|F | = |Fy| sind 
= mgsin0. 


Since the sine is always less than one, the applied force is always 
less than mg, |.e., pushing the block up the ramp is easier than 
lifting it straight up. This is presumably the principle on which the 
pyramids were constructed: the ancient Egyptians would have 
had a hard time applying the forces of enough slaves to equal the 
full weight of the huge blocks of stone. 


Essentially the same analysis applies to several other simple ma- 
chines, such as the wedge and the screw. 


Fa 


Bais 


i/The applied force F4 pushes 
the block up the frictionless ramp. 


j/lf the block is to move at 
constant velocity, Newton’s first 
law says that the three force 
vectors acting on it must add 
up to zero. To perform vector 
addition, we put the vectors tip 
to tail, and in this case we are 
adding three vectors, so each 
one’s tail goes against the tip of 
the previous one. Since they are 
supposed to add up to zero, the 
third vector’s tip must come back 
to touch the tail of the first vector. 
They form a triangle, and since 
the applied force is perpendicular 
to the normal force, it is a right 
triangle. 


Section 8.3. The force vector and simple machines 225 


k / Example 6 and problem 18 on 
p. 237. 





|/ Example 7. 








A layback example 6 
The figure shows a rock climber using a technique called a lay- 
back. He can make the normal forces Fry; and Fy large, which 
has the side-effect of increasing the frictional forces Fe, and Fro, 
so that he doesn’t slip down due to the gravitational (weight) force 
Fy. The purpose of the problem is not to analyze all of this in de- 
tail, but simply to practice finding the components of the forces 
based on their magnitudes. To keep the notation simple, let's 
write Fry; for |Fyi|, etc. The crack overhangs by a small, positive 
angle 8 + 9°. 


In this example, we determine the x component of Fyy;. The other 
nine components are left as an exercise to the reader (problem 
18, p. 237). 


The easiest method is the one demonstrated in example 5 on 
p. 209. Casting vector F,y;’s shadow on the ground, we can tell 
that it would point to the left, so its x component is negative. The 
only two possibilities for its x component are therefore —Fyy; cos 0 
or —Fyyi sin@. We expect this force to have a large x component 
and a much smaller y. Since 0 is small, cos 9 ~ 1, while sin@ is 
small. Therefore the x component must be — Fy cos 8. 





‘Pushing a broom example 7 
> Figure | shows a man pushing a broom at an angle 0 relative to 
the horizontal. The mass m of the broom is concentrated at the 
brush. If the magnitude of the broom’s acceleration is a, find the 
force F,, that the man must make on the handle. 


> First we analyze all the forces on the brush. 


226 Chapter 8 Vectors and Motion 

















force acting on brush 3rd-law partner 

handle’s normal force brush’s normal force 

on brush, Fy, S| on handle, KR 
earth’s gravitational force brush’s gravitational force 

on brush, mg, { | on earth, ‘i 
floor’s normal force brush’s normal force 

on brush, Fy, + | on floor, L 
floor’s kinetic friction force brush’s kinetic friction force 
on brush, Fx, < | on floor, > 














Newton's second law is: 

Fx +mgQ+ Fy + F; 

aaa m 

where the addition is vector addition. If we actually want to carry 
out the vector addition of the forces, we have to do either graph- 
ical addition (as in example 5) or analytic addition. Let’s do an- 
alytic addition, which means finding all the components of the 
forces, adding the x’s, and adding the y's. 





BI) 


Most of the forces have components that are trivial to express in 
terms of their magnitudes, the exception being Fy, whose com- 
ponents we can determine using the technique demonstrated in 
example 5 on p. 209 and example 6 on p. 226. Using the coordi- 
nate system shown in the figure, the results are: 


Fuy = Fy cos 8 Fry = —F,,sin0 


mgx =0 mgy = —mg 
Fx =0 Fry = Fu 
Fry = —Fr Fry =0 


Note that we don’t yet know the magnitudes Fy, Fry, and Fx. 
That’s all right. First we need to set up Newton’s laws, and then 
we can worry about solving the equations. 


Newton's second law in the x direction gives: 


[1] 


ee Fy,cos 0 — Fx 
_ m 


The acceleration in the vertical direction is zero, so Newton’s sec- 
ond law in the y direction tells us that 


[2] 0 =—Fysin@ —mg+ Fn. 


Finally, we have the relationship between kinetic friction and the 
normal force, 


[3] Fx = ux Fy. 


Equations [1]-[3] are three equations, which we can use to de- 
termine the three unknowns, Fy, Fy, and Fx. Straightforward 


algebra gives 
A+ UG 
Fy =m 
a (ssf ons) 


Section 8.3 The force vector and simple machines 227 





Discussion question A. 


~ 


Discussion question B. 


> Solved problem: A cargo plane page 234, problem 9 
> Solved problem: The angle of repose page 235, problem 11 


> Solved problem: A wagon page 234, problem 10 
Discussion questions 


A __ The figure shows a block being pressed diagonally upward against a 
wall, causing it to slide up the wall. Analyze the forces involved, including 
their directions. 


B_ The figure shows a roller coaster car rolling down and then up under 
the influence of gravity. Sketch the car’s velocity vectors and acceleration 
vectors. Pick an interesting point in the motion and sketch a set of force 
vectors acting on the car whose vector sum could have resulted in the 
right acceleration vector. 





J Calculus with vectors 


Using the unit vector notation introduced in section 7.4, the defini- 
tions of the velocity and acceleration components given in chapter 
6 can be translated into calculus notation as 


yu its ees dz, 
~ at™ " at 





and 


dv; A dvy A dv, A 
a= x Z. 
di" a’ ad 
To make the notation less cumbersome, we generalize the concept 
of the derivative to include derivatives of vectors, so that we can 


abbreviate the above equations as 





dr 
vat 

and 
dv 


In words, to take the derivative of a vector, you take the derivatives 
of its components and make a new vector out of those. This defini- 
tion means that the derivative of a vector function has the familiar 
properties 


def) af 
di. dt 





[c is a constant] 


and 
df+g) df dg 


dt dt dt’ 
The integral of a vector is likewise defined as integrating component 


by component. 





228 Chapter 8 Vectors and Motion 


The second derivative of a vector example 8 
> Two objects have positions as functions of time given by the 
equations 


ry = 32x + ty 
and 
ro = 3t*X% + ty. 
Find both objects’ accelerations using calculus. Could either an- 
swer have been found without calculus? 
> Taking the first derivative of each component, we find 
Vi= 6X + y 
Vo = 120% +y, 
and taking the derivatives again gives acceleration, 
a; = 6X 
ap = 36f°X. 
The first object’s acceleration could have been found without cal- 
culus, simply by comparing the x and y coordinates with the 
constant-acceleration equation Ax = VoAt + sad. The second 
equation, however, isn’t just a second-order polynomial in t, so 


the acceleration isn’t constant, and we really did need calculus to 
find the corresponding acceleration. 


The integral of a vector example 9 
> Starting from rest, a flying saucer of mass m is observed to 
vary its propulsion with mathematical precision according to the 
equation 
F = bt*?x% + ct'®’y. 

(The aliens inform us that the numbers 42 and 137 have a special 
religious significance for them.) Find the saucer’s velocity as a 
function of time. 


> From the given force, we can easily find the acceleration 


a= — 
m 


Dao, C a 
= © 2g 4 = f1379, 
m m 


The velocity vector v is the integral with respect to time of the 
acceleration, 


v= [ade 
= / (Pets <r'srg) dt, 
m m 


Section 8.4 —_[{ Calculus with vectors 


229 


230 


and integrating component by component gives 


= (| Frat) x4 (f Sr°rat) 9 
m m 


439 CC 4138y 
Eagan gic) Ge eee 
4am’ ** 738m! 


where we have omitted the constants of integration, since the 
saucer was starting from rest. 


A fire-extinguisher stunt on ice example 10 
> Prof. Puerile smuggles a fire extinguisher into a skating rink. 
Climbing out onto the ice without any skates on, he sits down and 
pushes off from the wall with his feet, acquiring an initial velocity 
Voy. At tf = 0, he then discharges the fire extinguisher at a 45- 
degree angle so that it applies a force to him that is backward 
and to the left, i.e., along the negative y axis and the positive x 
axis. The fire extinguisher’s force is strong at first, but then dies 
down according to the equation |F| = b — ct, where b and c are 
constants. Find the professor’s velocity as a function of time. 


> Measured counterclockwise from the x axis, the angle of the 
force vector becomes 315°. Breaking the force down into x and 
y components, we have 
Fy = |F| cos 315° 
= (b— ct) 
Fy = |F|sin315° 
= (—b+ct). 
In unit vector notation, this is 
F =(b—ct)X+(—b+ct)y. 


Newton’s second law gives 





a=F/m 
b-—ct, —b+ect. 
= X+ 
J/2m J/2m 


To find the velocity vector as a function of time, we need to inte- 
grate the acceleration vector with respect to time, 


v= [adt 
= [ (Aare a) at 
7 J/2m J/2m 


2 aq | Ween + (b+ ct) y] dt 





Chapter 8 Vectors and Motion 


A vector function can be integrated component by component, so 
this can be broken down into two integrals, 


A 


x y 
v = —— /(b-ct)dt + [Co+ena 
v2m [ v2m ( ) 
bia Sor =pbt+ der 
= {7 2°" st constant #1) x+ {= 2"" + constant #2 | y 
JV2m J2m 
Here the physical significance of the two constants of integration 
is that they give the initial velocity. Constant #1 is therefore zero, 
and constant #2 must equal Vo. The final result is 


ey ae 5ct? PO feels set? es 
= |) —— —— | xt [| + 





Section 8.4 —_[{ Calculus with vectors 


231 


Summary 


The velocity vector points in the direction of the object’s motion. 
Relative motion can be described by vector addition of velocities. 


The acceleration vector need not point in the same direction as 
the object’s motion. We use the word “acceleration” to describe any 
change in an object’s velocity vector, which can be either a change 
in its magnitude or a change in its direction. 


An important application of the vector addition of forces is the 
use of Newton’s first law to analyze mechanical systems. 


232 Chapter 8 Vectors and Motion 


Problems 
Key 


VA computerized answer check is available online. 
{A problem that requires calculus. 
x A difficult problem. 


north ‘ 
‘ 
* direction of motion 
m of glacier relative 
‘\ y 
sooo sto continent, 1.1x10 Tmls 
ss nes ‘ 
See a ‘ 
= ‘ 
direction of motion a 
of fossil relative to Mi 
glacier, 2.3x10 -7 mis A 
Problem 1. 
1 As shown in the diagram, a dinosaur fossil is slowly moving 


down the slope of a glacier under the influence of wind, rain and 
gravity. At the same time, the glacier is moving relative to the 
continent underneath. The dashed lines represent the directions but 
not the magnitudes of the velocities. Pick a scale, and use graphical 
addition of vectors to find the magnitude and the direction of the 
fossil’s velocity relative to the continent. You will need a ruler and 
protractor. Vv 


2 Is it possible for a helicopter to have an acceleration due east 
and a velocity due west? If so, what would be going on? If not, why 
not? 


3 A bird is initially flying horizontally east at 21.1 m/s, but one 
second later it has changed direction so that it is flying horizontally 
and 7° north of east, at the same speed. What are the magnitude 
and direction of its acceleration vector during that one second time 
interval? (Assume its acceleration was roughly constant.) Vv 





Problem 4. 


4 A person of mass M stands in the middle of a tightrope, 
which is fixed at the ends to two buildings separated by a horizontal 
distance L. The rope sags in the middle, stretching and lengthening 
the rope slightly. 


Problems 


233 


(a) If the tightrope walker wants the rope to sag vertically by no 
more than a height h, find the minimum tension, T’, that the rope 
must be able to withstand without breaking, in terms of h, g, M, 
and L. Vv 
(b) Based on your equation, explain why it is not possible to get 
h = 0, and give a physical interpretation. 


5 Your hand presses a block of mass m against a wall with a 
force Fy acting at an angle 0, as shown in the figure. Find the 
minimum and maximum possible values of |Fy| that can keep the 
block stationary, in terms of m, g, 0, and ps, the coefficient of static 
friction between the block and the wall. Check both your answers 
in the case of 9 = 90°, and interpret the case where the maximum 
Problem 5. force is infinite. Vox 





6 A skier of mass m is coasting down a slope inclined at an angle 
? compared to horizontal. Assume for simplicity that the treatment 
of kinetic friction given in chapter 5 is appropriate here, although a 
soft and wet surface actually behaves a little differently. The coeffi- 
cient of kinetic friction acting between the skis and the snow is pz, 
and in addition the skier experiences an air friction force of magni- 
tude bv”, where b is a constant. 

(a) Find the maximum speed that the skier will attain, in terms of 
the variables m, g, 0, 4x, and b. v 
(b) For angles below a certain minimum angle 0,,;,, the equation 
gives a result that is not mathematically meaningful. Find an equa- 
tion for Omin, and give a physical explanation of what is happening 
for 8 < Omin- 


7 A gun is aimed horizontally to the west. The gun is fired, and 
the bullet leaves the muzzle at t = 0. The bullet’s position vector 
as a function of time is r = bx + cty + dt?z, where b, c, and d are 
positive constants. 

(a) What units would b, c, and d need to have for the equation to 
make sense? 

(b) Find the bullet’s velocity and acceleration as functions of time. 
(c) Give physical interpretations of b, c, d, X, y, and @. if 


Fthrust Fiitt 





Problem 9. 


8 Annie Oakley, riding north on horseback at 30 mi/hr, shoots 
| her rifle, aiming horizontally and to the northeast. The muzzle speed 
of the rifle is 140 mi/hr. When the bullet hits a defenseless fuzzy 
animal, what is its speed of impact? Neglect air resistance, and 
ignore the vertical motion of the bullet. > Solution, p. 554 


9 A cargo plane has taken off from a tiny airstrip in the Andes, 
and is climbing at constant speed, at an angle of 0 = 17° with 
respect to horizontal. Its engines supply a thrust of Fihrust = 200 
KN, and the lift from its wings is Fi, = 654 kN. Assume that air 
Problem 10. resistance (drag) is negligible, so the only forces acting are thrust, 
lift, and weight. What is its mass, in kg? > Solution, p. 554 


234 Chapter 8 Vectors and Motion 


10 A wagon is being pulled at constant speed up a slope 6 by a 
rope that makes an angle ¢ with the vertical. 

(a) Assuming negligible friction, show that the tension in the rope 
is given by the equation 


sin 6 
ee 
rT sinfo+¢) 


where Fi is the weight force acting on the wagon. 
(b) Interpret this equation in the special cases of ¢ = 0 and ¢ = 
180° — 0. > Solution, p. 555 


11 The angle of repose is the maximum slope on which an object 
will not slide. On airless, geologically inert bodies like the moon or 
an asteroid, the only thing that determines whether dust or rubble 
will stay on a slope is whether the slope is less steep than the angle 
of repose. (See figure n, p. 272.) 

(a) Find an equation for the angle of repose, deciding for yourself 
what are the relevant variables. 

(b) On an asteroid, where g can be thousands of times lower than 
on Earth, would rubble be able to lie at a steeper angle of repose? 

> Solution, p. 555 


12 The figure shows an experiment in which a cart is released 
from rest at A, and accelerates down the slope through a distance 
x until it passes through a sensor’s light beam. The point of the 
experiment is to determine the cart’s acceleration. At B, a card- 
board vane mounted on the cart enters the light beam, blocking the 
light beam, and starts an electronic timer running. At C, the vane 
emerges from the beam, and the timer stops. 

(a) Find the final velocity of the cart in terms of the width w of 
the vane and the time t, for which the sensor’s light beam was 


blocked. Vv 
(b) Find the magnitude of the cart’s acceleration in terms of the 
measurable quantities x, ty, and w. Vv 


(c) Analyze the forces in which the cart participates, using a table in 
the format introduced in section 5.3. Assume friction is negligible. 
(d) Find a theoretical value for the acceleration of the cart, which 
could be compared with the experimentally observed value extracted 
in part b. Express the theoretical value in terms of the angle 0 of 
the slope, and the strength g of the gravitational field. Vv 


13 The figure shows a boy hanging in three positions: (1) with 
his arms straight up, (2) with his arms at 45 degrees, and (3) with 
his arms at 60 degrees with respect to the vertical. Compare the 
tension in his arms in the three cases. 


sensor 


WwW 


Bec 


Problem 12. 








Problem 13 (Millikan and Gale, 
1920). 


Problems 235 





Problem 15. 





to climber 
Problem 16. 
a 
Problem 17. 


14 Driving down a hill inclined at an angle @ with respect to 
horizontal, you slam on the brakes to keep from hitting a deer. Your 
antilock brakes kick in, and you don’t skid. 

(a) Analyze the forces. (Ignore rolling resistance and air friction.) 
(b) Find the car’s maximum possible deceleration, a (expressed as 
a positive number), in terms of g, 0, and the relevant coefficient of 
friction. Vv 
(c) Explain physically why the car’s mass has no effect on your 
answer. 

(d) Discuss the mathematical behavior and physical interpretation 
of your result for negative values of 0. 

(ce) Do the same for very large positive values of 0. 


15 The figure shows the path followed by Hurricane Irene in 
2005 as it moved north. The dots show the location of the center 
of the storm at six-hour intervals, with lighter dots at the time 
when the storm reached its greatest intensity. Find the time when 
the storm’s center had a velocity vector to the northeast and an 
acceleration vector to the southeast. Explain. 


16 For safety, mountain climbers often wear a climbing harness 
and tie in to other climbers on a rope team or to anchors such as 
pitons or snow anchors. When using anchors, the climber usually 
wants to tie in to more than one, both for extra strength and for 
redundancy in case one fails. The figure shows such an arrangement, 
with the climber hanging from a pair of anchors forming a symmetric 
“Y” at an angle 0. The metal piece at the center is called a carabiner. 
The usual advice is to make @ < 90°; for large values of 0, the stress 
placed on the anchors can be many times greater than the actual 
load L, so that two anchors are actually less safe than one. 

(a) Find the force S at each anchor in terms of L and 0. v 
(b) Verify that your answer makes sense in the case of 0 = 0. 

(c) Interpret your answer in the case of 6 = 180°. 

(d) What is the smallest value of 0 for which S equals or exceeds 
L, so that for larger angles a failure of at least one anchor is more 
likely than it would have been with a single anchor? v 


17 (a) The person with mass m hangs from the rope, hauling the 
box of mass M up a slope inclined at an angle 6. There is friction 
between the box and the slope, described by the usual coefficients 
of friction. The pulley, however, is frictionless. Find the magnitude 
of the box’s acceleration. Vv 
(b) Show that the units of your answer make sense. 

(c) Check the physical behavior of your answer in the special cases 
of M = 0 and 6 = —90°. 


236 Chapter 8 Vectors and Motion 


18 Complete example 6 on p. 226 by expressing the remaining 
nine x and y components of the forces in terms of the five magnitudes 
and the small, positive angle 0 = 9° by which the crack overhangs. 


v 


19 Problem 16 discussed a possible correct way of setting up 
a redundant anchor for mountaineering. The figure for this prob- 
lem shows an incorrect way of doing it, by arranging the rope in 
a triangle (which we'll take to be isoceles). One of the bad things 
about the triangular arrangement is that it requires more force from 
the anchors, making them more likely to fail. (a) Using the same 
notation as in problem 16, find S in terms of L and 6. Vv to climber 
(b) Verify that your answer makes sense in the case of 6 = 0, and 





compare with the correct setup. Problem 19. 
20 A telephone wire of mass m is strung between two poles, 
making an angle @ with the horizontal at each end. (a) Find the 
tension at the center. Vv 


(b) Which is greater, the tension at the center or at the ends? 


21 The figure shows an arcade game called skee ball that is 
similar to bowling. The player rolls the ball down a horizontal alley. 
The ball then rides up a curved lip and is launched at an initial 
speed u, at an angle a@ above horizontal. Suppose we want the ball Problem 20. 
to go into a hole that is at horizontal distance @ and height h, as 

shown in the figure. 











(a) Find the initial speed u that is required, in terms of the other 
variables and g. Vv 
(b) Check that your answer to part a has units that make sense. 
(c) Check that your answer to part a depends on g in a way that 
makes sense. This means that you should first determine on physical 
grounds whether increasing g should increase u, or decrease it. Then 
see whether your answer to part a has this mathematical behavior. 
(d) Do the same for the dependence on h. 

(e) Interpret your equation in the case where a = 90°. 

(f) Interpret your equation in the case where tana = h/é. 

(g) Find wu numerically if h = 70 cm, ¢ = 60 cm, and a= 65°. V 


Problem 21. 





Problems 237 





Problem 23. 





Problem 24. 


Problem 25. 


22 A plane flies toward a city directly north and a distance D 
away. The wind speed is u, and the plane’s speed with respect to 
the wind is v. 

(a) If the wind is blowing from the west (towards the east), what 
direction should the plane head (what angle west of north)? v 
(b) How long does it take the plane to get to the city? Vv 
(c) Check that your answer to part b has units that make sense. 
(d) Comment on the behavior of your answer in the case where 
U= Uv. [problem by B. Shotwell] 


23 A force F is applied to a box of mass M at an angle 0 below 
the horizontal (see figure). The coefficient of static friction between 
the box and the floor is ys, and the coefficient of kinetic friction 
between the two surfaces is Ux. 

(a) What is the magnitude of the normal force on the box from the 
floor? Vv 
(b) What is the minimum value of F' to get the box to start moving 
from rest? v 
(c) What is the value of F' so that the box will move with constant 
velocity (assuming it is already moving)? Vv 
(d) If 6 is greater than some critical angle 0.it, it is impossible to 
have the scenario described in part c. What is Ocrit? 

V [problem by B. Shotwell] 


24 (a) A mass M is at rest on a fixed, frictionless ramp inclined 
at angle 6 with respect to the horizontal. The mass is connected 
to the force probe, as shown. What is the reading on the force 


probe? Vv 
(b) Check that your answer to part a makes sense in the special 
cases 6 = 0 and 6 = 90°. [problem by B. Shotwell] 


25 The figure shows a rock climber wedged into a dihedral or 
“open book” consisting of two vertical walls of rock at an angle 6 rel- 
ative to one another. This position can be maintained without any 
ledges or holds, simply by pressing the feet against the walls. The 
left hand is being used just for a little bit of balance. (a) Find the 
minimum coefficient of friction between the rubber climbing shoes 
and the rock. (b) Interpret the behavior of your expression at ex- 
treme values of 6. (c) Steven Won has done tabletop experiments 
using climbing shoes on the rough back side of a granite slab from 
a kitchen countertop, and has estimated yw, = 1.17. Find the corre- 
sponding maximum value of 0. > Solution, p. 556 


26 You throw a rock horizontally from the edge of the roof of 
a building of height h with speed vg. What is the (positive) angle 
between the final velocity vector and the horizontal when the rock 
hits the ground? V [problem by B. Shotwell] 


238 Chapter 8 Vectors and Motion 


27 The figure shows a block acted on by two external forces, c pal tS) 
each of magnitude F’. One of the forces is horizontal, but the other —_> 

is applied at a downward angle @. Gravity is negligible compared to 

these forces. The block rests on a surface with friction described by 

a coefficient of friction ws. (a) Find the minimum value of js that Problem 27. 

is required if the block is to remain at rest. v 

(b) Show that this expression has the correct limit as 0 approaches 

Zero. 


J 


Problems 239 


Exercise 8: Vectors and motion 


Each diagram on page ?? shows the motion of an object in an x — y plane. Each dot is one 
location of the object at one moment in time. The time interval from one dot to the next is 
always the same, so you can think of the vector that connects one dot to the next as a v vector, 
and subtract to find Av vectors. 


1. Suppose the object in diagram 1 is moving from the top left to the bottom right. Deduce 
whatever you can about the force acting on it. Does the force always have the same magnitude? 
The same direction? 


Invent a physical situation that this diagram could represent. 


What if you reinterpret the diagram by reversing the object’s direction of motion? Redo the 
construction of one of the Av vectors and see what happens. 


2. What can you deduce about the force that is acting in diagram 2? 
Invent a physical situation that diagram 2 could represent. 
3. What can you deduce about the force that is acting in diagram 3? 


Invent a physical situation. 
